
{::nomarkdown}
<div class="lwarp-contents">
{% raw %}
<div class="hidden" >

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\newcommand {\mathllap }[2][]{{#1#2}}\)

\(\newcommand {\mathrlap }[2][]{{#1#2}}\)

\(\newcommand {\mathclap }[2][]{{#1#2}}\)

\(\newcommand {\mathmbox }[1]{#1}\)

\(\newcommand {\clap }[1]{#1}\)

\(\newcommand {\LWRmathmakebox }[2][]{#2}\)

\(\newcommand {\mathmakebox }[1][]{\LWRmathmakebox }\)

\(\newcommand {\cramped }[2][]{{#1#2}}\)

\(\newcommand {\crampedllap }[2][]{{#1#2}}\)

\(\newcommand {\crampedrlap }[2][]{{#1#2}}\)

\(\newcommand {\crampedclap }[2][]{{#1#2}}\)

\(\newenvironment {crampedsubarray}[1]{}{}\)

\(\newcommand {\crampedsubstack }{}\)

\(\newcommand {\smashoperator }[2][]{#2\limits }\)

\(\newcommand {\adjustlimits }{}\)

\(\newcommand {\SwapAboveDisplaySkip }{}\)

\(\require {extpfeil}\)

\(\Newextarrow \xleftrightarrow {10,10}{0x2194}\)

\(\Newextarrow \xLeftarrow {10,10}{0x21d0}\)

\(\Newextarrow \xhookleftarrow {10,10}{0x21a9}\)

\(\Newextarrow \xmapsto {10,10}{0x21a6}\)

\(\Newextarrow \xRightarrow {10,10}{0x21d2}\)

\(\Newextarrow \xLeftrightarrow {10,10}{0x21d4}\)

\(\Newextarrow \xhookrightarrow {10,10}{0x21aa}\)

\(\Newextarrow \xrightharpoondown {10,10}{0x21c1}\)

\(\Newextarrow \xleftharpoondown {10,10}{0x21bd}\)

\(\Newextarrow \xrightleftharpoons {10,10}{0x21cc}\)

\(\Newextarrow \xrightharpoonup {10,10}{0x21c0}\)

\(\Newextarrow \xleftharpoonup {10,10}{0x21bc}\)

\(\Newextarrow \xleftrightharpoons {10,10}{0x21cb}\)

\(\newcommand {\LWRdounderbracket }[3]{\underset {#3}{\underline {#1}}}\)

\(\newcommand {\LWRunderbracket }[2][]{\LWRdounderbracket {#2}}\)

\(\newcommand {\underbracket }[1][]{\LWRunderbracket }\)

\(\newcommand {\LWRdooverbracket }[3]{\overset {#3}{\overline {#1}}}\)

\(\newcommand {\LWRoverbracket }[2][]{\LWRdooverbracket {#2}}\)

\(\newcommand {\overbracket }[1][]{\LWRoverbracket }\)

\(\newcommand {\LaTeXunderbrace }[1]{\underbrace {#1}}\)

\(\newcommand {\LaTeXoverbrace }[1]{\overbrace {#1}}\)

\(\newenvironment {matrix*}[1][]{\begin {matrix}}{\end {matrix}}\)

\(\newenvironment {pmatrix*}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bmatrix*}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bmatrix*}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vmatrix*}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vmatrix*}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newenvironment {smallmatrix*}[1][]{\begin {matrix}}{\end {matrix}}\)

\(\newenvironment {psmallmatrix*}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bsmallmatrix*}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bsmallmatrix*}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vsmallmatrix*}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vsmallmatrix*}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newenvironment {psmallmatrix}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bsmallmatrix}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bsmallmatrix}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vsmallmatrix}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vsmallmatrix}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newcommand {\LWRmultlined }[1][]{\begin {multline*}}\)

\(\newenvironment {multlined}[1][]{\LWRmultlined }{\end {multline*}}\)

\(\let \LWRorigshoveleft \shoveleft \)

\(\renewcommand {\shoveleft }[1][]{\LWRorigshoveleft }\)

\(\let \LWRorigshoveright \shoveright \)

\(\renewcommand {\shoveright }[1][]{\LWRorigshoveright }\)

\(\newenvironment {dcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {dcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {rcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {rcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {drcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {drcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {cases*}{\begin {cases}}{\end {cases}}\)

\(\newcommand {\MoveEqLeft }[1][]{}\)

\(\def \LWRAboxed #1&amp;#2&amp;#3!|!{\fbox {\(#1\)}&amp;\fbox {\(#2\)}} \newcommand {\Aboxed }[1]{\LWRAboxed #1&amp;&amp;!|!} \)

\( \newcommand {\LWRABLines }[1][\Updownarrow ]{#1 \notag \\}\newcommand {\ArrowBetweenLines }{\ifstar \LWRABLines \LWRABLines } \)

\(\newcommand {\shortintertext }[1]{\text {#1}\notag \\}\)

\(\newcommand {\vdotswithin }[1]{\hspace {.5em}\vdots }\)

\(\newcommand {\LWRshortvdotswithinstar }[1]{\vdots \hspace {.5em} &amp; \\}\)

\(\newcommand {\LWRshortvdotswithinnostar }[1]{&amp; \hspace {.5em}\vdots \\}\)

\(\newcommand {\shortvdotswithin }{\ifstar \LWRshortvdotswithinstar \LWRshortvdotswithinnostar }\)

\(\newcommand {\MTFlushSpaceAbove }{}\)

\(\newcommand {\MTFlushSpaceBelow }{\\}\)

\(\newcommand \lparen {(}\)

\(\newcommand \rparen {)}\)

\(\newcommand {\ordinarycolon }{:}\)

\(\newcommand {\vcentcolon }{\mathrel {\mathop \ordinarycolon }}\)

\(\newcommand \dblcolon {\vcentcolon \vcentcolon }\)

\(\newcommand \coloneqq {\vcentcolon =}\)

\(\newcommand \Coloneqq {\dblcolon =}\)

\(\newcommand \coloneq {\vcentcolon {-}}\)

\(\newcommand \Coloneq {\dblcolon {-}}\)

\(\newcommand \eqqcolon {=\vcentcolon }\)

\(\newcommand \Eqqcolon {=\dblcolon }\)

\(\newcommand \eqcolon {\mathrel {-}\vcentcolon }\)

\(\newcommand \Eqcolon {\mathrel {-}\dblcolon }\)

\(\newcommand \colonapprox {\vcentcolon \approx }\)

\(\newcommand \Colonapprox {\dblcolon \approx }\)

\(\newcommand \colonsim {\vcentcolon \sim }\)

\(\newcommand \Colonsim {\dblcolon \sim }\)

\(\newcommand {\nuparrow }{\mathrel {\cancel {\uparrow }}}\)

\(\newcommand {\ndownarrow }{\mathrel {\cancel {\downarrow }}}\)

\(\newcommand {\bigtimes }{\mathop {\Large \times }\limits }\)

\(\newcommand {\prescript }[3]{{}^{#1}_{#2}#3}\)

\(\newenvironment {lgathered}{\begin {gathered}}{\end {gathered}}\)

\(\newenvironment {rgathered}{\begin {gathered}}{\end {gathered}}\)

\(\newcommand {\splitfrac }[2]{{}^{#1}_{#2}}\)

\(\let \splitdfrac \splitfrac \)

\(\newcommand {\LWRoverlaysymbols }[2]{\mathord {\smash {\mathop {#2\strut }\limits ^{\smash {\lower 3ex{#1}}}}\strut }}\)

\(\newcommand{\alphaup}{\unicode{x03B1}}\)

\(\newcommand{\betaup}{\unicode{x03B2}}\)

\(\newcommand{\gammaup}{\unicode{x03B3}}\)

\(\newcommand{\digammaup}{\unicode{x03DD}}\)

\(\newcommand{\deltaup}{\unicode{x03B4}}\)

\(\newcommand{\epsilonup}{\unicode{x03F5}}\)

\(\newcommand{\varepsilonup}{\unicode{x03B5}}\)

\(\newcommand{\zetaup}{\unicode{x03B6}}\)

\(\newcommand{\etaup}{\unicode{x03B7}}\)

\(\newcommand{\thetaup}{\unicode{x03B8}}\)

\(\newcommand{\varthetaup}{\unicode{x03D1}}\)

\(\newcommand{\iotaup}{\unicode{x03B9}}\)

\(\newcommand{\kappaup}{\unicode{x03BA}}\)

\(\newcommand{\varkappaup}{\unicode{x03F0}}\)

\(\newcommand{\lambdaup}{\unicode{x03BB}}\)

\(\newcommand{\muup}{\unicode{x03BC}}\)

\(\newcommand{\nuup}{\unicode{x03BD}}\)

\(\newcommand{\xiup}{\unicode{x03BE}}\)

\(\newcommand{\omicronup}{\unicode{x03BF}}\)

\(\newcommand{\piup}{\unicode{x03C0}}\)

\(\newcommand{\varpiup}{\unicode{x03D6}}\)

\(\newcommand{\rhoup}{\unicode{x03C1}}\)

\(\newcommand{\varrhoup}{\unicode{x03F1}}\)

\(\newcommand{\sigmaup}{\unicode{x03C3}}\)

\(\newcommand{\varsigmaup}{\unicode{x03C2}}\)

\(\newcommand{\tauup}{\unicode{x03C4}}\)

\(\newcommand{\upsilonup}{\unicode{x03C5}}\)

\(\newcommand{\phiup}{\unicode{x03D5}}\)

\(\newcommand{\varphiup}{\unicode{x03C6}}\)

\(\newcommand{\chiup}{\unicode{x03C7}}\)

\(\newcommand{\psiup}{\unicode{x03C8}}\)

\(\newcommand{\omegaup}{\unicode{x03C9}}\)

\(\newcommand{\Alphaup}{\unicode{x0391}}\)

\(\newcommand{\Betaup}{\unicode{x0392}}\)

\(\newcommand{\Gammaup}{\unicode{x0393}}\)

\(\newcommand{\Digammaup}{\unicode{x03DC}}\)

\(\newcommand{\Deltaup}{\unicode{x0394}}\)

\(\newcommand{\Epsilonup}{\unicode{x0395}}\)

\(\newcommand{\Zetaup}{\unicode{x0396}}\)

\(\newcommand{\Etaup}{\unicode{x0397}}\)

\(\newcommand{\Thetaup}{\unicode{x0398}}\)

\(\newcommand{\Varthetaup}{\unicode{x03F4}}\)

\(\newcommand{\Iotaup}{\unicode{x0399}}\)

\(\newcommand{\Kappaup}{\unicode{x039A}}\)

\(\newcommand{\Lambdaup}{\unicode{x039B}}\)

\(\newcommand{\Muup}{\unicode{x039C}}\)

\(\newcommand{\Nuup}{\unicode{x039D}}\)

\(\newcommand{\Xiup}{\unicode{x039E}}\)

\(\newcommand{\Omicronup}{\unicode{x039F}}\)

\(\newcommand{\Piup}{\unicode{x03A0}}\)

\(\newcommand{\Varpiup}{\unicode{x03D6}}\)

\(\newcommand{\Rhoup}{\unicode{x03A1}}\)

\(\newcommand{\Sigmaup}{\unicode{x03A3}}\)

\(\newcommand{\Tauup}{\unicode{x03A4}}\)

\(\newcommand{\Upsilonup}{\unicode{x03A5}}\)

\(\newcommand{\Phiup}{\unicode{x03A6}}\)

\(\newcommand{\Chiup}{\unicode{x03A7}}\)

\(\newcommand{\Psiup}{\unicode{x03A8}}\)

\(\newcommand{\Omegaup}{\unicode{x03A9}}\)

\(\newcommand{\alphait}{\unicode{x1D6FC}}\)

\(\newcommand{\betait}{\unicode{x1D6FD}}\)

\(\newcommand{\gammait}{\unicode{x1D6FE}}\)

\(\newcommand{\digammait}{\mathit{\unicode{x03DD}}}\)

\(\newcommand{\deltait}{\unicode{x1D6FF}}\)

\(\newcommand{\epsilonit}{\unicode{x1D716}}\)

\(\newcommand{\varepsilonit}{\unicode{x1D700}}\)

\(\newcommand{\zetait}{\unicode{x1D701}}\)

\(\newcommand{\etait}{\unicode{x1D702}}\)

\(\newcommand{\thetait}{\unicode{x1D703}}\)

\(\newcommand{\varthetait}{\unicode{x1D717}}\)

\(\newcommand{\iotait}{\unicode{x1D704}}\)

\(\newcommand{\kappait}{\unicode{x1D705}}\)

\(\newcommand{\varkappait}{\unicode{x1D718}}\)

\(\newcommand{\lambdait}{\unicode{x1D706}}\)

\(\newcommand{\muit}{\unicode{x1D707}}\)

\(\newcommand{\nuit}{\unicode{x1D708}}\)

\(\newcommand{\xiit}{\unicode{x1D709}}\)

\(\newcommand{\omicronit}{\unicode{x1D70A}}\)

\(\newcommand{\piit}{\unicode{x1D70B}}\)

\(\newcommand{\varpiit}{\unicode{x1D71B}}\)

\(\newcommand{\rhoit}{\unicode{x1D70C}}\)

\(\newcommand{\varrhoit}{\unicode{x1D71A}}\)

\(\newcommand{\sigmait}{\unicode{x1D70E}}\)

\(\newcommand{\varsigmait}{\unicode{x1D70D}}\)

\(\newcommand{\tauit}{\unicode{x1D70F}}\)

\(\newcommand{\upsilonit}{\unicode{x1D710}}\)

\(\newcommand{\phiit}{\unicode{x1D719}}\)

\(\newcommand{\varphiit}{\unicode{x1D711}}\)

\(\newcommand{\chiit}{\unicode{x1D712}}\)

\(\newcommand{\psiit}{\unicode{x1D713}}\)

\(\newcommand{\omegait}{\unicode{x1D714}}\)

\(\newcommand{\Alphait}{\unicode{x1D6E2}}\)

\(\newcommand{\Betait}{\unicode{x1D6E3}}\)

\(\newcommand{\Gammait}{\unicode{x1D6E4}}\)

\(\newcommand{\Digammait}{\mathit{\unicode{x03DC}}}\)

\(\newcommand{\Deltait}{\unicode{x1D6E5}}\)

\(\newcommand{\Epsilonit}{\unicode{x1D6E6}}\)

\(\newcommand{\Zetait}{\unicode{x1D6E7}}\)

\(\newcommand{\Etait}{\unicode{x1D6E8}}\)

\(\newcommand{\Thetait}{\unicode{x1D6E9}}\)

\(\newcommand{\Varthetait}{\unicode{x1D6F3}}\)

\(\newcommand{\Iotait}{\unicode{x1D6EA}}\)

\(\newcommand{\Kappait}{\unicode{x1D6EB}}\)

\(\newcommand{\Lambdait}{\unicode{x1D6EC}}\)

\(\newcommand{\Muit}{\unicode{x1D6ED}}\)

\(\newcommand{\Nuit}{\unicode{x1D6EE}}\)

\(\newcommand{\Xiit}{\unicode{x1D6EF}}\)

\(\newcommand{\Omicronit}{\unicode{x1D6F0}}\)

\(\newcommand{\Piit}{\unicode{x1D6F1}}\)

\(\newcommand{\Rhoit}{\unicode{x1D6F2}}\)

\(\newcommand{\Sigmait}{\unicode{x1D6F4}}\)

\(\newcommand{\Tauit}{\unicode{x1D6F5}}\)

\(\newcommand{\Upsilonit}{\unicode{x1D6F6}}\)

\(\newcommand{\Phiit}{\unicode{x1D6F7}}\)

\(\newcommand{\Chiit}{\unicode{x1D6F8}}\)

\(\newcommand{\Psiit}{\unicode{x1D6F9}}\)

\(\newcommand{\Omegait}{\unicode{x1D6FA}}\)

\(\let \digammaup \Digammaup \)

\(\renewcommand {\digammait }{\mathit {\digammaup }}\)

\(\newcommand {\smallin }{\unicode {x220A}}\)

\(\newcommand {\smallowns }{\unicode {x220D}}\)

\(\newcommand {\notsmallin }{\LWRoverlaysymbols {/}{\unicode {x220A}}}\)

\(\newcommand {\notsmallowns }{\LWRoverlaysymbols {/}{\unicode {x220D}}}\)

\(\newcommand {\rightangle }{\unicode {x221F}}\)

\(\newcommand {\intclockwise }{\unicode {x2231}}\)

\(\newcommand {\ointclockwise }{\unicode {x2232}}\)

\(\newcommand {\ointctrclockwise }{\unicode {x2233}}\)

\(\newcommand {\oiint }{\unicode {x222F}}\)

\(\newcommand {\oiiint }{\unicode {x2230}}\)

\(\newcommand {\ddag }{\unicode {x2021}}\)

\(\newcommand {\P }{\unicode {x00B6}}\)

\(\newcommand {\copyright }{\unicode {x00A9}}\)

\(\newcommand {\dag }{\unicode {x2020}}\)

\(\newcommand {\pounds }{\unicode {x00A3}}\)

\(\newcommand {\iddots }{\unicode {x22F0}}\)

\(\newcommand {\utimes }{\overline {\times }}\)

\(\newcommand {\dtimes }{\underline {\times }}\)

\(\newcommand {\udtimes }{\overline {\underline {\times }}}\)

\(\newcommand {\leftwave }{\left \{}\)

\(\newcommand {\rightwave }{\right \}}\)

\(\newcommand {\toprule }[1][]{\hline }\)

\(\let \midrule \toprule \)

\(\let \bottomrule \toprule \)

\(\newcommand {\cmidrule }[2][]{}\)

\(\newcommand {\morecmidrules }{}\)

\(\newcommand {\specialrule }[3]{\hline }\)

\(\newcommand {\addlinespace }[1][]{}\)

\(\newcommand {\LWRsubmultirow }[2][]{#2}\)

\(\newcommand {\LWRmultirow }[2][]{\LWRsubmultirow }\)

\(\newcommand {\multirow }[2][]{\LWRmultirow }\)

\(\newcommand {\mrowcell }{}\)

\(\newcommand {\mcolrowcell }{}\)

\(\newcommand {\STneed }[1]{}\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\newcommand {\tothe }[1]{^{#1}}\)

\(\newcommand {\raiseto }[2]{{#2}^{#1}}\)

\(\newcommand {\ang }[2][]{(\mathrm {#2})\degree }\)

\(\newcommand {\num }[2][]{\mathrm {#2}}\)

\(\newcommand {\si }[2][]{\mathrm {#2}}\)

\(\newcommand {\LWRSI }[2][]{\mathrm {#1\LWRSInumber \,#2}}\)

\(\newcommand {\SI }[2][]{\def \LWRSInumber {#2}\LWRSI }\)

\(\newcommand {\numlist }[2][]{\mathrm {#2}}\)

\(\newcommand {\numrange }[3][]{\mathrm {#2\,\unicode {x2013}\,#3}}\)

\(\newcommand {\SIlist }[3][]{\mathrm {#2\,#3}}\)

\(\newcommand {\SIrange }[4][]{\mathrm {#2\,#4\,\unicode {x2013}\,#3\,#4}}\)

\(\newcommand {\tablenum }[2][]{\mathrm {#2}}\)

\(\newcommand {\ampere }{\mathrm {A}}\)

\(\newcommand {\candela }{\mathrm {cd}}\)

\(\newcommand {\kelvin }{\mathrm {K}}\)

\(\newcommand {\kilogram }{\mathrm {kg}}\)

\(\newcommand {\metre }{\mathrm {m}}\)

\(\newcommand {\mole }{\mathrm {mol}}\)

\(\newcommand {\second }{\mathrm {s}}\)

\(\newcommand {\becquerel }{\mathrm {Bq}}\)

\(\newcommand {\degreeCelsius }{\unicode {x2103}}\)

\(\newcommand {\coulomb }{\mathrm {C}}\)

\(\newcommand {\farad }{\mathrm {F}}\)

\(\newcommand {\gray }{\mathrm {Gy}}\)

\(\newcommand {\hertz }{\mathrm {Hz}}\)

\(\newcommand {\henry }{\mathrm {H}}\)

\(\newcommand {\joule }{\mathrm {J}}\)

\(\newcommand {\katal }{\mathrm {kat}}\)

\(\newcommand {\lumen }{\mathrm {lm}}\)

\(\newcommand {\lux }{\mathrm {lx}}\)

\(\newcommand {\newton }{\mathrm {N}}\)

\(\newcommand {\ohm }{\mathrm {\Omega }}\)

\(\newcommand {\pascal }{\mathrm {Pa}}\)

\(\newcommand {\radian }{\mathrm {rad}}\)

\(\newcommand {\siemens }{\mathrm {S}}\)

\(\newcommand {\sievert }{\mathrm {Sv}}\)

\(\newcommand {\steradian }{\mathrm {sr}}\)

\(\newcommand {\tesla }{\mathrm {T}}\)

\(\newcommand {\volt }{\mathrm {V}}\)

\(\newcommand {\watt }{\mathrm {W}}\)

\(\newcommand {\weber }{\mathrm {Wb}}\)

\(\newcommand {\day }{\mathrm {d}}\)

\(\newcommand {\degree }{\mathrm {^\circ }}\)

\(\newcommand {\hectare }{\mathrm {ha}}\)

\(\newcommand {\hour }{\mathrm {h}}\)

\(\newcommand {\litre }{\mathrm {l}}\)

\(\newcommand {\liter }{\mathrm {L}}\)

\(\newcommand {\arcminute }{^\prime }\)
\(\newcommand {\minute }{\mathrm {min}}\)

\(\newcommand {\arcsecond }{^{\prime \prime }}\)

\(\newcommand {\tonne }{\mathrm {t}}\)

\(\newcommand {\astronomicalunit }{au}\)

\(\newcommand {\atomicmassunit }{u}\)

\(\newcommand {\bohr }{\mathit {a}_0}\)

\(\newcommand {\clight }{\mathit {c}_0}\)

\(\newcommand {\dalton }{\mathrm {D}_\mathrm {a}}\)

\(\newcommand {\electronmass }{\mathit {m}_{\mathrm {e}}}\)

\(\newcommand {\electronvolt }{\mathrm {eV}}\)

\(\newcommand {\elementarycharge }{\mathit {e}}\)

\(\newcommand {\hartree }{\mathit {E}_{\mathrm {h}}}\)

\(\newcommand {\planckbar }{\mathit {\unicode {x210F}}}\)

\(\newcommand {\angstrom }{\mathrm {\unicode {x212B}}}\)

\(\let \LWRorigbar \bar \)

\(\newcommand {\bar }{\mathrm {bar}}\)

\(\newcommand {\barn }{\mathrm {b}}\)

\(\newcommand {\bel }{\mathrm {B}}\)

\(\newcommand {\decibel }{\mathrm {dB}}\)

\(\newcommand {\knot }{\mathrm {kn}}\)

\(\newcommand {\mmHg }{\mathrm {mmHg}}\)

\(\newcommand {\nauticalmile }{\mathrm {M}}\)

\(\newcommand {\neper }{\mathrm {Np}}\)

\(\newcommand {\yocto }{\mathrm {y}}\)

\(\newcommand {\zepto }{\mathrm {z}}\)

\(\newcommand {\atto }{\mathrm {a}}\)

\(\newcommand {\femto }{\mathrm {f}}\)

\(\newcommand {\pico }{\mathrm {p}}\)

\(\newcommand {\nano }{\mathrm {n}}\)

\(\newcommand {\micro }{\mathrm {\unicode {x00B5}}}\)

\(\newcommand {\milli }{\mathrm {m}}\)

\(\newcommand {\centi }{\mathrm {c}}\)

\(\newcommand {\deci }{\mathrm {d}}\)

\(\newcommand {\deca }{\mathrm {da}}\)

\(\newcommand {\hecto }{\mathrm {h}}\)

\(\newcommand {\kilo }{\mathrm {k}}\)

\(\newcommand {\mega }{\mathrm {M}}\)

\(\newcommand {\giga }{\mathrm {G}}\)

\(\newcommand {\tera }{\mathrm {T}}\)

\(\newcommand {\peta }{\mathrm {P}}\)

\(\newcommand {\exa }{\mathrm {E}}\)

\(\newcommand {\zetta }{\mathrm {Z}}\)

\(\newcommand {\yotta }{\mathrm {Y}}\)

\(\newcommand {\percent }{\mathrm {\%}}\)

\(\newcommand {\meter }{\mathrm {m}}\)

\(\newcommand {\metre }{\mathrm {m}}\)

\(\newcommand {\gram }{\mathrm {g}}\)

\(\newcommand {\kg }{\kilo \gram }\)

\(\newcommand {\of }[1]{_{\mathrm {#1}}}\)

\(\newcommand {\squared }{^2}\)

\(\newcommand {\square }[1]{\mathrm {#1}^2}\)

\(\newcommand {\cubed }{^3}\)

\(\newcommand {\cubic }[1]{\mathrm {#1}^3}\)

\(\newcommand {\per }{/}\)

\(\newcommand {\celsius }{\unicode {x2103}}\)

\(\newcommand {\fg }{\femto \gram }\)

\(\newcommand {\pg }{\pico \gram }\)

\(\newcommand {\ng }{\nano \gram }\)

\(\newcommand {\ug }{\micro \gram }\)

\(\newcommand {\mg }{\milli \gram }\)

\(\newcommand {\g }{\gram }\)

\(\newcommand {\kg }{\kilo \gram }\)

\(\newcommand {\amu }{\mathrm {u}}\)

\(\newcommand {\nm }{\nano \metre }\)

\(\newcommand {\um }{\micro \metre }\)

\(\newcommand {\mm }{\milli \metre }\)

\(\newcommand {\cm }{\centi \metre }\)

\(\newcommand {\dm }{\deci \metre }\)

\(\newcommand {\m }{\metre }\)

\(\newcommand {\km }{\kilo \metre }\)

\(\newcommand {\as }{\atto \second }\)

\(\newcommand {\fs }{\femto \second }\)

\(\newcommand {\ps }{\pico \second }\)

\(\newcommand {\ns }{\nano \second }\)

\(\newcommand {\us }{\micro \second }\)

\(\newcommand {\ms }{\milli \second }\)

\(\newcommand {\s }{\second }\)

\(\newcommand {\fmol }{\femto \mol }\)

\(\newcommand {\pmol }{\pico \mol }\)

\(\newcommand {\nmol }{\nano \mol }\)

\(\newcommand {\umol }{\micro \mol }\)

\(\newcommand {\mmol }{\milli \mol }\)

\(\newcommand {\mol }{\mol }\)

\(\newcommand {\kmol }{\kilo \mol }\)

\(\newcommand {\pA }{\pico \ampere }\)

\(\newcommand {\nA }{\nano \ampere }\)

\(\newcommand {\uA }{\micro \ampere }\)

\(\newcommand {\mA }{\milli \ampere }\)

\(\newcommand {\A }{\ampere }\)

\(\newcommand {\kA }{\kilo \ampere }\)

\(\newcommand {\ul }{\micro \litre }\)

\(\newcommand {\ml }{\milli \litre }\)

\(\newcommand {\l }{\litre }\)

\(\newcommand {\hl }{\hecto \litre }\)

\(\newcommand {\uL }{\micro \liter }\)

\(\newcommand {\mL }{\milli \liter }\)

\(\newcommand {\L }{\liter }\)

\(\newcommand {\hL }{\hecto \liter }\)

\(\newcommand {\mHz }{\milli \hertz }\)

\(\newcommand {\Hz }{\hertz }\)

\(\newcommand {\kHz }{\kilo \hertz }\)

\(\newcommand {\MHz }{\mega \hertz }\)

\(\newcommand {\GHz }{\giga \hertz }\)

\(\newcommand {\THz }{\tera \hertz }\)

\(\newcommand {\mN }{\milli \newton }\)

\(\newcommand {\N }{\newton }\)

\(\newcommand {\kN }{\kilo \newton }\)

\(\newcommand {\MN }{\mega \newton }\)

\(\newcommand {\Pa }{\pascal }\)

\(\newcommand {\kPa }{\kilo \pascal }\)

\(\newcommand {\MPa }{\mega \pascal }\)

\(\newcommand {\GPa }{\giga \pascal }\)

\(\newcommand {\mohm }{\milli \ohm }\)

\(\newcommand {\kohm }{\kilo \ohm }\)

\(\newcommand {\Mohm }{\mega \ohm }\)

\(\newcommand {\pV }{\pico \volt }\)

\(\newcommand {\nV }{\nano \volt }\)

\(\newcommand {\uV }{\micro \volt }\)

\(\newcommand {\mV }{\milli \volt }\)

\(\newcommand {\V }{\volt }\)

\(\newcommand {\kV }{\kilo \volt }\)

\(\newcommand {\W }{\watt }\)

\(\newcommand {\uW }{\micro \watt }\)

\(\newcommand {\mW }{\milli \watt }\)

\(\newcommand {\kW }{\kilo \watt }\)

\(\newcommand {\MW }{\mega \watt }\)

\(\newcommand {\GW }{\giga \watt }\)

\(\newcommand {\J }{\joule }\)

\(\newcommand {\uJ }{\micro \joule }\)

\(\newcommand {\mJ }{\milli \joule }\)

\(\newcommand {\kJ }{\kilo \joule }\)

\(\newcommand {\eV }{\electronvolt }\)

\(\newcommand {\meV }{\milli \electronvolt }\)

\(\newcommand {\keV }{\kilo \electronvolt }\)

\(\newcommand {\MeV }{\mega \electronvolt }\)

\(\newcommand {\GeV }{\giga \electronvolt }\)

\(\newcommand {\TeV }{\tera \electronvolt }\)

\(\newcommand {\kWh }{\kilo \watt \hour }\)

\(\newcommand {\F }{\farad }\)

\(\newcommand {\fF }{\femto \farad }\)

\(\newcommand {\pF }{\pico \farad }\)

\(\newcommand {\K }{\mathrm {K}}\)

\(\newcommand {\dB }{\mathrm {dB}}\)

\(\newcommand {\kibi }{\mathrm {Ki}}\)

\(\newcommand {\mebi }{\mathrm {Mi}}\)

\(\newcommand {\gibi }{\mathrm {Gi}}\)

\(\newcommand {\tebi }{\mathrm {Ti}}\)

\(\newcommand {\pebi }{\mathrm {Pi}}\)

\(\newcommand {\exbi }{\mathrm {Ei}}\)

\(\newcommand {\zebi }{\mathrm {Zi}}\)

\(\newcommand {\yobi }{\mathrm {Yi}}\)

\(\require {mhchem}\)

\(\require {cancel}\)

\(\newcommand {\fint }{âĺŊ}\)

\(\newcommand {\hdots }{\cdots }\)

\(\newcommand {\mathnormal }[1]{#1}\)

\(\newcommand {\vecs }[2]{\vec {#1}_{#2}}\)

\(\renewcommand {\O }{\ensuremath {\mathcal {O}}}\)

\(\newcommand {\vol }{\ensuremath {\operatorname {vol}}}\)

\(\renewcommand {\i }{\ensuremath {\mathrm {i}}}\)

\(\newcommand {\FFT }{\ensuremath {\operatorname {FFT}}}\)

\(\newcommand {\IFFT }{\ensuremath {\operatorname {IFFT}}}\)

\(\newcommand {\diag }{\ensuremath {\operatorname {diag}}}\)

\(\newcommand {\grad }{\ensuremath {\operatorname {grad}}}\)

\(\newcommand {\const }{\ensuremath {\operatorname {const}}}\)

\(\newcommand {\name }[1]{\textsc {#1}}\)

\(\newcommand {\smallpmatrix }[1]{\left (\begin {smallmatrix}#1\end {smallmatrix}\right )}\)

\(\newcommand {\matlab }{{\fontfamily {bch}\scshape \selectfont {}Matlab}}\)

\(\newcommand {\innerproduct }[1]{\left \langle {#1}\right \rangle }\)

\(\newcommand {\norm }[1]{\left \Vert {#1}\right \Vert }\)

\(\renewcommand {\natural }{\mathbb {N}}\)

\(\newcommand {\integer }{\mathbb {Z}}\)

\(\newcommand {\rational }{\mathbb {Q}}\)

\(\newcommand {\real }{\mathbb {R}}\)

\(\newcommand {\complex }{\mathbb {C}}\)

\(\renewcommand {\d }{\mathop {}\!\mathrm {d}}\)

\(\newcommand {\dr }{\d {}r}\)

\(\newcommand {\ds }{\d {}s}\)

\(\newcommand {\dt }{\d {}t}\)

\(\newcommand {\du }{\d {}u}\)

\(\newcommand {\dv }{\d {}v}\)

\(\newcommand {\dw }{\d {}w}\)

\(\newcommand {\dx }{\d {}x}\)

\(\newcommand {\dy }{\d {}y}\)

\(\newcommand {\dz }{\d {}z}\)

\(\newcommand {\dsigma }{\d {}\sigma }\)

\(\newcommand {\dphi }{\d {}\phi }\)

\(\newcommand {\dvarphi }{\d {}\varphi }\)

\(\newcommand {\dtau }{\d {}\tau }\)

\(\newcommand {\dxi }{\d {}\xi }\)

\(\newcommand {\dtheta }{\d {}\theta }\)

\(\newcommand {\tp }{\mathrm {T}}\)

</div>

<style type="text/css">
.lwarp-contents li.list-item-f0::marker {
  font-style:italic;
  content:'(1)\00a0\00a0';
}
.lwarp-contents li.list-item-f1::marker {
  font-style:italic;
  content:'(2)\00a0\00a0';
}
.lwarp-contents li.list-item-f2::marker {
  font-style:italic;
  content:'(3)\00a0\00a0';
}
.lwarp-contents li.list-item-f3::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f4::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f5::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f6::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f7::marker {
  font-style:italic;
  content:'(1)\00a0\00a0';
}
.lwarp-contents li.list-item-f8::marker {
  font-style:italic;
  content:'(2)\00a0\00a0';
}
</style>
<p>

</p>


<h2 id="nullstellen-von-funktionen">Nullstellen von Funktionen</h2>

</p>

<h3 id="bisektionsverfahren">Bisektionsverfahren</h3>

</p>

<p>
Sei \(f\colon [a, b] \rightarrow \real \) eine stetige Funktion mit \(f(a) f(b) \le 0\).<br />
Nach dem Zwischenwertsatz besitzt \(f\) mindestens eine Nullstelle in \([a, b]\).
</p>
<p>
Halbiert man das Intervall und wertet \(f\) an der Intervallmitte \(c = \frac {a + b}{2}\) aus, so kann man mithilfe des Vorzeichens entscheiden, in welchem Teilintervall eine Nullstelle liegen muss:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                              f (a) f (c) ≤ 0           ⇒       es gibt eine Nullstelle in [a, c]
                                                                                              f (a) f (c) ≥ 0           ⇒       es gibt eine Nullstelle in [c, b].



-->


<p>

\begin{align*}
f(a)f(c) \le 0 \quad \Rightarrow &amp; \quad \text {es gibt eine Nullstelle in } [a, c] \\ f(a)f(c) \ge 0 \quad \Rightarrow &amp; \quad \text {es gibt eine
Nullstelle in } [c, b].
\end{align*}
Nun wa&#x0308;hlt man das entsprechende Teilintervall aus und iteriert das Verfahren, bis die La&#x0308;nge des Intervalls die gewu&#x0308;nschte Genauigkeit erreicht hat.
</p>
<p>
Dieses Verfahren zur Nullstellenbestimmung heißt <b>Bisektionsverfahren</b> (Zweiteilung).<br />
Die Schnelligkeit der Konvergenz eines Iterationsverfahrens kann mit der Konvergenzordnung \(p \ge 1\) beurteilt werden. Ist der \(\ell \)-te Fehler \(e_\ell := x_\ell - x_\ast \) mit der \(\ell \)-ten
Na&#x0308;herung \(x_\ell \) und der exakten Lo&#x0308;sung \(x_\ast \), so gilt \(|e_{\ell +1}| \le c \cdot |e_\ell |^p\) mit \(p \ge 1\) und \(c &lt; 1\) fu&#x0308;r \(p = 1\) (lineare
Konvergenz). Die Konvergenzordnung \(p\) hat wesentlichen Einfluss auf die Konvergenzgeschwindigkeit, zum Beispiel bei \(p = 3\) (kubische Konvergenz) und \(e_0 = \frac {1}{10}\) ist schon \(|e_3| \le
10^{-14}\). Im Falle des Bisektionsverfahrens ist \(p = 1\) und \(c = \frac {1}{2}\), die Bisektion ist also recht langsam (ungefa&#x0308;hr drei Schritte fu&#x0308;r eine Dezimalstelle).
</p>


<h3 id="sekanten-verfahren">Sekanten-Verfahren</h3>

</p>

<p>
Zwei hinreichend gute Na&#x0308;herungen \(x_{\ell -1}\) und \(x_\ell \) einer Nullstelle \(x_\ast \) von \(f\) ko&#x0308;nnen im Allgemeinen durch Bestimmung der Nullstelle der interpolierenden Gerade
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                       x `−1 f (x ` ) − x ` f (x `−1 )        (x ` − x `−1 ) f (x ` )
                                                                                             x `+1 =                                   = x` −
                                                                                                           f (x ` ) − f (x `−1 )               f (x ` ) − f (x `−1 )


-->


<p>

\begin{align*}
x_{\ell +1} = \frac {x_{\ell -1} f(x_\ell ) - x_\ell f(x_{\ell -1})} {f(x_\ell ) - f(x_{\ell -1})} = x_\ell - \frac {(x_\ell - x_{\ell -1}) f(x_\ell )}
{f(x_\ell ) - f(x_{\ell -1})}
\end{align*}
verbessert werden.<br />
Die wiederholte Anwendung dieser linearen Approximation heißt <b>Sekanten-Verfahren</b>.
</p>
<p>
Ist die approximierte Nullstelle \(x_\ast \) einfach (d.&#x202f;h. \(f’(x_\ast ) \not = 0\)), so besitzt das Verfahren die Konvergenzordnung \(r = \frac {1 + \sqrt {5}}{2}\) (Goldener Schnitt). Genauer
gilt fu&#x0308;r den Fehler \(e_\ell := |x_\ell - x_\ast |\)
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                                                 1/r
                                                                                                                        e`+1  f 00 (x ∗ )
                                                                                                                     lim r =                                               .
                                                                                                                    `→∞ e
                                                                                                                          `
                                                                                                                             2 f 0 (x ∗ )


-->


<p>

\begin{align*}
\lim _{\ell \to \infty } \frac {e_{\ell +1}}{e_\ell ^r} = \left |\frac {f”(x_\ast )}{2 f’(x_\ast )}\right |^{1/r}.
\end{align*}
Da pro Iterationsschritt nur eine Funktionsauswertung erforderlich ist, ist das Sekanten-Verfahren etwas effizienter als die Newton-Iteration (\(r^2 &gt; 2\)).
</p>


<h3 id="inverse-interpolation">Inverse Interpolation</h3>

</p>

<p>
Aus Na&#x0308;herungen \(x_{\ell -n}, \dotsc , x_\ell \) fu&#x0308;r eine Nullstelle \(x_\ast \) einer Funktion \(f\) kann man eine Approximation \(x_{\ell +1} \approx x_\ast \) durch
<b>inverse Interpolation</b> der Funktionswerte \(f_k = f(x_k)\) mit einem Polynom \(p\) vom Grad \(\le n\) gewinnen. Sind die Werte \(f_k\) paarweise verschieden, so ist
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--

                                                                                                                                                                                    !
                                                                                                                                  X̀                      Y                fj
                                                                                                        x `+1 = p(0) =                           xk                                     .
                                                                                                                                 k=`−n                    j6=k
                                                                                                                                                                   f j − fk



-->


<p>

\begin{align*}
x_{\ell +1} = p(0) = \sum _{k=\ell -n}^\ell \left (x_k \prod _{j\not =k} \frac {f_j}{f_j - f_k}\right ).
\end{align*}
Die Iteration des Verfahrens ist fu&#x0308;r glatte Funktionen bei einfachen Nullstellen lokal konvergent und sehr effizient. Allerdings ist der Iterationsschritt nicht immer durchfu&#x0308;hrbar: Die mo&#x0308;glichen
Ausnahmefa&#x0308;lle mu&#x0308;ssen mithilfe eines anderen Verfahrens (z.&#x202f;B. Bisektion) u&#x0308;berbru&#x0308;ckt werden.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Ha&#x0308;ufig angewendet wird die <b>quadratische inverse Interpolation</b> (\(n = 2\)).<br />
Fu&#x0308;r die Daten \((x_0, f_0)\), \((x_1, f_1)\) und \((x_2, f_2)\) hat die Approximation dann die Form
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                     f1 f2                       f0 f2                       f0 f1
                                                                                   x∗ ≈ x0                          + x1                        + x2                        .
                                                                                             ( f1 − f0 )( f2 − f0 )      ( f0 − f1 )( f2 − f1 )      ( f0 − f2 )( f1 − f2 )


-->


<p>

\begin{align*}
x_\ast \approx x_0 \frac {f_1 f_2}{(f_1 - f_0)(f_2 - f_0)} + x_1 \frac {f_0 f_2}{(f_0 - f_1)(f_2 - f_1)} + x_2 \frac {f_0 f_1}{(f_0 - f_2)(f_1 - f_2)}.
\end{align*}
Beginnt man mit einem Intervall \([a, b]\), an dessen Endpunkten die Funktion \(f\) ihr Vorzeichen wechselt, so kann man die inverse Interpolation sehr effektiv mit dem Bisektionsverfahren verbinden. Neben einer Folge von
Approximationen \(x_0, x_1, \dotsc \) werden Intervalle \(I_\ell \) mit Vorzeichenwechsel von \(f\) gespeichert, deren einer Eckpunkt \(x_\ell \) ist. Ein Iterationsschritt \(x_\ell \rightarrow x_{\ell
+1}\) verla&#x0308;uft wie folgt:
</p>
<ul style="list-style-type:none">

<li class="list-item-f0"><p>Ist die inverse quadratische Interpolation mit \(x_{\ell -2}\), \(x_{\ell -1}\) und \(x_\ell \) durchfu&#x0308;hrbar und liefert einen Punkt \(x_{\ell +1}\) im Inneren von \(I_\ell \), so
wird die Approximation akzeptiert.
</p>
</li>
<li class="list-item-f1"><p>Andernfalls (oder falls im letzten Schritt mit Interpolation keine signifikante Verbesserung erzielt wurde) wird \(x_{\ell +1}\) mit Bisektion aus den Endpunkten von \(I_\ell \) bestimmt.
</p>
</li>
<li class="list-item-f2"><p>Durch Einbeziehung des neuen Punktes wird das Intervall aktualisiert. Bei einem Bisektionsschritt werden zusa&#x0308;tzlich \(x_{\ell -1}\) und \(x_\ell \) durch die neuen Intervallendpunkte ersetzt.
</p>
</li>
</ul>


<h3 id="newton-verfahren"><span style="font-variant: small-caps;">Newton</span>-Verfahren</h3>

</p>

<p>
Mit dem <b><span class="textsc" >Newton</span>-Verfahren</b> kann eine Nullstelle \(x_\ast \) einer Funktion \(f\) numerisch bestimmt werden. Die Folge \(x_0, x_1, \dotsc \) der Approximationen
wird durch Linearisierung gewonnen. Die Na&#x0308;herung \(x_{\ell +1}\) ist der Schnittpunkt der Tangente im Punkt \((x_\ell , f(x_\ell ))\) mit der \(x\)-Achse:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                                 f (x ` )
                                                                                                                        x `+1 = x ` −                       .
                                                                                                                                                 f 0 (x ` )


-->


<p>

\begin{align*}
x_{\ell +1} = x_\ell - \frac {f(x_\ell )}{f’(x_\ell )}.
\end{align*}
Fu&#x0308;r eine einfache Nullstelle \(x_\ast \) konvergiert die Newton-Iteration lokal quadratisch, d.&#x202f;h.<br />
\(|x_{\ell +1} - x_\ast | \le c |x_\ell - x_\ast |^2\) fu&#x0308;r Startpunkte \(x_0\) in einer hinreichend kleinen Umgebung von \(x_\ast \).
</p>
<p>
Die Voraussetzung, dass der Startwert \(x_0\) in einer hinreichend kleinen Umgebung von \(x_\ast \) liegt, ist notwendig, d.&#x202f;h. das Newton-Verfahren konvergiert i.&#x202f;A. nicht fu&#x0308;r Startwerte außerhalb
der Umgebung.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Das <b><span class="textsc" >Heron</span>-Verfahren</b> (auch babylonisches Wurzelziehen genannt) \(x \leftarrow (x + a/x) / 2\) zur Berechnung der Wurzel einer positiven Zahl \(a\) stellt sich
bei genauerer Betrachtung als das Newton-Verfahren fu&#x0308;r \(f(x) = x^2 - a\) heraus:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                        1    a     x2 − a
                                                                                                               x←          x+    =x−        .
                                                                                                                        2     x       2x


-->


<p>

\begin{align*}
x \leftarrow \frac {1}{2} \left (x + \frac {a}{x}\right ) = x - \frac {x^2 - a}{2x}.
\end{align*}
Die Konvergenz ist z.&#x202f;B. fu&#x0308;r \(a = 2\), \(x_0 = 1\) a&#x0308;ußerst schnell. Bei jedem Schritt verdoppelt sich die Anzahl der korrekten Stellen.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Fa&#x0308;rbt man bei der komplexen Newton-Iteration Bereiche der komplexen Zahlenebene je nach Konvergenzgeschwindigkeit unterschiedlich ein, so erha&#x0308;lt man teilweise merkwu&#x0308;rdig aussehende Fraktale
(<b><span class="textsc" >Newton</span>-Fraktale</b>). Ein solches ergibt sich bspw. fu&#x0308;r die Gleichung \(z^3 - 1 = 0\), \(z \in \complex \).
</p>


<h3 id="muellers-verfahren"><span style="font-variant: small-caps;">Müller</span>s Verfahren</h3>

</p>

<p>
Mit <b><span class="textsc" >Mu&#x0308;ller</span>s Verfahren</b> ko&#x0308;nnen sowohl reelle als auch komplexe Nullstellen einer Funktion \(f\) approximiert werden. Dabei wird eine Folge \(z_0,
z_1, \dotsc \) von Na&#x0308;herungen fu&#x0308;r eine Nullstelle \(z_\ast \) mithilfe von quadratischer Interpolation generiert. Die Approximation \(z_{\ell +1}\) ist die am na&#x0308;chsten bei \(z_\ell \)
gelegene Nullstelle der Parabel, die die Punkte \((z_\ell , f(z_\ell ))\), \((z_{\ell -1}, f(z_{\ell -1}))\), \((z_{\ell -2}, f(z_{\ell -2}))\) interpoliert:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                                2 f (z` )
                                                                                                          z`+1 = z` −                      q                                        ,
                                                                                                                                β` ±           β`2 − 4 f (z` )α`


-->


<p>

\begin{align*}
z_{\ell +1} = z_\ell - \frac {2 f(z_\ell )}{\beta _\ell \pm \sqrt {\beta _\ell ^2 - 4 f(z_\ell ) \alpha _\ell }},
\end{align*}

</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                        α` = ∆(z` , z`−1 , z`−2 ) f ,            β` = ∆(z` , z`−1 ) f + α` (z` − z`−1 ).



-->


<p>

\begin{align*}
\alpha _\ell = \Delta (z_\ell , z_{\ell -1}, z_{\ell -2}) f, \quad \beta _\ell = \Delta (z_\ell , z_{\ell -1}) f + \alpha _\ell (z_\ell - z_{\ell -1}).
\end{align*}
Dabei ist das Vorzeichen so gewa&#x0308;hlt, dass der Betrag des Nenners am gro&#x0308;ßten wird.
</p>
<p>
Im Fall von zusammenfallenden Punkten sind die Dividierten Differenzen mithilfe entsprechender Ableitungen definiert. Allerdings steht dies nicht im Einklang mit dem ableitungsfreien Charakter des Verfahrens. In der Praxis treten
jedoch solche und andere Ausnahmefa&#x0308;lle (\(\alpha _\ell = 0\), \(z_{\ell +1} = \infty \)) sehr selten auf.
</p>
<p>
Fu&#x0308;r glatte Funktionen konvergiert Mu&#x0308;llers Verfahren lokal fast mit Ordnung \(2\).
</p>


<h3 id="schranken-fuer-nullstellen-von-polynonmen">Schranken für Nullstellen von Polynonmen</h3>

</p>

<p>
Die <b>Betra&#x0308;ge der Nullstellen eines Polynoms</b> \(p(x) = a_n x^n + \dotsb + a_1 x + a_0\) ko&#x0308;nnen abgescha&#x0308;tzt werden:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                 n−1
                                                                                                                                     ¨                             «
                                                                                                                                 X   |ai |
                                                                                                                    |x| ≤ max 1,                                        .
                                                                                                                                 i=0
                                                                                                                                     |a n|




-->


<p>

\begin{align*}
|x| \le \max \left \{1, \sum _{i=0}^{n-1} \frac {|a_i|}{|a_n|}\right \}.
\end{align*}

</p>


<h3 id="sturmsche-kette"><span style="font-variant: small-caps;">Sturm</span>sche Kette</h3>

</p>

<p>
Die Polynomfolge \(p_n, p_{n-1}, \dotsc , p_m\) bildet eine <b><span class="textsc" >Sturm</span>sche Kette</b>, falls
</p>
<ul style="list-style-type:none">

<li class="list-item-f3"><p>alle reellen Nullstellen von \(p_n\) einfach sind,
</p>
</li>
<li class="list-item-f4"><p>\(p_m\) sein Vorzeichen nicht a&#x0308;ndert,
</p>
</li>
<li class="list-item-f5"><p>\(p_{n-1}\) an allen reellen Nullstellen von \(p_n\) ein anderes Vorzeichen als \(p_n’\) hat und
</p>
</li>
<li class="list-item-f6"><p>\(p_{k+1}(x) p_{k-1}(x) &lt; 0\) fu&#x0308;r alle reellen Nullstellen \(x\) von \(p_k\), \(k = n - 1, \dotsc , m + 1\) ist.
</p>
</li>
</ul>
<p>
Eine Sturmsche Kette kann zur Nullstellen-Bestimmung von Polynomen verwendet werden: Bezeichnet \(s(x)\) die Anzahl der Vorzeichenwechsel der Folge \(p_n(x), \dotsc , p_m(x)\), dann ist die Anzahl der reellen
Nullstellen von \(p_n\) im Intervall \(\left [a, b\right )\) gleich der Differenz \(s(b) - s(a)\).
</p>
<p>
Diese Eigenschaft bildet die Grundlage fu&#x0308;r ein Bisektionsverfahren: Man beginnt mit einem Intervall, dass alle reelle Nullstellen von \(p_n\) entha&#x0308;lt. Durch fortgesetzte Unterteilung, gema&#x0308;ß der Anzahl
der Vorzeichenwechsel der Kette an den Teilintervall-Endpunkten, ko&#x0308;nnen so alle reellen Nullstellen von \(p_n\) bestimmt werden.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Zu einem beliebigen Polynom \(q_n\) kann man eine Sturmsche Kette \(p_n, \dotsc , p_m\) mithilfe des Euklidischen Algorithmus konstruieren. Man bestimmt dazu zuna&#x0308;chst den gro&#x0308;ßten gemeinsamen
Teiler \(q_m\) von \(q_n\) und \(q_{n-1} := -q_n’\) durch sukzessive Polynomdivision
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                        qk+1 = rk qk − qk−1 ,                    k = n − 1, . . . , m



-->


<p>

\begin{align*}
q_{k+1} = r_k q_k - q_{k-1}, \quad k = n - 1, \dotsc , m
\end{align*}
mit \(q_{m-1} = 0\) und setzt anschließend \(p_k := q_k / q_m\).
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Die charakteristischen Polynome der symmetrischen Tridiagonalmatrix
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                                                              
                                                                                                                 a1       b1                                       0
                                                                                                                b1       a2      b2                                           
                                                                                                                                  ..             ..
                                                                                                                                                                              
                                                                                                               
                                                                                                                         b2         .                .                        
                                                                                                                                                                               
                                                                                                               
                                                                                                                                  ..                                           
                                                                                                               
                                                                                                                                      .                                       
                                                                                                                                                                               
                                                                                                                                                                bn−1          
                                                                                                                    0                          bn−1               an


-->


<p>

\begin{align*}
\begin{pmatrix} a_1 &amp; b_1 &amp; &amp; &amp; 0 \\ b_1 &amp; a_2 &amp; b_2 \\ &amp; b_2 &amp; \ddots &amp; \ddots \\ &amp; &amp; \ddots \\ &amp; &amp;
&amp; &amp; b_{n-1} \\ 0 &amp; &amp; &amp; b_{n-1} &amp; a_n \end {pmatrix}
\end{align*}
erfu&#x0308;llen die Rekursion
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                 pn+1 (λ) = (an+1 − λ)pn (λ) − bn2 pn−1 (λ),                                             n∈N



-->


<p>

\begin{align*}
p_{n+1}(\lambda ) = (a_{n+1} - \lambda ) p_n(\lambda ) - b_n^2 p_{n-1}(\lambda ), \quad n \in \natural
\end{align*}
mit \(p_0(\lambda ) = 1\) und \(p_1(\lambda ) = a_1 - \lambda \). Sind alle Nebendiagonalelemente \(b_k\) ungleich Null, so sind die Nullstellen wie bei orthogonalen Polynomen geschachtelt und die Folge
\(p_n, \dotsc , p_0\) bildet eine Sturmsche Kette. Mithilfe Sturmscher Ketten ko&#x0308;nnen somit die Eigenwerte beliebiger symmetrischer tridiagonaler Matrizen bestimmt werden.
</p>


<h3 id="nullstellenbestimmung-mit-matlab">Nullstellenbestimmung mit MATLAB</h3>

</p>

<p>
Nullstellen einer reellen Funktion <span class="inlineprogramlisting"></span>f ko&#x0308;nnen in M ATLAB mit <span class="inlineprogramlisting">x = fzero(f, x0</span>); bestimmt
werden. <span class="inlineprogramlisting"></span>f ist dabei als Funktionshandle, Funktionsname und Inline-Funktion gegeben. <span class="inlineprogramlisting"></span>x0 ist ein Punkt, in
dessen Umgebung eine Nullstelle gefunden werden soll. Statt eines Punktes kann auch ein Intervall <span class="inlineprogramlisting">[a, b</span>] u&#x0308;bergeben werden. Der verwendete Algorithmus
basiert auf einer Kombination von Bisektion und inverser quadratischer Interpolation. Wird kein Intervall u&#x0308;bergeben, so wird zuna&#x0308;chst ein Intervall mit einem Vorzeichenwechsel der Funktion an den Endpunkten
bestimmt. Kann kein solches Intervall bestimmt werden, wird ein Fehler ausgegeben und <span class="inlineprogramlisting"></span>NaN zuru&#x0308;ckgegeben.
</p>
<p>
Komplexe Nullstellen ko&#x0308;nnen nicht gefunden werden.<br />
Fu&#x0308;r Polynome <span class="inlineprogramlisting"></span>p (\(p(z) = p_1 z^n + \dotsb + p_n z + p_{n+1}\)) schafft hier der Befehl <span
class="inlineprogramlisting">z = roots(c</span>); Abhilfe. Hier werden alle Nullstellen (reell wie komplex) bestimmt.
</p>


<h2 id="nicht-lineare-systeme">Nicht-lineare Systeme</h2>

</p>

<h3 id="nicht-lineares-gleichungssystem">Nicht-lineares Gleichungssystem</h3>

</p>

<p>
Ein <b>nicht-lineares Gleichungssystem</b> hat die Form
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                                                  f1 (x 1 , . . . , x n )                         =        0
                                                                                                                                 
                                                                                                                                                                                   ..
                                                                                                   f (x) = 0            ⇔                                                           .
                                                                                                                                           f m (x 1 , . . . , x n )                =
                                                                                                                                 
                                                                                                                                                                                            0


-->


<p>

\begin{align*}
f(x) = 0 \quad \Leftrightarrow \quad \left \{\begin{array}{rcl} f_1(x_1, \dotsc , x_n) &amp; = &amp; 0 \\ &amp; \vdots \\ f_m(x_1, \dotsc , x_n) &amp; =
&amp; 0 \end {array}\right .
\end{align*}
mit Unbekannten \(x_i\) und gegebenen Funktionen \(f_j\) (\(i = 1, \dotsc , n\), \(j = 1, \dotsc , m\)).
</p>
<p>
Im Gegensatz zu linearen Gleichungssystemen (LGS) ko&#x0308;nnen keine generellen Aussagen u&#x0308;ber die Lo&#x0308;sbarkeit eines nicht-linearen Gleichungssystems gemacht werden. Im Allgemeinen existieren jedoch
fu&#x0308;r \(m = n\) nur endlich viele Lo&#x0308;sungen. Fu&#x0308;r \(m &gt; n\) ist das System normalerweise u&#x0308;berbestimmt. Es existiert keine Lo&#x0308;sung und man spricht von einem nicht-linearen
Ausgleichsproblem. Fu&#x0308;r \(m &lt; n\) ist das System i.&#x202f;A. unterbestimmt, d.&#x202f;h. Unbekannte \(x_j\) ko&#x0308;nnen frei gewa&#x0308;hlt werden.
</p>


<h3 id="banachscher-fixpunktsatz"><span style="font-variant: small-caps;">Banach</span>scher Fixpunktsatz</h3>

</p>

<p>
Sei \(g\colon D \rightarrow \real ^n\) mit \(D \subset \real ^n\), \(D \not = \emptyset \) gegeben. Ist \(D\) abgeschlossen (\(D = \overline {D}\)), wird \(D\) von \(g\) in sich selbst
abgebildet (\(g(D) \subset D\)) und ist \(g\) eine Kontraktion (\(\norm {g(x) - g(y)} \le c \norm {x - y}\) fu&#x0308;r alle \(x, y \in D\), wobei \(0 \le c &lt; 1\)), so besitzt \(g\) einen
eindeutigen Fixpunkt \(x_\ast = g(x_\ast ) \in D\).
</p>
<p>
Ausgehend von einem beliebigen Punkt \(x_0 \in D\) kann \(x_\ast \) durch die Folge
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                         x0,        x 1 = g(x 0 ),             x 2 = g(x 1 ),                      ...



-->


<p>

\begin{align*}
x_0, \quad x_1 = g(x_0), \quad x_2 = g(x_1), \quad \dotsc
\end{align*}
approximiert werden. Fu&#x0308;r den Fehler gilt dabei
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                        ck
                                                                                                                kx ∗ − x k k ≤             kx 1 − x 0 k ,
                                                                                                                                       1−c


-->


<p>

\begin{align*}
\norm {x_\ast - x_k} \le \frac {c^k}{1 - c} \norm {x_1 - x_0},
\end{align*}
d.&#x202f;h. die Iterationsfolge konvergiert fu&#x0308;r jeden Startwert linear.
</p>
<p>
Der Fixpunktsatz gilt auch allgemein in vollsta&#x0308;ndigen metrischen Ra&#x0308;umen. Dabei wird die Norm \(\norm {x - y}\) einfach durch die Abstandsfunktion \(d(x, y)\) ersetzt.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Zum Nachweis der Kontraktionseigenschaft von differenzierbaren Abbildungen \(g\) benutzt man oft den <b>Mittelwertsatzes (Satz von <span class="textsc" >Lagrange</span>)</b>. Fu&#x0308;r \(n = 1\)
lautet dieser
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--
                                                                                           g( y) − g(x) = g 0 (z) · ( y − x)                     für ein      z ∈ x, y.



-->


<p>

\begin{align*}
g(y) - g(x) = g’(z) \cdot (y - x) \quad \text {fÃijr ein}\quad z \in \overline {x,y}.
\end{align*}
La&#x0308;sst sich nun die Ableitung nach oben abscha&#x0308;tzen, d.&#x202f;h. \(g’(z) \le \max _{z \in D} |g’(z)| = c\) mit \(c &lt; 1\), so ist die Kontraktionseigenschaft nachgewiesen.
</p>
<p>
Fu&#x0308;r \(n \ge 2\) bedient man sich des <b>verallgemeinerten Mittelwertsatzes</b>:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                           1
                                                                                             g( y) − g(x) =                     g 0 (x + t( y − x))( y − x) dt.
                                                                                                                        0




-->


<p>

\begin{align*}
g(y) - g(x) = \int _0^1 g’(x + t(y - x))(y - x) \dt .
\end{align*}
Hier muss die Norm der Jacobi-Matrix nach oben abgescha&#x0308;tzt werden:<br />
\(\norm {g’(x + t(y - x))} \le \max _{z \in D} \norm {g’(z)} = c\). Fu&#x0308;r \(c &lt; 1\) liegt wieder eine Kontraktion vor.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Eine typische Anwendung ist ein leicht gesto&#x0308;rtes System \(Ax + \varepsilon f(x) = b\) mit einer quadratischen Matrix \(A\). Die Funktion \(f(x)\) stellt die Sto&#x0308;rung dar und besitzt eine komplizierte
Abha&#x0308;ngigkeit von \(x\). Fu&#x0308;r hinreichend kleine \(\varepsilon \) dominiert aber lineare Anteil und das System kann mit der Iteration \(x \leftarrow g(x) := A^{-1} (b - \varepsilon
f(x))\) gelo&#x0308;st werden. Man nimmt dabei an, dass \(A\) invertierbar und \(f\) Lipschitz-stetig mit Konstante \(c_f\) ist.
</p>
<p>
Zur U&#x0308;berpru&#x0308;fung der Voraussetzungen des Banachschen Fixpunktsatzes wa&#x0308;hlt man<br />
\(D := \overline {U_r(p)} = \{y \in \real ^n \;|\; \norm {y - p} \le r\}\) mit \(p = A^{-1} b\), denn fu&#x0308;r kleines \(\varepsilon \) liegt die Lo&#x0308;sung nahe bei der
Lo&#x0308;sung von \(Ax = b\). Fu&#x0308;r \(x \in D\) beliebig gilt dann \(\norm {g(x) - p} = \varepsilon \norm {A^{-1} f(x)} \le \varepsilon \norm {A^{-1}} \max _{y \in D}
\norm {f(y)}\). Wa&#x0308;hlt man also \(\varepsilon \le \frac {r}{\norm {A^{-1}} \max _{y \in D} \norm {f(y)}}\), so gilt \(g(x) \in D\), d.&#x202f;h. \(g\) bildet \(D\) in sich selbst
ab. Fu&#x0308;r die Kontraktionskonstante gilt \(\norm {g(x) - g(y)} = \varepsilon \norm {A^{-1} (f(x) - f(y))} \le \varepsilon \norm {A^{-1}} c_f \norm {x - y} = c \norm
{x - y}\) mit \(c := \varepsilon \norm {A^{-1}} c_f\). Es gilt \(c &lt; 1\), falls \(\varepsilon &lt; \frac {1}{\norm {A^{-1}} c_f}\) gilt. Fu&#x0308;r ein hinreichend kleines
\(\varepsilon &gt; 0\) sind beide Bedingungen erfu&#x0308;llt und der Fixpunktsatz von Banach la&#x0308;sst sich anwenden.
</p>


<h3 id="multivariates-newton-verfahren">Multivariates <span style="font-variant: small-caps;">Newton</span>-Verfahren</h3>

</p>

<p>
Ein Iterationsschritt \(x \rightarrow y\) der Newton-Iteration zur Bestimmung einer Nullstelle \(x_\ast \) eines nicht-linearen Gleichungssystems \(f_k(x_1, \dotsc , x_n) = 0\) fu&#x0308;r \(k = 1,
\dotsc , n\) hat die Form
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                    ∆ := − f 0 (x)−1 f (x),                      y := x + ∆.



-->


<p>

\begin{align*}
\Delta := -f’(x)^{-1} f(x), \qquad y := x + \Delta .
\end{align*}
Dabei wird die Jacobi-Matrix \(f’(x)\) nicht invertiert, sondern das Inkrement \(\Delta \) wird als Lo&#x0308;sung des LGS \(f’(x) \Delta = -f(x)\) bestimmt.
</p>
<p>
Fu&#x0308;r \(\det f’(x_\ast ) \not = 0\) konvergiert die Newton-Iteration lokal quadratisch, d.&#x202f;h.
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                   k y − x ∗ k ≤ c kx − x ∗ k2                   für     x ≈ x∗.



-->


<p>

\begin{align*}
\norm {y - x_\ast } \le c \norm {x - x_\ast }^2 \quad \text {fÃijr}\quad x \approx x_\ast .
\end{align*}

</p>


<h3 id="kantorovich-kriterium"><span style="font-variant: small-caps;">Kantorovich</span>-Kriterium</h3>

</p>

<p>
Das <b><span class="textsc" >Kantorovich</span>-Kriterium</b> gibt eine hinreichende Bedingung fu&#x0308;r die Konvergenz des Newton-Verfahrens fu&#x0308;r ein System nicht-linearer Gleichungen
\(f_k(x_1, \dotsc , x_n) = 0\), \(k = 1, \dotsc , n\).<br />
Gilt fu&#x0308;r einen Startvektor \(y\)
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                       r
                                                                                                                     f 0 ( y)−1 f ( y) ≤
                                                                                                                                       2
                                                                                                                                       1
                                                                                                     f 0 ( y)−1 ( f 0 (x) − f 0 (e
                                                                                                                                 x )) ≤ kx − e
                                                                                                                                             xk
                                                                                                                                       r


-->


<p>

\begin{align*}
\norm {f’(y)^{-1} f(y)} &amp; \le \frac {r}{2} \\ \norm {f’(y)^{-1} (f’(x) - f’(\widetilde {x}))} &amp; \le \frac {1}{r} \norm {x - \widetilde {x}}
\end{align*}
fu&#x0308;r alle \(x, \widetilde {x} \in U_r(y)\), dann existiert eine Lo&#x0308;sung \(x_\ast \) des Systems in \(\overline {U_r(y)}\) und das Newton-Verfahren mit Startvektor \(y\) konvergiert gegen
\(x_\ast \).
</p>


<h3 id="fortsetzungsmethode">Fortsetzungsmethode</h3>

</p>

<p>
Bei einem von einem Parameter \(t\) abha&#x0308;ngigen nicht-linearen Gleichungssystem<br />
\(f_k(x_1, \dotsc , x_n, t) = 0\), \(k = 1, \dotsc , n\) kann eine Lo&#x0308;sung \(x(t)\) fu&#x0308;r kleines \(\Delta t\) als Na&#x0308;herung fu&#x0308;r \(x(t + \Delta t)\) verwendet
werden. Ist die Jacobi-Matrix von \(f\) bzgl. \(x\) bei \(x(t)\) invertierbar, so erha&#x0308;lt man durch die lineare Taylor-Entwicklung
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                              x(t + ∆t) ≈ x(t) − f x (x(t), t)−1 f t (x(t), t)∆t



-->


<p>

\begin{align*}
x(t + \Delta t) \approx x(t) - f_x(x(t), t)^{-1} f_t(x(t), t) \Delta t
\end{align*}
eine verbesserte Approximation (<b>Fortsetzungsmethode</b>).
</p>
<p>
Die Fortsetzungsmethode kann insbesondere in Kombination mit iterativen Verfahren benutzt werden, um das nicht-lineare Gleichungssystem fu&#x0308;r eine Parameterfolge \(t_0 &lt; t_1 &lt; \dotsb \) zu
lo&#x0308;sen. Die Lo&#x0308;sungen \(x(t_k)\) dienen dabei jeweils als Startwerte zur Berechnung von \(x(t_{k+1})\).
</p>


<h3 id="gedaempftes-newton-verfahren">Gedämpftes <span style="font-variant: small-caps;">Newton</span>-Verfahren</h3>

</p>

<p>
Mit dem Newton-Verfahren wird eine Lo&#x0308;sung \(x_\ast \) eines nicht-linearen Gleichungssystems<br />
\(f_k(x_1, \dotsc , x_n) = 0\), \(k = 1, \dotsc , n\) ausgehend von einer hinreichend guten Startna&#x0308;herung \(x\) approximiert. Beim <b>geda&#x0308;mpften <span class="textsc"
>Newton</span>-Verfahren</b> will man in jedem Fall eine Verkleinerung der Norm des Funktionswerts erreichen. Daher hat ein Iterationsschritt \(x \rightarrow y\) die folgende Form:
</p>
<ul style="list-style-type:none">

<li class="list-item-f7"><p>Das Inkrement \(\Delta x\) wird durch Lo&#x0308;sung des LGS \(f’(x) \Delta x = f(x)\) berechnet.
</p>
</li>
<li class="list-item-f8"><p>Man bestimmt einen <b>Da&#x0308;mpfungsparameter</b> \(\lambda \in \{1, 1/2, 1/4, \dotsc \}\), sodass \(\norm {f(y)}\) fu&#x0308;r \(y = x - \lambda \Delta x\)
signifikant kleiner als \(\norm {f(x)}\) ist.
</p>
</li>
</ul>
<p>
Als Test zur Bestimmung von \(\lambda \) dient der Vergleich
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                              k∆ yk ≤ (1 − λ/2) k∆xk ,                        f 0 (x)∆ y = f ( y)



-->


<p>

\begin{align*}
\norm {\Delta y} \le (1 - \lambda /2) \norm {\Delta x}, \quad f’(x) \Delta y = f(y)
\end{align*}
mit einer geeignet gewa&#x0308;hlten Norm \(\norm {\cdot }\). Dabei kann eine bereits bestimmte LR- oder QR-Zerlegung der Jacobi-Matrix \(f’(x)\) zur schnelleren Berechnung von \(\Delta y\) benutzt werden.
Durch die Multiplikation der Funktionswerte mit \(f’(x)^{-1}\) wird der Vergleich affin invariant. Insbesondere ist damit auch das geda&#x0308;mpfte Newton-Verfahren skalierungsunabha&#x0308;ngig, was in einem
Vergleich der Form \(\norm {f(y)} \le (1 - \rho ) \norm {f(x)}\) nicht gewa&#x0308;hrleistet wa&#x0308;re.
</p>
<p>
Bei der Implementierung empfiehlt es sich, \(\lambda \) nicht abrupt zu a&#x0308;ndern. Man beginnt den Vergleich mit dem zuletzt gewa&#x0308;hlten Da&#x0308;mpfungsparameter. Entsprechend dem Result der Abfrage
wird \(\lambda \) halbiert oder verdoppelt, wobei eine Verdoppelung dann fru&#x0308;hestens im na&#x0308;chsten Iterationsschritt umgesetzt wird. Ist die Jacobi-Matrix fu&#x0308;r die approximierte Lo&#x0308;sung
\(x_\ast \) regula&#x0308;r, so ist \(\lambda = 1\) fu&#x0308;r Approximationen nahe genug bei \(x_\ast \). Die quadratische Konvergenz wird somit durch die Da&#x0308;mpfung nicht beeintra&#x0308;chtigt.
</p>


<h3 id="gauss-newton-verfahren"><span style="font-variant: small-caps;">Gauß</span>-<span style="font-variant: small-caps;">Newton</span>-Verfahren</h3>

</p>

<p>
Die Lo&#x0308;sung \(x_\ast \in \real ^n\) eines nicht-linearen Ausgleichsproblems
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                          m
                                                                                                                          X
                                                                                          k f (x 1 , . . . , x n )k22 =           | f k (x)|2 → min             (m > n)
                                                                                                                          k=1



-->


<p>

\begin{align*}
\norm {f(x_1, \dotsc , x_n)}_2^2 = \sum _{k=1}^m |f_k(x)|^2 \rightarrow \min \qquad (m &gt; n)
\end{align*}
kann mit der durch
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                              f (x) + f 0 (x)∆x              2
                                                                                                                                                 → min
                                                                                                             x ← x + ∆x



-->


<p>

\begin{align*}
&amp; \norm {f(x) + f’(x) \Delta x}_2 \rightarrow \min \\ &amp; x \leftarrow x + \Delta x
\end{align*}
definierten <b>Gauß-Newton-Iteration</b> bestimmt werden. In jedem Iterationsschritt wird dabei ein lineares Ausgleichsproblem mit der \(m \times n\)-Matrix \(f’(x)\) gelo&#x0308;st.
</p>


<h2 id="minimierung-ohne-nebenbedingungen">Minimierung ohne Nebenbedingungen</h2>

</p>

<h3 id="goldene-suche">Goldene Suche</h3>

</p>

<p>
Ein lokales Minimum einer stetigen Funktion \(f\) kann mithilfe eines Unterteilungsalgorithmus (<b>Goldene Suche</b>, analog der Bisektion bei Nullstellen) bestimmt werden. Man geht dazu von drei Punkten \(a\), \(b\) und
\(c\) mit
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                                  f (a) ≥ f (b) ≤ f (c)



-->


<p>

\begin{align*}
f(a) \ge f(b) \le f(c)
\end{align*}
aus. Es muss mindestens ein lokales Minimum von \(f\) im Intervall \((a, c)\) liegen. Zur Verkleinerung des Intervalls wird \(f\) nun an einem weiterem Punkt \(x\) im gro&#x0308;ßeren der Teilintervalle \((a, b)\) und
\((b, c)\) ausgewertet. Dann wird einer der Eckpunkte durch \(x\) ersetzt, sodass fu&#x0308;r das neue Tripel \(\{a’, b’, c’\}\) wiederum \(f(a’) \ge f(b’) \le f(c’)\) gilt. Die Prozedur wird solange
wiederholt, bis eine vorgegebene Genauigkeit erreicht ist.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Die optimale Unterteilung der Intervalle erfolgt im Verha&#x0308;ltnis (<b>Goldener Schnitt</b>)
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                      p
                                                                                                                                       5−1
                                                                                                r : (1 − r)        mit           r :=      ≈ 0.61803.
                                                                                                                                        2


-->


<p>

\begin{align*}
r : (1 - r) \quad \text {mit}\quad r := \frac {\sqrt {5} - 1}{2} \approx 0.61803.
\end{align*}
Der Parameter \(r\) ist die positive Lo&#x0308;sung der Gleichung \(r^2 = 1 - r\). Gilt \((c’ - a’) = r(c - a)\), so wird durch dieses Teilverha&#x0308;ltnis eine konstante Reduktion der Intervallla&#x0308;nge
pro Schritt unabha&#x0308;ngig von dem Vergleich zwischen \(f(x)\) und \(f(b)\) erreicht.<br />
Fu&#x0308;r beliebige Startpunkte \(a \le b \le c\) ist die Intervallla&#x0308;nge nach \(n\) Schritten \(\le r^{n-1} (c - a)\).
</p>


<h3 id="quadratische-suche">Quadratische Suche</h3>

</p>

<p>
Aus Approximationen \(x_{\ell -2}\), \(x_{\ell -1}\), \(x_\ell \) fu&#x0308;r das Minimum \(x_\ast \) einer Funktion \(f\) mit<br />
\(\Delta (x_\ell , x_{\ell -1}, x_{\ell -2}) f &gt; 0\) kann eine verbesserte Approximation durch Minimierung der interpolierenden Parabel bestimmt werden (<b>quadratische Suche</b>):
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                           1                   ∆(x ` , x `−1 ) f
                                                                                                                                                      
                                                                                              x `+1 :=         x ` + x `−1 −                             .
                                                                                                           2                 ∆(x ` , x `−1 , x `−2 ) f


-->


<p>

\begin{align*}
x_{\ell +1} := \frac {1}{2} \left (x_\ell + x_{\ell -1} - \frac {\Delta (x_\ell , x_{\ell -1}) f} {\Delta (x_\ell , x_{\ell -1}, x_{\ell -2}) f}\right ).
\end{align*}
Fallen Punkte \(x_k\) zusammen, so sind die auftretenden Divideren Differenzen mithilfe der entsprechenden Ableitungswerte zu berechnen.
</p>
<p>
Ist \(f”(x_\ast ) &gt; 0\), dann ist die quadratische Suche lokal konvergent. Insbesondere ist<br />
\(\Delta (x_\ell , x_{\ell -1}, x_{\ell -2}) f &gt; 0\) fu&#x0308;r \(x_k\) nahe bei \(x_\ast \), sodass die Methode immer durchfu&#x0308;hrbar ist.
</p>


<h3 id="steilster-abstieg">Steilster Abstieg</h3>

</p>

<p>
Die <b>Methode des steilsten Abstiegs</b> dient zur Minimierung multivariater Funktionen<br />
\(f\colon \real ^n \rightarrow \real \). Zur Durchfu&#x0308;hrung eines Iterationsschrittes wird zuna&#x0308;chst der negative Gradient
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                                    d := − grad f (x)



-->


<p>

\begin{align*}
d := -\grad f(x)
\end{align*}
als lokal beste Abstiegsrichtung berechnet. Dann wird \(y\) als eine Minimalstelle von \(f\) in Richtung von \(d\) bestimmt:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                                f ( y) = min f (x + t d).
                                                                                                                                 t≥0




-->


<p>

\begin{align*}
f(y) = \min _{t \ge 0} f(x + td).
\end{align*}
Die Suchrichtung ist dabei orthogonal zu der Niveaumenge durch \(x\) und beru&#x0308;hrt eine Niveaumenge zu einem kleineren Funktionswert in \(y\).
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Die Konvergenz der durch die Methode des steilsten Abstiegs erzeugten Folge \(x_0, x_1, \dotsc \) kann unter sehr allgemeinen Voraussetzungen gezeigt werden. Hinreichend ist, dass \(f\) nach unten beschra&#x0308;nkt
ist und \(\grad f\) in einer Umgebung \(U\) der Menge \(\{x \in \real ^n \;|\; f(x) \le f(x_0)\}\) Lipschitz-stetig ist, d.&#x202f;h.
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                           kgrad f (x) − grad f (e
                                                                                                                 x )k ≤ L kx − e
                                                                                                                               xk ,                              x ∈ U.
                                                                                                                                                              x, e



-->


<p>

\begin{align*}
\norm {\grad f(x) - \grad f(\widetilde {x})} \le L \norm {x - \widetilde {x}}, \quad x, \widetilde {x} \in U.
\end{align*}
Dann gilt
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                              ∞
                                                                                                              X
                                                                                                                    kgrad f (x ` )k2 < ∞.
                                                                                                              `=0



-->


<p>

\begin{align*}
\sum _{\ell =0}^\infty \norm {\grad f(x_\ell )}^2 &lt; \infty .
\end{align*}
Dies impliziert insbesondere, dass jeder Ha&#x0308;ufungspunkt der Folge \(x_0, x_1, \dotsc \) ein kritischer Punkt von \(f\) ist. Dass es sich um ein lokales Minimum handelt, ist statistisch gesehen fast sicher, kann jedoch
nicht zwingend gefolgert werden.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Im Algorithmus braucht die eindimensionale Minimierung nur na&#x0308;herungsweise durchgefu&#x0308;hrt werden. Die Suchrichtung \(d\) muss nicht als der negative Gradient gewa&#x0308;hlt und eine globale Minimalstelle
\(y\) nicht bestimmt werden. Entscheidend fu&#x0308;r Konvergenz ist lediglich, dass in jedem Iterationsschritt eine Reduktion des Funktionswerts proportional zu \(\norm {\grad f(x)}^2\) erreicht wird.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Fu&#x0308;r eine symmetrische Matrix \(A\), einen Vektor \(b\) und einen Skalar \(c\) wird durch
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                1 t
                                                                                                              q(x) =              x Ax + b t x + c
                                                                                                                                2


-->


<p>

\begin{align*}
q(x) = \frac {1}{2} x^t A x + b^t x + c
\end{align*}
eine <b>quadratische Funktion</b> definiert.
</p>
<p>
Ist \(A\) symmetrisch und positiv definit, so kann man fu&#x0308;r die quadratische Funktion<br />
\(f(x) = \frac {1}{2} x^t A x - b^t x\) ein Iterationsschritt \(x \rightarrow y\) der Methode des steilsten Abstiegs explizit angeben. Es ist \(y = x + td\) mit \(d = -\grad f(x) = b - Ax\) und
\(t = \frac {d^t d}{d^t A d}\), denn das Minimum von \(f(x + td) = \frac {1}{2} (x + td)^t A (x + td) - b^t (x + td) = \frac {1}{2} d^t A d t^2 + (x^t A d - b^t d)
t + c\) kann durch Nullsetzen der Ableitung nach \(t\) bestimmt werden: \(0 = d^t A d t - (b - Ax)^t d = d^t A d t - d^t d\).
</p>
<p>
Es kann zu unerwu&#x0308;nschten Oszillationen kommen, wenn \(A\) Eigenwerte stark unterschiedlicher Gro&#x0308;ßenordnung besitzt. Fu&#x0308;r
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                                           
                                                                                                         1      0                         0                  100
                                                                                                A=                  ,             b=         ,         x=
                                                                                                         0     100                        0                   1


-->


<p>

\begin{align*}
A = \begin{pmatrix}1 &amp; 0\\0 &amp; 100\end {pmatrix}, \quad b = \begin{pmatrix}0\\0\end {pmatrix}, \quad x = \begin{pmatrix}100\\1\end {pmatrix}
\end{align*}
verringert sich zum Beispiel in jedem Iterationsschritt der Abstand zum Minimum im Ursprung nur um weniger als \(1\) Prozent.
</p>


<h3 id="kantorovich-ungleichung"><span style="font-variant: small-caps;">Kantorovich</span>-Ungleichung</h3>

</p>

<p>
Sei \(x \rightarrow y\) ein Schritt bei der Minimierung der quadratischen Funktion
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                  1 t
                                                                                                                 f (x) =            x Ax − b t x
                                                                                                                                  2


-->


<p>

\begin{align*}
f(x) = \frac {1}{2} x^t A x - b^t x
\end{align*}
mit symmetrischer, positiv definiter Matrix \(A\) durch die Methode des steilsten Abstiegs.<br />
Dann gilt die <b><span class="textsc" >Kantorovich</span>-Ungleichung</b>:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                   κ−1
                                                                                                          k y − x ∗ kA ≤               kx − x ∗ kA .
                                                                                                                                   κ+1


-->


<p>

\begin{align*}
\norm {y - x_\ast }_A \le \frac {\kappa - 1}{\kappa + 1} \norm {x - x_\ast }_A.
\end{align*}
Dabei bezeichnet \(x_\ast := A^{-1} b\) die Lo&#x0308;sung, die Kondition \(\kappa := \lambda _{\max } / \lambda _{\min }\) den Quotienten der extremalen Eigenwerte von \(A\) und \(\norm
{z}_A := \sqrt {z^t A z}\) die von \(A\) induzierte Norm.
</p>


<h3 id="einschub-konjugierte-gradienten-cg-verfahren">Einschub: Konjugierte Gradienten (cg-Verfahren)</h3>

</p>

<p>
<b>\(A\)-Skalarprodukt</b>: Zu einer symmetrischen, positiv definiten Matrix \(A\) la&#x0308;sst sich durch
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                                     〈x, y〉A := x t Ay



-->


<p>

\begin{align*}
\innerproduct {x, y}_A := x^t A y
\end{align*}
ein Skalarprodukt definieren.<br />
Zwei Vektoren \(x\) und \(y\) heißen <b>\(A\)-orthogonal</b>, falls \(\innerproduct {x, y}_A = 0\) ist.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
<b>konjugierte Gradienten</b>: Ausgehend von einem beliebigen Startvektor \(x_0\) und<br />
\(g_0 := -u_1 := Ax_0 - b\) erha&#x0308;lt man mit der Iteration
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                       〈g`−1 , g`−1 〉
                                                                                                              x ` := x `−1 +                          u`
                                                                                                                                        〈u` , u` 〉A
                                                                                                              g` := Ax ` − b
                                                                                                                                         〈g` , g` 〉
                                                                                                          u`+1 := −g` +                               u`
                                                                                                                                       〈g`−1 , g`−1 〉


-->


<p>

\begin{align*}
x_\ell &amp; := x_{\ell -1} + \frac {\innerproduct {g_{\ell -1}, g_{\ell -1}}}{\innerproduct {u_\ell , u_\ell }_A} u_\ell \\ g_\ell &amp; := Ax_\ell - b \\
u_{\ell +1} &amp; := -g_\ell + \frac {\innerproduct {g_\ell , g_\ell }}{\innerproduct {g_{\ell -1}, g_{\ell -1}}} u_\ell
\end{align*}
die Lo&#x0308;sung des linearen LGS \(Ax = b\) mit symmetrischer, positiv definiter \(n \times n\)-Matrix \(A\) und \(\innerproduct {\cdot , \cdot }_A\) dem \(A\)-Skalarprodukt in maximal \(n\)
Schritten.<br />
Bei exakter Rechnung ist \(g_\ell = 0\) spa&#x0308;testens fu&#x0308;r \(\ell = n\). Dieses Verfahren nennt man die <b>Methode der konjugierten Gradienten (cg-Verfahren)</b>.
</p>
<p>
Mit \(f(x) = \frac {1}{2} x^t A x - b^t x\) ist \(f(x)\) minimal genau dann, wenn \(Ax = b\) (es gilt \(f’(x) = Ax - b\)), d.&#x202f;h. man kann das Verfahren auch als Minimierung der quadratischen
Funktion \(f\) auffassen.
</p>
<p>
Fu&#x0308;r die Gradienten \(g_\ell \) und die Suchrichtungen \(u_\ell \) gilt
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                                       〈g`−1 , g`−1 〉
                                                                                              g` = g`−1 + α` Au`                  mit α` :=                           ,
                                                                                                                                                        〈u` , u` 〉A


-->


<p>

\begin{align*}
g_\ell = g_{\ell -1} + \alpha _\ell A u_\ell \quad \text {mit}\quad \alpha _\ell := \frac {\innerproduct {g_{\ell -1}, g_{\ell -1}}}{\innerproduct {u_\ell
, u_\ell }_A},
\end{align*}
d.&#x202f;h. es ist mo&#x0308;glich, bei der Implementierung eines Iterationsschritts<br />
\((x_{\ell -1}, g_{\ell -1}, u_\ell ) \rightarrow (x_\ell , g_\ell , u_{\ell +1})\) mit nur einer Matrix-Multiplikation (\(Au_\ell \)) auszukommen.
</p>


<h3 id="konjugierte-gradienten-von-fletcher-und-reeves">Konjugierte Gradienten von <span style="font-variant: small-caps;">Fletcher</span> und <span style="font-variant: small-caps;">Reeves</span></h3>

</p>

<p>
Das Verfahren der konjugierten Gradienten bestimmt das Minimum einer quadratischen Funktion \(f(x) = \frac {1}{2} x^t A x - b^t x\) mit einer symmetrischen, positiv definiten \(n \times n\)-Matrix \(A\)
bei exakter Rechnung in ho&#x0308;chstens \(n\) Schritten. <b><span class="textsc" >Fletcher</span> und <span class="textsc" >Reeves</span></b> formulierten den Algorithmus um, sodass dieser
auf beliebige, glatte Funktion \(f\) angewendet werden kann.
</p>
<p>
Ausgehend von Startwerten
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                  x0,         g0 := grad f (x 0 ),                     d0 := −g0



-->


<p>

\begin{align*}
x_0, \qquad g_0 := \grad f(x_0), \qquad d_0 := -g_0
\end{align*}
erzeugt man eine Folge von Na&#x0308;herungen \(x_\ell \) fu&#x0308;r eine Minimalstelle und Suchrichtungen \(d_\ell \) durch folgende Rekursionen:
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                              x `+1 := x ` + α` d`
                                                                                              g`+1 := grad f (x `+1 )
                                                                                                                                                       〈g`+1 , g`+1 〉
                                                                                              d`+1 := −g`+1 + β` d` ,                      β` :=                      ,
                                                                                                                                                         〈g` , g` 〉


-->


<p>

\begin{align*}
x_{\ell +1} &amp; := x_\ell + \alpha _\ell d_\ell \\ g_{\ell +1} &amp; := \grad f(x_{\ell +1}) \\ d_{\ell +1} &amp; := -g_{\ell +1} + \beta _\ell d_\ell ,
\qquad \beta _\ell := \frac {\innerproduct {g_{\ell +1}, g_{\ell +1}}}{\innerproduct {g_\ell , g_\ell }},
\end{align*}
wobei \(\alpha _\ell &gt; 0\) bestimmt ist durch Minimierung von \(f(x_\ell + \alpha d_\ell )\) fu&#x0308;r \(\alpha &gt; 0\).
</p>
<p>
Der einzige Unterschied zum quadratischen Fall ist, dass \(\alpha _\ell \) nicht explizit bestimmt werden kann.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Eine gute Performance kann besonders dann erzielt werden, wenn \(f\) gut durch eine quadratische konvexe Funktion approximiert wird. Ist dies nicht der Fall, so sollte in geeigneten Absta&#x0308;nden ein Neustart des
Verfahrens erfolgen. Die Konvergenzgeschwindigkeit steigt in der Na&#x0308;he des Minimums rapide an (in der Na&#x0308;he des Minimums a&#x0308;hnelt jede Funktion stark einer quadratischen Funktion).
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Die eindimensionale Minimierung wird i.&#x202f;A. nicht exakt durchgefu&#x0308;hrt. Dann ist jedoch darauf zu achten, dass
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--



                                                                                                                    〈g`+1 , d`+1 〉 < 0,



-->


<p>

\begin{align*}
\innerproduct {g_{\ell +1}, d_{\ell +1}} &lt; 0,
\end{align*}
d.&#x202f;h. die Suchrichtungen sind lokale Abstiegsrichtungen. Ist \(x_{\ell +1}\) ein lokales Minimum von \(f\) in Richtung \(d_\ell \), so gilt \(\innerproduct {g_{\ell +1}, d_\ell } = 0\), sodass
aufgrund der Definition von \(d_{\ell +1}\) diese Bedingung automatisch erfu&#x0308;llt ist.
</p>
<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>
<p>
Es existieren einige Varianten bei der Parameterwahl, die ebenfalls mit dem quadratischen Fall konsistent sind. Beispielsweise definieren <span class="textsc" >Polak</span> und <span class="textsc"
>Ribiere</span>
</p>
<span class="hidden" > \(\seteqnumber{0}{}{0}\)</span>


<!--


                                                                                                                                〈g`+1 − g` , d` 〉
                                                                                                                β` :=                             .
                                                                                                                                   〈g` , d` 〉


-->


<p>

\begin{align*}
\beta _\ell := \frac {\innerproduct {g_{\ell +1} - g_\ell , d_\ell }}{\innerproduct {g_\ell , d_\ell }}.
\end{align*}
Diese Wahl fu&#x0308;hrt in der Praxis oft zu besseren Ergebnissen als die klassische Variante von Fletcher und Reeves.
</p>
<h3 id="minimierung-mit-matlab">Minimierung mit MATLAB</h3>

</p>

<p>
Ein lokales Minimum einer reellen Funktion auf einem Intervall \([a, b]\) kann in M ATLAB mit dem Befehl <span class="inlineprogramlisting">[x, fx] = fminbnd(f, a, b</span>); bestimmt werden.
Die Funktion <span class="inlineprogramlisting"></span>f wird als Funktionshandle oder Inline-Funktion u&#x0308;bergeben. Der Ru&#x0308;ckgabewert <span
class="inlineprogramlisting"></span>x entha&#x0308;lt die gefundene Minimalstelle und der optionale Ru&#x0308;ckgabewert <span class="inlineprogramlisting"></span>fx den
entsprechenden Funktionswert. Es werden sowohl lokale Randminima als auch innere lokale Minima gefunden, jedoch nicht immer das globale Minimum.
</p>
<p>
Zur Minimierung multivariater Funktionen steht <span class="inlineprogramlisting">[x, fx] = fminsearch(f, x0</span>); zur Verfu&#x0308;gung. Damit wird ein lokales Minimum in der
Na&#x0308;he eines Startvektors <span class="inlineprogramlisting"></span>x0 gefunden.
</p>
<a id="numerical-analysis-1-autofile-last"></a>
{% endraw %}
</div>
{:/nomarkdown}
