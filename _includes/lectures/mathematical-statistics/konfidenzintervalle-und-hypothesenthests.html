
{::nomarkdown}
<div class="lwarp-contents">
{% raw %}
<div class="hidden" >

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\newcommand {\mathllap }[2][]{{#1#2}}\)

\(\newcommand {\mathrlap }[2][]{{#1#2}}\)

\(\newcommand {\mathclap }[2][]{{#1#2}}\)

\(\newcommand {\mathmbox }[1]{#1}\)

\(\newcommand {\clap }[1]{#1}\)

\(\newcommand {\LWRmathmakebox }[2][]{#2}\)

\(\newcommand {\mathmakebox }[1][]{\LWRmathmakebox }\)

\(\newcommand {\cramped }[2][]{{#1#2}}\)

\(\newcommand {\crampedllap }[2][]{{#1#2}}\)

\(\newcommand {\crampedrlap }[2][]{{#1#2}}\)

\(\newcommand {\crampedclap }[2][]{{#1#2}}\)

\(\newenvironment {crampedsubarray}[1]{}{}\)

\(\newcommand {\crampedsubstack }{}\)

\(\newcommand {\smashoperator }[2][]{#2\limits }\)

\(\newcommand {\adjustlimits }{}\)

\(\newcommand {\SwapAboveDisplaySkip }{}\)

\(\require {extpfeil}\)

\(\Newextarrow \xleftrightarrow {10,10}{0x2194}\)

\(\Newextarrow \xLeftarrow {10,10}{0x21d0}\)

\(\Newextarrow \xhookleftarrow {10,10}{0x21a9}\)

\(\Newextarrow \xmapsto {10,10}{0x21a6}\)

\(\Newextarrow \xRightarrow {10,10}{0x21d2}\)

\(\Newextarrow \xLeftrightarrow {10,10}{0x21d4}\)

\(\Newextarrow \xhookrightarrow {10,10}{0x21aa}\)

\(\Newextarrow \xrightharpoondown {10,10}{0x21c1}\)

\(\Newextarrow \xleftharpoondown {10,10}{0x21bd}\)

\(\Newextarrow \xrightleftharpoons {10,10}{0x21cc}\)

\(\Newextarrow \xrightharpoonup {10,10}{0x21c0}\)

\(\Newextarrow \xleftharpoonup {10,10}{0x21bc}\)

\(\Newextarrow \xleftrightharpoons {10,10}{0x21cb}\)

\(\newcommand {\LWRdounderbracket }[3]{\underset {#3}{\underline {#1}}}\)

\(\newcommand {\LWRunderbracket }[2][]{\LWRdounderbracket {#2}}\)

\(\newcommand {\underbracket }[1][]{\LWRunderbracket }\)

\(\newcommand {\LWRdooverbracket }[3]{\overset {#3}{\overline {#1}}}\)

\(\newcommand {\LWRoverbracket }[2][]{\LWRdooverbracket {#2}}\)

\(\newcommand {\overbracket }[1][]{\LWRoverbracket }\)

\(\newcommand {\LaTeXunderbrace }[1]{\underbrace {#1}}\)

\(\newcommand {\LaTeXoverbrace }[1]{\overbrace {#1}}\)

\(\newenvironment {matrix*}[1][]{\begin {matrix}}{\end {matrix}}\)

\(\newenvironment {pmatrix*}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bmatrix*}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bmatrix*}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vmatrix*}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vmatrix*}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newenvironment {smallmatrix*}[1][]{\begin {matrix}}{\end {matrix}}\)

\(\newenvironment {psmallmatrix*}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bsmallmatrix*}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bsmallmatrix*}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vsmallmatrix*}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vsmallmatrix*}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newenvironment {psmallmatrix}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bsmallmatrix}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bsmallmatrix}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vsmallmatrix}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vsmallmatrix}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newcommand {\LWRmultlined }[1][]{\begin {multline*}}\)

\(\newenvironment {multlined}[1][]{\LWRmultlined }{\end {multline*}}\)

\(\let \LWRorigshoveleft \shoveleft \)

\(\renewcommand {\shoveleft }[1][]{\LWRorigshoveleft }\)

\(\let \LWRorigshoveright \shoveright \)

\(\renewcommand {\shoveright }[1][]{\LWRorigshoveright }\)

\(\newenvironment {dcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {dcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {rcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {rcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {drcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {drcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {cases*}{\begin {cases}}{\end {cases}}\)

\(\newcommand {\MoveEqLeft }[1][]{}\)

\(\def \LWRAboxed #1&amp;#2&amp;#3!|!{\fbox {\(#1\)}&amp;\fbox {\(#2\)}} \newcommand {\Aboxed }[1]{\LWRAboxed #1&amp;&amp;!|!} \)

\( \newcommand {\LWRABLines }[1][\Updownarrow ]{#1 \notag \\}\newcommand {\ArrowBetweenLines }{\ifstar \LWRABLines \LWRABLines } \)

\(\newcommand {\shortintertext }[1]{\text {#1}\notag \\}\)

\(\newcommand {\vdotswithin }[1]{\hspace {.5em}\vdots }\)

\(\newcommand {\LWRshortvdotswithinstar }[1]{\vdots \hspace {.5em} &amp; \\}\)

\(\newcommand {\LWRshortvdotswithinnostar }[1]{&amp; \hspace {.5em}\vdots \\}\)

\(\newcommand {\shortvdotswithin }{\ifstar \LWRshortvdotswithinstar \LWRshortvdotswithinnostar }\)

\(\newcommand {\MTFlushSpaceAbove }{}\)

\(\newcommand {\MTFlushSpaceBelow }{\\}\)

\(\newcommand \lparen {(}\)

\(\newcommand \rparen {)}\)

\(\newcommand {\ordinarycolon }{:}\)

\(\newcommand {\vcentcolon }{\mathrel {\mathop \ordinarycolon }}\)

\(\newcommand \dblcolon {\vcentcolon \vcentcolon }\)

\(\newcommand \coloneqq {\vcentcolon =}\)

\(\newcommand \Coloneqq {\dblcolon =}\)

\(\newcommand \coloneq {\vcentcolon {-}}\)

\(\newcommand \Coloneq {\dblcolon {-}}\)

\(\newcommand \eqqcolon {=\vcentcolon }\)

\(\newcommand \Eqqcolon {=\dblcolon }\)

\(\newcommand \eqcolon {\mathrel {-}\vcentcolon }\)

\(\newcommand \Eqcolon {\mathrel {-}\dblcolon }\)

\(\newcommand \colonapprox {\vcentcolon \approx }\)

\(\newcommand \Colonapprox {\dblcolon \approx }\)

\(\newcommand \colonsim {\vcentcolon \sim }\)

\(\newcommand \Colonsim {\dblcolon \sim }\)

\(\newcommand {\nuparrow }{\mathrel {\cancel {\uparrow }}}\)

\(\newcommand {\ndownarrow }{\mathrel {\cancel {\downarrow }}}\)

\(\newcommand {\bigtimes }{\mathop {\Large \times }\limits }\)

\(\newcommand {\prescript }[3]{{}^{#1}_{#2}#3}\)

\(\newenvironment {lgathered}{\begin {gathered}}{\end {gathered}}\)

\(\newenvironment {rgathered}{\begin {gathered}}{\end {gathered}}\)

\(\newcommand {\splitfrac }[2]{{}^{#1}_{#2}}\)

\(\let \splitdfrac \splitfrac \)

\(\newcommand {\LWRoverlaysymbols }[2]{\mathord {\smash {\mathop {#2\strut }\limits ^{\smash {\lower 3ex{#1}}}}\strut }}\)

\(\newcommand{\alphaup}{\unicode{x03B1}}\)

\(\newcommand{\betaup}{\unicode{x03B2}}\)

\(\newcommand{\gammaup}{\unicode{x03B3}}\)

\(\newcommand{\digammaup}{\unicode{x03DD}}\)

\(\newcommand{\deltaup}{\unicode{x03B4}}\)

\(\newcommand{\epsilonup}{\unicode{x03F5}}\)

\(\newcommand{\varepsilonup}{\unicode{x03B5}}\)

\(\newcommand{\zetaup}{\unicode{x03B6}}\)

\(\newcommand{\etaup}{\unicode{x03B7}}\)

\(\newcommand{\thetaup}{\unicode{x03B8}}\)

\(\newcommand{\varthetaup}{\unicode{x03D1}}\)

\(\newcommand{\iotaup}{\unicode{x03B9}}\)

\(\newcommand{\kappaup}{\unicode{x03BA}}\)

\(\newcommand{\varkappaup}{\unicode{x03F0}}\)

\(\newcommand{\lambdaup}{\unicode{x03BB}}\)

\(\newcommand{\muup}{\unicode{x03BC}}\)

\(\newcommand{\nuup}{\unicode{x03BD}}\)

\(\newcommand{\xiup}{\unicode{x03BE}}\)

\(\newcommand{\omicronup}{\unicode{x03BF}}\)

\(\newcommand{\piup}{\unicode{x03C0}}\)

\(\newcommand{\varpiup}{\unicode{x03D6}}\)

\(\newcommand{\rhoup}{\unicode{x03C1}}\)

\(\newcommand{\varrhoup}{\unicode{x03F1}}\)

\(\newcommand{\sigmaup}{\unicode{x03C3}}\)

\(\newcommand{\varsigmaup}{\unicode{x03C2}}\)

\(\newcommand{\tauup}{\unicode{x03C4}}\)

\(\newcommand{\upsilonup}{\unicode{x03C5}}\)

\(\newcommand{\phiup}{\unicode{x03D5}}\)

\(\newcommand{\varphiup}{\unicode{x03C6}}\)

\(\newcommand{\chiup}{\unicode{x03C7}}\)

\(\newcommand{\psiup}{\unicode{x03C8}}\)

\(\newcommand{\omegaup}{\unicode{x03C9}}\)

\(\newcommand{\Alphaup}{\unicode{x0391}}\)

\(\newcommand{\Betaup}{\unicode{x0392}}\)

\(\newcommand{\Gammaup}{\unicode{x0393}}\)

\(\newcommand{\Digammaup}{\unicode{x03DC}}\)

\(\newcommand{\Deltaup}{\unicode{x0394}}\)

\(\newcommand{\Epsilonup}{\unicode{x0395}}\)

\(\newcommand{\Zetaup}{\unicode{x0396}}\)

\(\newcommand{\Etaup}{\unicode{x0397}}\)

\(\newcommand{\Thetaup}{\unicode{x0398}}\)

\(\newcommand{\Varthetaup}{\unicode{x03F4}}\)

\(\newcommand{\Iotaup}{\unicode{x0399}}\)

\(\newcommand{\Kappaup}{\unicode{x039A}}\)

\(\newcommand{\Lambdaup}{\unicode{x039B}}\)

\(\newcommand{\Muup}{\unicode{x039C}}\)

\(\newcommand{\Nuup}{\unicode{x039D}}\)

\(\newcommand{\Xiup}{\unicode{x039E}}\)

\(\newcommand{\Omicronup}{\unicode{x039F}}\)

\(\newcommand{\Piup}{\unicode{x03A0}}\)

\(\newcommand{\Varpiup}{\unicode{x03D6}}\)

\(\newcommand{\Rhoup}{\unicode{x03A1}}\)

\(\newcommand{\Sigmaup}{\unicode{x03A3}}\)

\(\newcommand{\Tauup}{\unicode{x03A4}}\)

\(\newcommand{\Upsilonup}{\unicode{x03A5}}\)

\(\newcommand{\Phiup}{\unicode{x03A6}}\)

\(\newcommand{\Chiup}{\unicode{x03A7}}\)

\(\newcommand{\Psiup}{\unicode{x03A8}}\)

\(\newcommand{\Omegaup}{\unicode{x03A9}}\)

\(\newcommand{\alphait}{\unicode{x1D6FC}}\)

\(\newcommand{\betait}{\unicode{x1D6FD}}\)

\(\newcommand{\gammait}{\unicode{x1D6FE}}\)

\(\newcommand{\digammait}{\mathit{\unicode{x03DD}}}\)

\(\newcommand{\deltait}{\unicode{x1D6FF}}\)

\(\newcommand{\epsilonit}{\unicode{x1D716}}\)

\(\newcommand{\varepsilonit}{\unicode{x1D700}}\)

\(\newcommand{\zetait}{\unicode{x1D701}}\)

\(\newcommand{\etait}{\unicode{x1D702}}\)

\(\newcommand{\thetait}{\unicode{x1D703}}\)

\(\newcommand{\varthetait}{\unicode{x1D717}}\)

\(\newcommand{\iotait}{\unicode{x1D704}}\)

\(\newcommand{\kappait}{\unicode{x1D705}}\)

\(\newcommand{\varkappait}{\unicode{x1D718}}\)

\(\newcommand{\lambdait}{\unicode{x1D706}}\)

\(\newcommand{\muit}{\unicode{x1D707}}\)

\(\newcommand{\nuit}{\unicode{x1D708}}\)

\(\newcommand{\xiit}{\unicode{x1D709}}\)

\(\newcommand{\omicronit}{\unicode{x1D70A}}\)

\(\newcommand{\piit}{\unicode{x1D70B}}\)

\(\newcommand{\varpiit}{\unicode{x1D71B}}\)

\(\newcommand{\rhoit}{\unicode{x1D70C}}\)

\(\newcommand{\varrhoit}{\unicode{x1D71A}}\)

\(\newcommand{\sigmait}{\unicode{x1D70E}}\)

\(\newcommand{\varsigmait}{\unicode{x1D70D}}\)

\(\newcommand{\tauit}{\unicode{x1D70F}}\)

\(\newcommand{\upsilonit}{\unicode{x1D710}}\)

\(\newcommand{\phiit}{\unicode{x1D719}}\)

\(\newcommand{\varphiit}{\unicode{x1D711}}\)

\(\newcommand{\chiit}{\unicode{x1D712}}\)

\(\newcommand{\psiit}{\unicode{x1D713}}\)

\(\newcommand{\omegait}{\unicode{x1D714}}\)

\(\newcommand{\Alphait}{\unicode{x1D6E2}}\)

\(\newcommand{\Betait}{\unicode{x1D6E3}}\)

\(\newcommand{\Gammait}{\unicode{x1D6E4}}\)

\(\newcommand{\Digammait}{\mathit{\unicode{x03DC}}}\)

\(\newcommand{\Deltait}{\unicode{x1D6E5}}\)

\(\newcommand{\Epsilonit}{\unicode{x1D6E6}}\)

\(\newcommand{\Zetait}{\unicode{x1D6E7}}\)

\(\newcommand{\Etait}{\unicode{x1D6E8}}\)

\(\newcommand{\Thetait}{\unicode{x1D6E9}}\)

\(\newcommand{\Varthetait}{\unicode{x1D6F3}}\)

\(\newcommand{\Iotait}{\unicode{x1D6EA}}\)

\(\newcommand{\Kappait}{\unicode{x1D6EB}}\)

\(\newcommand{\Lambdait}{\unicode{x1D6EC}}\)

\(\newcommand{\Muit}{\unicode{x1D6ED}}\)

\(\newcommand{\Nuit}{\unicode{x1D6EE}}\)

\(\newcommand{\Xiit}{\unicode{x1D6EF}}\)

\(\newcommand{\Omicronit}{\unicode{x1D6F0}}\)

\(\newcommand{\Piit}{\unicode{x1D6F1}}\)

\(\newcommand{\Rhoit}{\unicode{x1D6F2}}\)

\(\newcommand{\Sigmait}{\unicode{x1D6F4}}\)

\(\newcommand{\Tauit}{\unicode{x1D6F5}}\)

\(\newcommand{\Upsilonit}{\unicode{x1D6F6}}\)

\(\newcommand{\Phiit}{\unicode{x1D6F7}}\)

\(\newcommand{\Chiit}{\unicode{x1D6F8}}\)

\(\newcommand{\Psiit}{\unicode{x1D6F9}}\)

\(\newcommand{\Omegait}{\unicode{x1D6FA}}\)

\(\let \digammaup \Digammaup \)

\(\renewcommand {\digammait }{\mathit {\digammaup }}\)

\(\newcommand {\smallin }{\unicode {x220A}}\)

\(\newcommand {\smallowns }{\unicode {x220D}}\)

\(\newcommand {\notsmallin }{\LWRoverlaysymbols {/}{\unicode {x220A}}}\)

\(\newcommand {\notsmallowns }{\LWRoverlaysymbols {/}{\unicode {x220D}}}\)

\(\newcommand {\rightangle }{\unicode {x221F}}\)

\(\newcommand {\intclockwise }{\unicode {x2231}}\)

\(\newcommand {\ointclockwise }{\unicode {x2232}}\)

\(\newcommand {\ointctrclockwise }{\unicode {x2233}}\)

\(\newcommand {\oiint }{\unicode {x222F}}\)

\(\newcommand {\oiiint }{\unicode {x2230}}\)

\(\newcommand {\ddag }{\unicode {x2021}}\)

\(\newcommand {\P }{\unicode {x00B6}}\)

\(\newcommand {\copyright }{\unicode {x00A9}}\)

\(\newcommand {\dag }{\unicode {x2020}}\)

\(\newcommand {\pounds }{\unicode {x00A3}}\)

\(\newcommand {\iddots }{\unicode {x22F0}}\)

\(\newcommand {\utimes }{\overline {\times }}\)

\(\newcommand {\dtimes }{\underline {\times }}\)

\(\newcommand {\udtimes }{\overline {\underline {\times }}}\)

\(\newcommand {\leftwave }{\left \{}\)

\(\newcommand {\rightwave }{\right \}}\)

\(\newcommand {\toprule }[1][]{\hline }\)

\(\let \midrule \toprule \)

\(\let \bottomrule \toprule \)

\(\newcommand {\cmidrule }[2][]{}\)

\(\newcommand {\morecmidrules }{}\)

\(\newcommand {\specialrule }[3]{\hline }\)

\(\newcommand {\addlinespace }[1][]{}\)

\(\newcommand {\LWRsubmultirow }[2][]{#2}\)

\(\newcommand {\LWRmultirow }[2][]{\LWRsubmultirow }\)

\(\newcommand {\multirow }[2][]{\LWRmultirow }\)

\(\newcommand {\mrowcell }{}\)

\(\newcommand {\mcolrowcell }{}\)

\(\newcommand {\STneed }[1]{}\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\newcommand {\tothe }[1]{^{#1}}\)

\(\newcommand {\raiseto }[2]{{#2}^{#1}}\)

\(\newcommand {\ang }[2][]{(\mathrm {#2})\degree }\)

\(\newcommand {\num }[2][]{\mathrm {#2}}\)

\(\newcommand {\si }[2][]{\mathrm {#2}}\)

\(\newcommand {\LWRSI }[2][]{\mathrm {#1\LWRSInumber \,#2}}\)

\(\newcommand {\SI }[2][]{\def \LWRSInumber {#2}\LWRSI }\)

\(\newcommand {\numlist }[2][]{\mathrm {#2}}\)

\(\newcommand {\numrange }[3][]{\mathrm {#2\,\unicode {x2013}\,#3}}\)

\(\newcommand {\SIlist }[3][]{\mathrm {#2\,#3}}\)

\(\newcommand {\SIrange }[4][]{\mathrm {#2\,#4\,\unicode {x2013}\,#3\,#4}}\)

\(\newcommand {\tablenum }[2][]{\mathrm {#2}}\)

\(\newcommand {\ampere }{\mathrm {A}}\)

\(\newcommand {\candela }{\mathrm {cd}}\)

\(\newcommand {\kelvin }{\mathrm {K}}\)

\(\newcommand {\kilogram }{\mathrm {kg}}\)

\(\newcommand {\metre }{\mathrm {m}}\)

\(\newcommand {\mole }{\mathrm {mol}}\)

\(\newcommand {\second }{\mathrm {s}}\)

\(\newcommand {\becquerel }{\mathrm {Bq}}\)

\(\newcommand {\degreeCelsius }{\unicode {x2103}}\)

\(\newcommand {\coulomb }{\mathrm {C}}\)

\(\newcommand {\farad }{\mathrm {F}}\)

\(\newcommand {\gray }{\mathrm {Gy}}\)

\(\newcommand {\hertz }{\mathrm {Hz}}\)

\(\newcommand {\henry }{\mathrm {H}}\)

\(\newcommand {\joule }{\mathrm {J}}\)

\(\newcommand {\katal }{\mathrm {kat}}\)

\(\newcommand {\lumen }{\mathrm {lm}}\)

\(\newcommand {\lux }{\mathrm {lx}}\)

\(\newcommand {\newton }{\mathrm {N}}\)

\(\newcommand {\ohm }{\mathrm {\Omega }}\)

\(\newcommand {\pascal }{\mathrm {Pa}}\)

\(\newcommand {\radian }{\mathrm {rad}}\)

\(\newcommand {\siemens }{\mathrm {S}}\)

\(\newcommand {\sievert }{\mathrm {Sv}}\)

\(\newcommand {\steradian }{\mathrm {sr}}\)

\(\newcommand {\tesla }{\mathrm {T}}\)

\(\newcommand {\volt }{\mathrm {V}}\)

\(\newcommand {\watt }{\mathrm {W}}\)

\(\newcommand {\weber }{\mathrm {Wb}}\)

\(\newcommand {\day }{\mathrm {d}}\)

\(\newcommand {\degree }{\mathrm {^\circ }}\)

\(\newcommand {\hectare }{\mathrm {ha}}\)

\(\newcommand {\hour }{\mathrm {h}}\)

\(\newcommand {\litre }{\mathrm {l}}\)

\(\newcommand {\liter }{\mathrm {L}}\)

\(\newcommand {\arcminute }{^\prime }\)
\(\newcommand {\minute }{\mathrm {min}}\)

\(\newcommand {\arcsecond }{^{\prime \prime }}\)

\(\newcommand {\tonne }{\mathrm {t}}\)

\(\newcommand {\astronomicalunit }{au}\)

\(\newcommand {\atomicmassunit }{u}\)

\(\newcommand {\bohr }{\mathit {a}_0}\)

\(\newcommand {\clight }{\mathit {c}_0}\)

\(\newcommand {\dalton }{\mathrm {D}_\mathrm {a}}\)

\(\newcommand {\electronmass }{\mathit {m}_{\mathrm {e}}}\)

\(\newcommand {\electronvolt }{\mathrm {eV}}\)

\(\newcommand {\elementarycharge }{\mathit {e}}\)

\(\newcommand {\hartree }{\mathit {E}_{\mathrm {h}}}\)

\(\newcommand {\planckbar }{\mathit {\unicode {x210F}}}\)

\(\newcommand {\angstrom }{\mathrm {\unicode {x212B}}}\)

\(\let \LWRorigbar \bar \)

\(\newcommand {\bar }{\mathrm {bar}}\)

\(\newcommand {\barn }{\mathrm {b}}\)

\(\newcommand {\bel }{\mathrm {B}}\)

\(\newcommand {\decibel }{\mathrm {dB}}\)

\(\newcommand {\knot }{\mathrm {kn}}\)

\(\newcommand {\mmHg }{\mathrm {mmHg}}\)

\(\newcommand {\nauticalmile }{\mathrm {M}}\)

\(\newcommand {\neper }{\mathrm {Np}}\)

\(\newcommand {\yocto }{\mathrm {y}}\)

\(\newcommand {\zepto }{\mathrm {z}}\)

\(\newcommand {\atto }{\mathrm {a}}\)

\(\newcommand {\femto }{\mathrm {f}}\)

\(\newcommand {\pico }{\mathrm {p}}\)

\(\newcommand {\nano }{\mathrm {n}}\)

\(\newcommand {\micro }{\mathrm {\unicode {x00B5}}}\)

\(\newcommand {\milli }{\mathrm {m}}\)

\(\newcommand {\centi }{\mathrm {c}}\)

\(\newcommand {\deci }{\mathrm {d}}\)

\(\newcommand {\deca }{\mathrm {da}}\)

\(\newcommand {\hecto }{\mathrm {h}}\)

\(\newcommand {\kilo }{\mathrm {k}}\)

\(\newcommand {\mega }{\mathrm {M}}\)

\(\newcommand {\giga }{\mathrm {G}}\)

\(\newcommand {\tera }{\mathrm {T}}\)

\(\newcommand {\peta }{\mathrm {P}}\)

\(\newcommand {\exa }{\mathrm {E}}\)

\(\newcommand {\zetta }{\mathrm {Z}}\)

\(\newcommand {\yotta }{\mathrm {Y}}\)

\(\newcommand {\percent }{\mathrm {\%}}\)

\(\newcommand {\meter }{\mathrm {m}}\)

\(\newcommand {\metre }{\mathrm {m}}\)

\(\newcommand {\gram }{\mathrm {g}}\)

\(\newcommand {\kg }{\kilo \gram }\)

\(\newcommand {\of }[1]{_{\mathrm {#1}}}\)

\(\newcommand {\squared }{^2}\)

\(\newcommand {\square }[1]{\mathrm {#1}^2}\)

\(\newcommand {\cubed }{^3}\)

\(\newcommand {\cubic }[1]{\mathrm {#1}^3}\)

\(\newcommand {\per }{/}\)

\(\newcommand {\celsius }{\unicode {x2103}}\)

\(\newcommand {\fg }{\femto \gram }\)

\(\newcommand {\pg }{\pico \gram }\)

\(\newcommand {\ng }{\nano \gram }\)

\(\newcommand {\ug }{\micro \gram }\)

\(\newcommand {\mg }{\milli \gram }\)

\(\newcommand {\g }{\gram }\)

\(\newcommand {\kg }{\kilo \gram }\)

\(\newcommand {\amu }{\mathrm {u}}\)

\(\newcommand {\nm }{\nano \metre }\)

\(\newcommand {\um }{\micro \metre }\)

\(\newcommand {\mm }{\milli \metre }\)

\(\newcommand {\cm }{\centi \metre }\)

\(\newcommand {\dm }{\deci \metre }\)

\(\newcommand {\m }{\metre }\)

\(\newcommand {\km }{\kilo \metre }\)

\(\newcommand {\as }{\atto \second }\)

\(\newcommand {\fs }{\femto \second }\)

\(\newcommand {\ps }{\pico \second }\)

\(\newcommand {\ns }{\nano \second }\)

\(\newcommand {\us }{\micro \second }\)

\(\newcommand {\ms }{\milli \second }\)

\(\newcommand {\s }{\second }\)

\(\newcommand {\fmol }{\femto \mol }\)

\(\newcommand {\pmol }{\pico \mol }\)

\(\newcommand {\nmol }{\nano \mol }\)

\(\newcommand {\umol }{\micro \mol }\)

\(\newcommand {\mmol }{\milli \mol }\)

\(\newcommand {\mol }{\mol }\)

\(\newcommand {\kmol }{\kilo \mol }\)

\(\newcommand {\pA }{\pico \ampere }\)

\(\newcommand {\nA }{\nano \ampere }\)

\(\newcommand {\uA }{\micro \ampere }\)

\(\newcommand {\mA }{\milli \ampere }\)

\(\newcommand {\A }{\ampere }\)

\(\newcommand {\kA }{\kilo \ampere }\)

\(\newcommand {\ul }{\micro \litre }\)

\(\newcommand {\ml }{\milli \litre }\)

\(\newcommand {\l }{\litre }\)

\(\newcommand {\hl }{\hecto \litre }\)

\(\newcommand {\uL }{\micro \liter }\)

\(\newcommand {\mL }{\milli \liter }\)

\(\newcommand {\L }{\liter }\)

\(\newcommand {\hL }{\hecto \liter }\)

\(\newcommand {\mHz }{\milli \hertz }\)

\(\newcommand {\Hz }{\hertz }\)

\(\newcommand {\kHz }{\kilo \hertz }\)

\(\newcommand {\MHz }{\mega \hertz }\)

\(\newcommand {\GHz }{\giga \hertz }\)

\(\newcommand {\THz }{\tera \hertz }\)

\(\newcommand {\mN }{\milli \newton }\)

\(\newcommand {\N }{\newton }\)

\(\newcommand {\kN }{\kilo \newton }\)

\(\newcommand {\MN }{\mega \newton }\)

\(\newcommand {\Pa }{\pascal }\)

\(\newcommand {\kPa }{\kilo \pascal }\)

\(\newcommand {\MPa }{\mega \pascal }\)

\(\newcommand {\GPa }{\giga \pascal }\)

\(\newcommand {\mohm }{\milli \ohm }\)

\(\newcommand {\kohm }{\kilo \ohm }\)

\(\newcommand {\Mohm }{\mega \ohm }\)

\(\newcommand {\pV }{\pico \volt }\)

\(\newcommand {\nV }{\nano \volt }\)

\(\newcommand {\uV }{\micro \volt }\)

\(\newcommand {\mV }{\milli \volt }\)

\(\newcommand {\V }{\volt }\)

\(\newcommand {\kV }{\kilo \volt }\)

\(\newcommand {\W }{\watt }\)

\(\newcommand {\uW }{\micro \watt }\)

\(\newcommand {\mW }{\milli \watt }\)

\(\newcommand {\kW }{\kilo \watt }\)

\(\newcommand {\MW }{\mega \watt }\)

\(\newcommand {\GW }{\giga \watt }\)

\(\newcommand {\J }{\joule }\)

\(\newcommand {\uJ }{\micro \joule }\)

\(\newcommand {\mJ }{\milli \joule }\)

\(\newcommand {\kJ }{\kilo \joule }\)

\(\newcommand {\eV }{\electronvolt }\)

\(\newcommand {\meV }{\milli \electronvolt }\)

\(\newcommand {\keV }{\kilo \electronvolt }\)

\(\newcommand {\MeV }{\mega \electronvolt }\)

\(\newcommand {\GeV }{\giga \electronvolt }\)

\(\newcommand {\TeV }{\tera \electronvolt }\)

\(\newcommand {\kWh }{\kilo \watt \hour }\)

\(\newcommand {\F }{\farad }\)

\(\newcommand {\fF }{\femto \farad }\)

\(\newcommand {\pF }{\pico \farad }\)

\(\newcommand {\K }{\mathrm {K}}\)

\(\newcommand {\dB }{\mathrm {dB}}\)

\(\newcommand {\kibi }{\mathrm {Ki}}\)

\(\newcommand {\mebi }{\mathrm {Mi}}\)

\(\newcommand {\gibi }{\mathrm {Gi}}\)

\(\newcommand {\tebi }{\mathrm {Ti}}\)

\(\newcommand {\pebi }{\mathrm {Pi}}\)

\(\newcommand {\exbi }{\mathrm {Ei}}\)

\(\newcommand {\zebi }{\mathrm {Zi}}\)

\(\newcommand {\yobi }{\mathrm {Yi}}\)

\(\require {mhchem}\)

\(\require {cancel}\)

\(\newcommand {\fint }{âĺŊ}\)

\(\newcommand {\hdots }{\cdots }\)

\(\newcommand {\mathnormal }[1]{#1}\)

\(\newcommand {\vecs }[2]{\vec {#1}_{#2}}\)

\(\renewcommand {\A }{\mathcal {A}}\)

\(\newcommand {\B }{\mathcal {B}}\)

\(\renewcommand {\C }{\mathcal {C}}\)

\(\newcommand {\EE }{\mathbb {E}}\)

\(\renewcommand {\N }{\mathcal {N}}\)

\(\renewcommand {\P }{\mathcal {P}}\)

\(\newcommand {\PP }{\mathbb {P}}\)

\(\newcommand {\T }{\mathcal {T}}\)

\(\renewcommand {\U }{\mathcal {U}}\)

\(\newcommand {\X }{\mathcal {X}}\)

\(\newcommand {\Y }{\mathcal {Y}}\)

\(\newcommand {\Bin }{\operatorname {Bin}}\)

\(\newcommand {\Pois }{\operatorname {Pois}}\)

\(\newcommand {\Exp }{\operatorname {Exp}}\)

\(\newcommand {\BetaV }{\operatorname {Beta}}\)

\(\newcommand {\GammaV }{\operatorname {Gamma}}\)

\(\newcommand {\pot }{\mathfrak {P}}\)

\(\renewcommand {\1}{𝟙}\)

\(\newcommand {\id }{\mathrm {id}}\)

\(\renewcommand {\i }{\mathrm {i}}\)

\(\renewcommand {\theta }{\vartheta }\)

\(\newcommand {\Tu }{\underline {T}}\)

\(\newcommand {\To }{\overline {T}}\)

\(\newcommand {\Var }{\mathrm {Var}}\)

\(\newcommand {\Cov }{\mathrm {Cov}}\)

\(\newcommand {\name }[1]{\textsc {#1}}\)

\(\newcommand {\smallpmatrix }[1]{\left (\begin {smallmatrix}#1\end {smallmatrix}\right )}\)

\(\newcommand {\matlab }{{\fontfamily {bch}\scshape \selectfont {}Matlab}}\)

\(\newcommand {\innerproduct }[1]{\left \langle {#1}\right \rangle }\)

\(\newcommand {\norm }[1]{\left \Vert {#1}\right \Vert }\)

\(\renewcommand {\natural }{\mathbb {N}}\)

\(\newcommand {\integer }{\mathbb {Z}}\)

\(\newcommand {\rational }{\mathbb {Q}}\)

\(\newcommand {\real }{\mathbb {R}}\)

\(\newcommand {\complex }{\mathbb {C}}\)

\(\renewcommand {\d }{\mathop {}\!\mathrm {d}}\)

\(\newcommand {\dr }{\d {}r}\)

\(\newcommand {\ds }{\d {}s}\)

\(\newcommand {\dt }{\d {}t}\)

\(\newcommand {\du }{\d {}u}\)

\(\newcommand {\dv }{\d {}v}\)

\(\newcommand {\dw }{\d {}w}\)

\(\newcommand {\dx }{\d {}x}\)

\(\newcommand {\dy }{\d {}y}\)

\(\newcommand {\dz }{\d {}z}\)

\(\newcommand {\dsigma }{\d {}\sigma }\)

\(\newcommand {\dphi }{\d {}\phi }\)

\(\newcommand {\dvarphi }{\d {}\varphi }\)

\(\newcommand {\dtau }{\d {}\tau }\)

\(\newcommand {\dxi }{\d {}\xi }\)

\(\newcommand {\dtheta }{\d {}\theta }\)

\(\newcommand {\tp }{\mathrm {T}}\)

</div>

<style type="text/css">
.lwarp-contents li.list-item-f0::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f1::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f2::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f3::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f4::marker {
  font-style:italic;
  content:'a)\00a0\00a0';
}
.lwarp-contents li.list-item-f5::marker {
  font-style:italic;
  content:'b)\00a0\00a0';
}
</style>
<p>

</p>



<h2 id="konfidenzintervalle">Konfidenzintervalle</h2>

</p>


<p>
<em>Bemerkung</em>: Die Angabe eines Scha&#x0308;tzwertes fu&#x0308;r einen unbekannten Parameter allein ist ha&#x0308;ufig noch nicht befriedigend. Konfidenzintervalle liefern zusa&#x0308;tzlich noch ein Maß
fu&#x0308;r die Pra&#x0308;zision des Scha&#x0308;tzers. Im Folgenden sei \(T(X)\) ein Scha&#x0308;tzer fu&#x0308;r \(q(\theta )\).
</p>

<p>
<b>Konfidenzintervall</b>:&#x2003; Zwei Statistiken \(\Tu = \Tu (X)\) und \(\To = \To (X)\) mit \(\Tu \le \To \) definieren ein <em><span class="dashuline" >\((1 - \alpha )\)-Konfidenzintervall (KI)</span></em>
fu&#x0308;r \(q(\theta )\) zum <em><span class="dashuline" >Konfidenzniveau</span></em> \((1 - \alpha ) \in (0, 1)\), falls<br />
\(\forall _{\theta \in \Theta }\; \PP _\theta (q(\theta ) \in [\Tu (X), \To (X)]) \ge 1 - \alpha \).
</p>

<p>
<em>Bemerkung</em>: Ist \(x\) eine Realisierung von \(X\), so ist \([\Tu (x), \To (x)]\) ein sog. <em><span class="dashuline" >konkretes \((1 - \alpha )\)-Konfidenzintervall</span></em> fu&#x0308;r \(q(\theta )\).
Eine typische Fehlvorstellung ist, dass mit Wahrscheinlichkeit \((1 - \alpha )\) gelten wu&#x0308;rde, dass \(q(\theta ) \in [\Tu (x), \To (x)]\). Dies ist unsinnig, da die Aussage „\(q(\theta ) \in [\Tu (x), \To
(x)]\)“ fu&#x0308;r eine konkrete Beobachtung \(x\) entweder wahr oder falsch ist. Die richtige Interpretation ist folgende: Sind \(x_1, \dotsc , x_n\) \(n\) Beobachtungen von \(n\) i.i.d. Zufallsvariablen mit derselben
Verteilung wie \(X\), so erwartet man, dass \(q(\theta ) \in [\Tu (x_i), \To (x_i)]\) fu&#x0308;r mindestens ca. \((1 - \alpha )n\) der \(i \in \{1, \dotsc , n\}\) wahr ist.
</p>

<p>
<b>Quantil</b>:&#x2003; Ist \(X\) eine reelle Zufallsvariable und \(F_X\) ihre Verteilungsfunktion, so heißt<br />
\(F_X^{-1}\colon (0, 1) \rightarrow \real \) mit \(F_X^{-1}(p) := \inf \{x \in \real \;|\; F_X(x) \ge p\}\) <em><span class="dashuline" >Quantilfunktion</span></em> von \(X\).<br />
Das Bild \(F_X^{-1}(p)\) einer Zahl \(p \in (0, 1)\) heißt <em><span class="dashuline" >\(p\)-Quantil</span></em> von \(X\).
</p>

<p>
<em>Beispiel</em>: Seien \(X_1, \dotsc , X_n \sim \N (\theta , \sigma ^2)\) i.i.d. mit bekannter Varianz \(\sigma ^2\). Dann gilt mit dem \((1 - \alpha /2)\)-Quantil \(z_{1-\alpha /2}\) der
Standard-Normalverteilung, dass \(1 - \alpha = \PP _\theta \!\left (\left |\frac {\overline {X} - \theta }{\sigma /\sqrt {n}}\right | \le z_{1-\alpha /2}\right )\), denn die Zufallsvariable zwischen den
Betragsstrichen ist \(\N (0, 1)\)-verteilt. Das entspricht<br />
\(\PP _\theta \!\left (\overline {X} - \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2} \le \theta \le \overline {X} + \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2}\right ) = \PP _\theta (\theta \in
[\Tu (X), \To (X)])\) mit \(\Tu (X) := \overline {X} - \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2}\) und \(\To (X) := \overline {X} + \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2}\). Man schreibt kurz,
dass \(\overline {X} \pm \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2}\) ein \((1 - \alpha )\)-Konfidenzintervall fu&#x0308;r den unbekannten Parameter \(\theta \) ist.<br />
Fu&#x0308;r \(n = 9\), \(\sigma ^2 = 4\), \(\alpha = \num {0.05}\) mit \(\overline {x} = \num {1.5}\) ist \(z_{1-\alpha /2} = z_{0.975} \approx \num {1.96}\), d.&#x202f;h.<br />
\(\num {1.5} \pm \frac {2}{3} \cdot \num {1.96} \approx [\num {0.19}, \num {2.81}]\) ist ein konkretes \(\SI {95}{\percent }\)-KI fu&#x0308;r den Erwartungswert \(\theta \).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>:<br />
Ein Konfidenzintervall \([\Tu (X), \To (X)]\) fu&#x0308;r \(q(\theta )\) sollte sinnvollerweise einige Kriterien erfu&#x0308;llen:
</p>
<ul style="list-style-type:none">

<li class="list-item-f0"><p>\(\EE _\theta (\To (X) - \Tu (X))\) sollte so klein wie mo&#x0308;glich sein.
</p>
</li>
<li class="list-item-f1"><p>\(\PP _\theta (q(\theta ) \in [\Tu (X), \To (X)])\) sollte unabha&#x0308;ngig von \(\theta \) sein.
</p>
</li>
</ul>

<p>
<b>Pivot-Statistik</b>:&#x2003; Eine Statistik \(G = G(X, \theta )\) heißt <em><span class="dashuline" >Pivot</span></em> (oder <em><span class="dashuline" >Pivot-Statistik</span></em>), falls deren Verteilung
unabha&#x0308;ngig von \(\theta \) ist.
</p>

<p>
<em>Beispiel</em>: Die Verteilung von \(G(X, \theta ) := \frac {\overline {X} - \theta }{\sigma /\sqrt {n}} \sim \N (0, 1)\) in obigem Beispiel ist unabha&#x0308;ngig von \(\theta \), damit ist \(G\) eine
Pivot-Statistik.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: A&#x0308;hnlich wie bei der Frage nach der Existenz von gleichma&#x0308;ßig optimalen Scha&#x0308;tzern existieren im Allgemeinen keine (gleichma&#x0308;ßig) kleinsten Konfidenzintervalle. Deshalb
schra&#x0308;nkt man sich auf unverzerrte Konfidenzintervalle ein.
</p>

<p>
<b>unverzerrtes Konfidenzintervall</b>:&#x2003; Ein \((1 - \alpha )\)-Konfidenzintervall \([\Tu , \To ]\) fu&#x0308;r \(q(\theta )\) heißt <em><span class="dashuline" >unverzerrt</span></em>, falls \(\forall
_{\theta , \theta ’ \in \Theta }\; \PP _\theta (q(\theta ) \in [\Tu (X), \To (X)]) \ge \PP _\theta (q(\theta ’) \in [\Tu (X), \To (X)])\), d.&#x202f;h. die Wahrscheinlichkeit, dass ein unverzerrtes KI
den wahren Wert \(q(\theta )\) einfa&#x0308;ngt, darf nicht kleiner sein als die Wahrscheinlichkeit, dass dieses KI einen anderen Wert \(q(\theta ’)\) einfa&#x0308;ngt.
</p>

<p>
<em>Beispiel</em>: Setzt man obiges Beispiel fort, so gilt<br />
\(\PP _\theta \!\left (\theta ’ \in \overline {X} \pm \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2}\right ) = \PP _\theta \!\left (\frac {\overline {X} - \theta }{\sigma /\sqrt {n}} \in \frac
{\theta ’ - \theta }{\sigma /\sqrt {n}} \pm z_{1-\alpha /2}\right ) = \Phi (x + c) - \Phi (x - c)\) mit \(x := \frac {\theta ’ - \theta }{\sigma /\sqrt {n}}\) und \(c := z_{1-\alpha /2}\), da
\(\frac {\overline {X} - \theta }{\sigma /\sqrt {n}} \sim \N (0, 1)\). Fu&#x0308;r festgehaltenes \(c &gt; 0\) ist \(f(x) := \Phi (x + c) - \Phi (x - c)\) maximal fu&#x0308;r \(x = 0\), denn \(f’(x) =
\frac {1}{\sqrt {2\pi }} \left (\exp \!\left (-\frac {(x+c)^2}{2}\right ) - \exp \!\left (-\frac {(x-c)^2}{2}\right )\right ) = 0\) gilt genau dann, wenn \(x = 0\). Wegen \(f’’(0) = \frac {1}{\sqrt
{2\pi }} \left (-c \exp \!\left (-\frac {c^2}{2}\right ) - c \exp \!\left (-\frac {c^2}{2}\right )\right ) = -2c \varphi (c) &lt; 0\) (da \(c &gt; 0\) und \(\varphi (c) &gt; 0\)) ist \(f(x)\)
maximal fu&#x0308;r \(x = 0\). Daher ist das Konfidenzintervall \(\overline {X} \pm \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2}\) unverzerrt.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Seien nun \(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\) i.i.d. mit \(\mu \) und \(\sigma ^2\) unbekannt, also \(\theta = (\mu , \sigma ^2)\). Gesucht ist wieder ein Konfidenzintervall
fu&#x0308;r den Erwartungswert \(\mu \). Definiert man \(S_n^2 := \frac {1}{n-1} \sum _{i=1}^n (X_i - \overline {X})^2\) als Stichprobenvarianz, dann kann man zeigen, dass \(\overline {X}_n\) und \(S_n^2\)
unabha&#x0308;ngige Zufallsvariablen sind. Damit sind Za&#x0308;hler und Nenner in \(T(X) := \frac {\sqrt {n} (\overline {X}_n - \mu )}{S_n} = \frac {\sqrt {n} (\overline {X}_n - \mu )/\sigma }{\sqrt
{1/(n-1) \cdot (n-1) S_n^2/\sigma ^2}}\) auch stochastisch unabha&#x0308;ngig, wobei der Za&#x0308;hler standard-normalverteilt und \(\frac {(n-1)S_n^2}{\sigma ^2} \sim \chi _{n-1}^2\) (Verteilung von
\(Z_1^2 + \dotsb + Z_n^2\) mit \(Z_1, \dotsc , Z_n \sim \N (0,1)\) i.i.d.). Daher genu&#x0308;gt der Quotient \(T(X)\) einer speziellen Verteilung, der sog. <em><span class="dashuline" ><span class="textsc"
>student</span>schen \(t\)-Verteilung</span></em> \(t_{n-1}\). \(T(X)\) ist unabha&#x0308;ngig von \(\theta \) und damit eine Pivot-Statistik. Mit dem \((1 - \alpha /2)\)-Quantil \(t_{n-1,1-\alpha /2}\) der
\(t_{n-1}\)-Verteilung gilt damit \(1 - \alpha = \PP _\theta \!\left (\left |\frac {\overline {X} - \mu }{S_n/\sqrt {n}}\right | \le t_{n-1,1-\alpha /2}\right ) = \PP _\theta (\mu \in [\Tu (X), \To
(X)])\) (durch Auflo&#x0308;sen nach \(\mu \)) mit \([\Tu (X), \To (X)] := \overline {X} \pm \frac {S_n}{\sqrt {n}} t_{n-1,1-\alpha /2}\), d.&#x202f;h. dieses Zufallsintervall ist ein \((1 - \alpha
)\)-Konfidenzintervall fu&#x0308;r \(\mu \).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Seien nun \(X_1, \dotsc , X_n \sim \Bin (1, \theta )\) i.i.d. Gesucht ist ein (approximatives) \((1 - \alpha )\)-Konfidenzintervall fu&#x0308;r \(\theta \). Der zentrale Grenzwertsatz besagt<br />
\(\frac {\overline {X}_n - \theta }{\sqrt {\theta (1 - \theta )}/\sqrt {n}} \xrightarrow {\text {(d)}} \N (0, 1)\text {-verteilte ZV}\), d.&#x202f;h. \(1 - \alpha \approx \PP _\theta \!\left
(\left |\frac {\overline {X}_n - \theta } {\sqrt {\theta (1 - \theta )}/\sqrt {n}}\right | \le z_{1-\alpha /2}\right )\)<br />
\(= \PP _\theta (n (\overline {X}_n - \theta )^2 \le (z_{1-\alpha /2})^2 \theta (1 - \theta ))\)<br />
\(= \PP _\theta (n (\overline {X}_n)^2 - \theta (2n \overline {X}_n + (z_{1-\alpha /2})^2) + \theta ^2 (n + (z_{1-\alpha /2})^2) \le 0) = \PP _\theta (\theta \in [\Tu (X), \To (X)])\)<br />
fu&#x0308;r bestimmte \(\Tu (X), \To (X)\) (der Ausdruck ist eine nach oben geo&#x0308;ffnete Parabel, d.&#x202f;h. \(\Tu (X)\) und \(\To (X)\) sind die Nullstellen der Parabel). Damit erha&#x0308;lt man ein
approximatives \((1 - \alpha )\)-Konfidenzintervall fu&#x0308;r \(\theta \). Als Faustregel gilt, dass dieses KI brauchbar ist, wenn \(n\theta \ge 5\) und \(n(1-\theta ) \ge 5\) (denn dann ist die Approximation durch
den zentralen GW-Satz brauchbar).
</p>

<p>
Ein alternatives approximatives \((1 - \alpha )\)-Konfidenzintervall erha&#x0308;lt man durch Scha&#x0308;tzung von \(\theta \) durch \(\overline {X}_n\). Damit ist \((1 - \alpha ) \approx \PP _\theta \!\left
(\left |\frac {\overline {X}_n - \theta } {\sqrt {\theta (1 - \theta )}/\sqrt {n}}\right | \le z_{1-\alpha /2}\right ) \approx \PP _\theta \!\left (\left |\frac {\overline {X}_n - \theta }
{\sqrt {\overline {X}_n (1 - \overline {X}_n)}/\sqrt {n}}\right | \le z_{1-\alpha /2}\right )\)<br />
\(\PP _\theta \!\left (\theta \in \overline {X}_n \pm \frac {\sqrt {\overline {X}_n (1 - \overline {X}_n)}}{\sqrt {n}} z_{1 - \alpha /2}\right )\). Damit ist \(\overline {X}_n \pm \frac {\sqrt
{\overline {X}_n (1 - \overline {X}_n)}}{\sqrt {n}} z_{1 - \alpha /2}\) ein approximatives \((1 - \alpha )\)-Konfidenzintervall fu&#x0308;r \(\theta \).
</p>

<p>
<em>Bemerkung</em>: Unter Verwendung von Statistik-Software kann auch ein exaktes \((1 - \alpha )\)-Konfidenzintervall fu&#x0308;r \(\theta \) berechnet werden.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<b>rechteckiger Konfidenzbereich</b>:&#x2003;<br />
Ist \(q(\theta ) = (q_1(\theta ), \dotsc , q_r(\theta ))\) vektorwertig, so ist das Zufallsrechteck<br />
\(I(X) := \{t \in \real ^r \;|\; \forall _{j=1,\dotsc , r}\; t_j \in [\Tu _j(X), \To _j(X)]\} = [\Tu _1(X), \To _1(X)] \times \dotsb \times [\Tu _r(X), \To _r(X)]\) basierend auf den Statistiken
\(\Tu _j\) und \(\To _j\) (\(j = 1, \dotsc , r\)) ein <em><span class="dashuline" >rechteckiger \((1 - \alpha )\)-Konfidenzbereich</span></em> fu&#x0308;r \(q(\theta )\), falls \(\forall _{\theta \in \Theta
}\; \PP _\theta (q(\theta ) \in I(X)) \ge 1 - \alpha \).
</p>

<p>
<em>Bemerkung</em>: Gegeben seien \((1 - \alpha _j)\)-Konfidenzintervalle \(I_j(X) := [\Tu _j(X), \To _j(X)]\) fu&#x0308;r \(q_j(\theta )\) (\(j = 1, \dotsc , r\)), wobei die Konfidenzniveaus \(1 - \alpha
_j\) noch allgemein und beliebig sein sollen. Definiert man \(I(X) := I_1(X) \times \dotsb \times I_r(X)\), so kann man Bedingungen an die \(\alpha _j\) stellen, damit das Zufallsrechteck \(I(X)\) fu&#x0308;r ein
gegebenes \(\alpha \) ein rechteckiger \((1 - \alpha )\)-Konfidenzbereich fu&#x0308;r \(q(\theta )\) ist:
</p>
<ul style="list-style-type:none">

<li class="list-item-f2"><p>Seien \(I_1(X), \dotsc , I_r(X)\) stochastisch unabha&#x0308;ngig, so gilt<br />
\(\PP _\theta (q(\theta ) \in I(X)) = \PP _\theta (q_1(\theta ) \in I_1(X), \dotsc , q_r(\theta ) \in I_r(X)) = \prod _{j=1}^r \PP _\theta (q_j(\theta ) \in I_j(X))\)<br />
\(\ge \prod _{j=1}^r (1 - \alpha _j)\), weil die \(I_j(X)\) \((1 - \alpha _j)\)-KIe fu&#x0308;r \(q_j(\theta )\) sind. Dies ist gro&#x0308;ßer oder gleich als \(1 - \alpha \), wenn \(1 - \alpha _j := (1 -
\alpha )^{1/r}\) gewa&#x0308;hlt wird.
</p>
</li>
<li class="list-item-f3"><p>Sind die \(I_1(X), \dotsc I_r(X)\) nicht notwendigerweise stochastisch unabha&#x0308;ngig, so gilt<br />
\(\PP _\theta (q(\theta ) \in I(X)) = 1 - \PP _\theta (q_1(\theta ) \notin I_1(X) \lor \dotsb \lor q_r(\theta ) \notin I_r(X))\)<br />
\(\ge 1 - \sum _{j=1}^r \PP _\theta (q_j(\theta ) \notin I_j(X)) \ge 1 - \sum _{j=1}^r \alpha _j\), da \(\PP _\theta (q_j(\theta ) \notin I_j(X)) \le \alpha _j\). Dies ist gro&#x0308;ßer oder gleich
als \(1 - \alpha \), falls \(\alpha _j := \frac {\alpha }{r}\) gewa&#x0308;hlt wird.
</p>
</li>
</ul>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Beim bayesianischen Ansatz ist \(\theta \) eine Zufallsvariable, wobei \(\theta \sim \pi \) mit der <em><span class="dashuline" >a-priori-Dichte</span></em> \(\pi \) (Za&#x0308;hl-/L.-B.-Dichte).
\(X|\theta \sim p(\cdot |\theta )\) ist die sogenannte <em><span class="dashuline" >Likelihood</span></em> von \(X\) und \(\theta |X=x \sim p(\cdot ,x)\) der <em><span class="dashuline"
>a-posteriori-Dichte</span></em>. Die a-posteriori-Dichte berechnet sich nach der Formel von Bayes \(p(\cdot |x) = \frac {\pi (\theta ) p(x|\theta )}{m(x)}\) mit \(m(x) := \sum _{\theta _i’} \pi (\theta _i’)
p(\theta _i’|x)\) bzw. \(m(x) := \int \pi (\theta ’) p(\theta ’|x) \d \theta ’\) (falls \(\theta \) diskret bzw. stetig verteilt ist).
</p>

<p>
Ein bayesianischer Intervallscha&#x0308;tzer (auch <em><span class="dashuline" >credible interval</span></em>) fu&#x0308;r \(\theta \) basierend auf der Beobachtung \(x\) ist dann jedes von \(x\) abha&#x0308;ngige
Intervall, das den (beliebigen) Wert \(\theta \) mindestens mit Wahrscheinlichkeit \((1 - \alpha )\) u&#x0308;berdeckt.
</p>

<p>
<b><span class="textsc" >bayes</span>ianischer Intervallscha&#x0308;tzer</b>:&#x2003; Ein <em><span class="dashuline" ><span class="textsc" >bayes</span>ianischer Intervallscha&#x0308;tzer</span></em>
fu&#x0308;r \(\theta \) zum Niveau \((1 - \alpha )\) ist ein (zufallsabha&#x0308;ngiges) Intervall \([\Tu , \To ]\) mit \(\PP (\theta \in [\Tu (X), \To (X)]|X=x) \ge 1 - \alpha \).
</p>

<p>
<em>Bemerkung</em>: Dabei sind \(\theta \) und \(X\) zufallsabha&#x0308;ngig. Im klassischen Ansatz eines (frequentistischen) Konfidenzintervalls ist diese Wahrscheinlichkeit sinnlos, da entweder \(= 0\) oder \(= 1\).
</p>



<h2 id="das-testen-von-hypothesen">Das Testen von Hypothesen</h2>

</p>


<p>
<em>Bemerkung</em>: Mit einem Scha&#x0308;tzverfahren kann z.&#x202f;B. die Erfolgswahrscheinlichkeit einer Therapie gescha&#x0308;tzt werden. Ha&#x0308;ufig ist man aber eher an der Frage interessiert, ob eine neue
Therapie besser ist als eine Standard-Therapie. Diese Frage kann jedoch meist nicht absolut beantwortet werden, die Wahrscheinlichkeit fu&#x0308;r eine Fehlentscheidung muss akzeptiert werden.
</p>

<p>
<b>Null-/Alternativhypothese</b>:&#x2003; Sei \(\P = \{\PP _\theta \;|\; \theta \in \Theta \}\) ein statistisches Modell mit einer Zerlegung \(\Theta := \Theta _0 \mathbin {\dot {\cup }} \Theta _1\) des
Parameterraums, wobei \(\Theta _0, \Theta _1 \not = \emptyset \). Dann heißt die Aussage \(H_0\colon \theta \in \Theta _0\) <em><span class="dashuline" >Nullhypothese</span></em> und \(H_1\colon \theta
\in \Theta _1\) <em><span class="dashuline" >Alternativhypothese</span></em>.
</p>

<p>
<em>Bemerkung</em>: Die zu widerlegende Hypothese wird normalerweise als Nullhypothese formuliert (wegen engl. <em>to nullify</em> = widerlegen).
</p>

<p>
<b>einfache/zusammengesetzte Hypothese</b>:&#x2003; Besteht \(\Theta _0\) nur aus einem Element \(\theta _0\), so heißt \(H_0\) <em><span class="dashuline" >einfache Hypothese</span></em>, andernfalls heißt
\(H_0\) <em><span class="dashuline" >zusammengesetzte Hypothese</span></em>.
</p>

<p>
<b>einseitige/zweiseitige Hypothese</b>:&#x2003;<br />
Ist \(\Theta \subset \real \) und \(\Theta _1 = \{\theta \in \Theta \;|\; \theta \not = \theta _0\}\), so heißt \(H_1\) <em><span class="dashuline" >zweiseitige Hypothese</span></em>.<br />
Im Fall \(\Theta _1 = \{\theta \in \Theta \;|\; \theta &gt; \theta _0\}\) bzw. \(\Theta _1 = \{\theta \in \Theta \;|\; \theta &lt; \theta _0\}\) heißt \(H_1\) <em><span class="dashuline" >einseitige
Hypothese</span></em> (genauer <em><span class="dashuline" >rechts- bzw. linksseitig</span></em>).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<b>Hypothesentest</b>:&#x2003; Ein <em><span class="dashuline" >(statistischer) Hypothesentest (oder Test)</span></em> \(\delta \) ist eine messbare Funktion \(\delta \colon \X \rightarrow [0, 1]\). Dabei
bedeutet \(\delta (X) = 0\), dass die <em><span class="dashuline" >Nullhypothese akzeptiert</span></em> wird, und \(\delta (X) = 1\), dass die <em><span class="dashuline" >Nullhypothese verworfen</span></em>
wird.
</p>

<p>
<b>kritischer Bereich</b>:&#x2003;<br />
Die Menge \(\{x \in \X \;|\; \delta (x) = 1\}\) heißt <em><span class="dashuline" >kritischer Bereich (Verwerfungsbereich)</span></em> von \(\delta \).
</p>

<p>
<b>kritischer Wert</b>:&#x2003;<br />
Ist \(T(X)\) eine Statistik mit \(\delta (X) = \1_{\{T(X) \ge c\}}\), so heißt \(c\) <em><span class="dashuline" >kritischer Wert</span></em> des Tests \(\delta \).
</p>

<p>
<em>Bemerkung</em>: Gema&#x0308;ß obiger Definition ist auch \(\delta (X) = p \in (0, 1)\) zula&#x0308;ssig. In diesem Fall wa&#x0308;hlt man \(Y \sim \Bin (1, p)\) unabha&#x0308;ngig von \(X\) und
entscheidet fu&#x0308;r \(H_0\), falls \(Y = 0\), und fu&#x0308;r \(H_1\) sonst. Dies nennt man <em><span class="dashuline" >randomisierten Test</span></em>, da er nicht nur von den Daten, sondern auch vom Ausgang
eines weiteren Zufallsexperiments abha&#x0308;ngt. Die Untersuchung randomisierter Tests hat vorwiegend theoretische Gru&#x0308;nde und wird im na&#x0308;chsten Kapitel diskutiert.
</p>

<p>
<em>Beispiel</em>: Um die Wirksamkeit eines neuen Medikaments zu testen, sei bekannt, dass \(\SI {20}{\percent }\) ohne Medikament gesund werden, d.&#x202f;h. \(X_1, \dotsc , X_n \sim \Bin (1, \theta )\) i.i.d.
Die Hypothesen lauten \(H_0\colon \theta = \theta _0\) vs. \(H_1\colon \theta &gt; \theta _0\) mit \(\theta _0 := \num {0.2}\). Ist \(\overline {X}\) (relative Ha&#x0308;ufigkeit einer Heilung) „deutlich“
gro&#x0308;ßer als \(\num {0.2}\), so spricht dies eher fu&#x0308;r \(H_1\). Man betrachtet also den Hypothesentest \(\delta _k(X) := 1\), falls \(n\overline {X} = \sum _{i=1}^n X_i \ge k\), und \(\delta _k(X) :=
0\) sonst. Die Frage ist, welches \(k\) man wa&#x0308;hlen soll.
</p>

<p>
<em>Bemerkung</em>: Folgende Tabelle stellt die mo&#x0308;glichen Entscheidungen dar.
</p>
<table>

<tr class="tbrule">
<td class="tdp">

</td>
<td class="tdp">

<p>
\(H_0\) wahr
</p>
</td>
<td class="tdp">

<p>
\(H_1\) wahr
</p>
</td>
</tr>


<tr class="hline" >
<td class="tdp">

<p>
\(H_0\) wird akzeptiert
</p>
</td>
<td class="tdp">

<p>
kein Fehler
</p>
</td>
<td class="tdp">

<p>
Fehler 2. Art
</p>
</td>
</tr>


<tr>
<td class="tdp">

<p>
\(H_1\) wird akzeptiert
</p>
</td>
<td class="tdp">

<p>
Fehler 1. Art
</p>
</td>
<td class="tdp">

<p>
kein Fehler
</p>
</td>
</tr>


<tr class="tbrule">
<td class="tdp">

</td>
<td class="tdp">

</td>
<td class="tdp">

</td>
</tr>

</table>

<p>
Da die Ablehnung von \(H_0\) das Ziel des Tests ist, wird eine fa&#x0308;lschliche Ablehnung von \(H_0\) als gravierender angesehen als eine fa&#x0308;lschliche Beibehaltung von \(H_0\). Man verfa&#x0308;hrt daher
folgendermaßen: Zuna&#x0308;chst betrachtet man nur die Hypothesentests, deren Wahrscheinlichkeit fu&#x0308;r einen Fehler 1. Art ein Niveau \(\alpha \) nicht u&#x0308;berschreitet. Unter diesen Tests sucht man dann
denjenigen, sodass die Wahrscheinlichkeit fu&#x0308;r einen Fehler 2. Art minimal ist.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<b>Gu&#x0308;tefunktion</b>:&#x2003;<br />
Die <em><span class="dashuline" >Gu&#x0308;tefunktion</span></em> \(G_\delta \colon \Theta \rightarrow [0, 1]\) des Tests \(\delta \) ist definiert durch \(G_\delta (\theta ) := \EE _\theta (\delta
(X))\).
</p>

<p>
<em>Bemerkung</em>: Ist \(\delta \) ein <em><span class="dashuline" >nicht-randomisierter Test</span></em> (d.&#x202f;h. \(\delta \in \{0, 1\}\)), so gilt fu&#x0308;r einen gegebenen Parameter \(\theta \in
\Theta \), dass \(G_\delta (\theta ) = \text {(W.keit für Fehler 1. Art)}\), falls \(\theta \in \Theta _0\), und<br />
\(G_\delta (\theta ) = 1 - \text {(W.keit für Fehler 2. Art)}\), falls \(\theta \in \Theta _1\).
</p>

<p>
<b>Test zum Niveau \(\alpha \)/Level-\(\alpha \)-Test</b>:&#x2003;<br />
Gilt fu&#x0308;r einen Test \(\delta \), dass \(\sup _{\theta \in \Theta _0} G_\delta (\theta ) \le \alpha \), so heißt \(\delta \) <em><span class="dashuline" >Test zum Niveau \(\alpha \)</span></em>.<br />
Gilt sogar \(\sup _{\theta \in \Theta _0} G_\delta (\theta ) = \alpha \), so heißt \(\delta \) <em><span class="dashuline" >Level-\(\alpha \)-Test</span></em>.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Bei obigem Beispiel ist die Wahrscheinlichkeit fu&#x0308;r einen Fehler 1. Art gleich<br />
\(\PP _{\theta _0}(\delta _k(X) = 1) = \PP _{\theta _0}(\sum _{i=1}^n X_i \ge k) = \sum _{j=k}^n \binom {n}{j} \theta _0^j (1-\theta _0)^{n-j}\). Die Wahrscheinlichkeit fu&#x0308;r einen Fehler 2. Art
ist gleich \(\PP _\theta (\delta _k(X) = 0) = \PP _\theta (\sum _{i=1}^n X_i &lt; k) = \sum _{j=0}^{k-1} \binom {n}{j} \theta ^j (1-\theta )^{n-j}\) (abha&#x0308;ngig von \(\theta \)). Die
Gu&#x0308;tefunktion ist gleich \(G_{\delta _k}(\theta ) = \PP _\theta (\delta _k(X) = 1) = \sum _{j=k}^n \binom {n}{j} \theta ^j (1-\theta )^{n-j}\) fu&#x0308;r \(\theta \in \Theta := (0, 1)\). Bei
gegebenen Signifikanzniveau \(\alpha \) wa&#x0308;hlt man nun \(k\) als das kleinste \(k_0\), sodass fu&#x0308;r die Fehlerwahrscheinlichkeit 1. Art \(\PP _{\theta _0}(\delta _{k_0}(X) = 1) \le \alpha \) gilt. Dies
ist a&#x0308;quivalent zu<br />
\(\PP _{\theta _0}(\overline {X} \ge k_0/n) \le \alpha \). Durch die Normierung \(\sqrt {\frac {n}{\theta (1-\theta _0)}} (\overline {X} - \theta _0)\) mit \(\EE _{\theta _0}(X_i) = \theta \) und
\(\Var _{\theta _0}(X_i) = \theta _0(1-\theta _0)\) kann man den zentralen Grenzwertsatz anwenden, der besagt, dass diese Zufallsvariable in Verteilung gegen eine \(\N (0,1)\)-verteilte Zufallsvariable konvergiert. Daher
ist<br />
\(\PP _{\theta _0}(\overline {X} \ge k/n) = \PP _{\theta _0}\!\left (\sqrt {\frac {n}{\theta _0(1-\theta _0)}} (\overline {X} - \theta _0) \ge \sqrt {\frac {n}{\theta (1-\theta _0)}} (k_0/n -
\theta _0)\right )\)<br />
\(\approx 1 - \Phi \!\left (\sqrt {\frac {n}{\theta _0(1-\theta _0)}} (k_0/n - \theta _0)\right ) = 1 - \Phi \!\left (\frac {k_0 - n \theta _0}{\sqrt {n \theta _0(1-\theta _0)}}\right )\). Eine
bessere Approximation fu&#x0308;r kleine \(n\) erha&#x0308;lt man, indem man im Za&#x0308;hler die sog. <em><span class="dashuline" >Stetigkeitskorrektur</span></em> \(- \num {0.5}\) anfu&#x0308;gt. Faustregel:
Die Approximation ist brauchbar, wenn \(n\theta _0 \ge 5\) und \(n(1 - \theta _0) \ge 5\). Es gilt nun<br />
\(1 - \Phi \!\left (\frac {k_0 - n \theta _0 - \num {0.5}} {\sqrt {n \theta _0(1-\theta _0)}}\right ) \le \alpha \iff \frac {k_0 - n \theta _0 - \num {0.5}}{\sqrt {n \theta _0(1-\theta _0)}}
\ge z_{1-\alpha } \iff k_0 \ge n\theta _0 + \num {0,5} + z_{1-\alpha } \sqrt {n \theta _0(1-\theta _0)}\).<br />
Damit ist \(\delta _{k_0}(X) = \1_{\{\sum _{i=1}^n X_i &gt; k_0\}}\) ein Test zum approximativen Niveau \(\alpha \) fu&#x0308;r \(H_0\colon \theta \le \theta _0\) vs. \(H_1\colon \theta &gt; \theta _0\).
Mit <b>R</b> kann auch ein exakter Binomialtest zum Niveau \(\alpha \) durchgefu&#x0308;hrt werden.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Beim <em><span class="dashuline" >einseitigen <span class="textsc" >Gauß</span>-Test fu&#x0308;r \(\mu \)</span></em> liegen \(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\) i.i.d. mit
\(\sigma ^2\) bekannt vor. Das Hypothesenpaar lautet \(H_0\colon \mu \le \mu _0\) vs. \(H_1\colon \mu &gt; \mu _0\). Wenn \(\overline {X}\) „groß“ ist, so spricht dies eher fu&#x0308;r \(H_1\). Daher wa&#x0308;hlt
man \(\delta _c(X) := \1_{\{\overline {X} \ge c\}}\). Die Gu&#x0308;tefunktion dieses Tests ist<br />
\(G_{\delta _c}(\mu ) = \PP _\mu (\delta _c(X) = 1) = \PP _\mu \!\left (\frac {\overline {X} - \mu }{\sigma /\sqrt {n}} \ge \frac {c - \mu }{\sigma /\sqrt {n}}\right ) = 1 - \Phi \!\left
(\frac {c - \mu }{\sigma /\sqrt {n}}\right )\).<br />
Damit ist \(\sup _{\mu \in \Theta _0} G_{\delta _c}(\mu ) = 1 - \Phi \!\left (\frac {c - \mu _0}{\sigma /\sqrt {n}}\right ) \overset {!}{=} \alpha \) fu&#x0308;r einen Level-\(\alpha \)-Test, was
a&#x0308;quivalent ist zu \(c = \mu _0 + \frac {\sigma }{\sqrt {n}} z_{1-\alpha }\). Mit diesem Wert ist also \(\delta _c\) ein Level-\(\alpha \)-Test fu&#x0308;r \(H_0\) vs. \(H_1\).<br />
Der Verlauf der Gu&#x0308;tefunktion \(G_{\delta _c}(\mu )\) ist eine Kurve a&#x0308;hnlich der der Verteilungsfunktion, die in \(\mu = \mu _0\) durch \(G_{\delta _c}(\mu _0)\) la&#x0308;uft. Fu&#x0308;r
gro&#x0308;ßeren Stichprobenumfang (oder alternativ kleinere Varianz) ist der Verlauf wesentlich steiler.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Um die Wahl eines konkreten Signifikanzniveaus nicht vorwegzunehmen, wurde der sog. \(p\)-Wert (U&#x0308;berschreitungswert) eingefu&#x0308;hrt als das kleinste Niveau \(\alpha \), zu dem die
Nullhypothese gerade noch abgelehnt werden kann.
</p>

<p>
<b>\(p\)-Wert</b>:&#x2003; Ist \(\delta = \delta ^\alpha \) ein Test zum Niveau \(\alpha \in (0, 1)\) mit <em><span class="dashuline" >kritischem Bereich</span></em><br />
\(K^\alpha := \{x \in \X \;|\; \delta ^\alpha (x) = 1\}\), wobei \(K^\alpha \subset K^{\alpha ’}\) fu&#x0308;r \(\alpha &lt; \alpha ’\) gelten soll, dann heißt<br />
\(p(X) := \inf \{\alpha \in (0,1) \;|\; X \in K^\alpha \}\) <em><span class="dashuline" >\(p\)-Wert</span></em> des Tests \(\delta \).
</p>

<p>
<em>Bemerkung</em>: Der \(p\)-Wert \(p(X)\) ist also selbst eine Zufallsvariable. Er wird ha&#x0308;ufig auch als Maß fu&#x0308;r die Evidenz gegen die Nullhypothese interpretiert. Allerdings kann man zeigen, dass \(p(X)
\sim \U ((0, 1))\), falls \(H_0\) gilt.<br />
Fu&#x0308;r Tests der Form \(\delta (X) := \1_{\{T(X) \ge c\}}\) lautet eine alternative Definition wie folgt:<br />
\(p(x) := \sup _{\theta \in \Theta _0} \PP _\theta (T(X) \ge T(x))\) (wobei \(\sup _{\theta \in \Theta _0} \PP _\theta (T(X) \ge c)\) die gro&#x0308;ßte Fehlerwahrscheinlichkeit 1. Art ist, die bei einem
bestimmten \(c\) auftreten kann).
</p>

<p>
<em>Beispiel</em>: Beim einseitigen Gauß-Test gilt \(\delta (x) = 1 \iff \overline {x} - \mu _0 \ge \frac {\sigma }{\sqrt {n}} z_{1-\alpha } = \frac {\sigma }{\sqrt {n}} \Phi ^{-1}(1-\alpha )\)<br />
\(\iff 1 - \alpha \ge \Phi \!\left (\frac {\overline {x} - \mu _0}{\sigma /\sqrt {n}}\right )\). Damit ist \(p(x) = 1 - \Phi \!\left (\frac {\overline {x} - \mu _0}{\sigma /\sqrt {n}}\right )\)
der \(p\)-Wert des Tests.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Der einseitige Gauß-Test hat fu&#x0308;r \(\mu _0 = 0\) die Gu&#x0308;tefunktion<br />
\(G_\delta (\mu ) = 1 - \Phi \!\left (\frac {c - \mu }{\sigma /\sqrt {n}}\right ) = 1 - \Phi \!\left (z_{1-\alpha } - \frac {\mu }{\sigma /\sqrt {n}}\right )\), d.&#x202f;h. fu&#x0308;r \(\mu
&gt; 0\) die Fehlerwahrscheinlichkeit 2. Art \(\Phi \!\left (z_{1-\alpha } - \frac {\mu }{\sigma /\sqrt {n}}\right )\). Ist \(\mu \) nur unwesentlich gro&#x0308;ßer \(0\), dann ist dies ungefa&#x0308;hr gleich \(1
- \alpha \), d.&#x202f;h. fast gleich \(1\). Eine Lo&#x0308;sung dieses Problems ist, auf die Kontrolle des Fehlers 2. Art in der sog. <em><span class="dashuline" >Indifferenzzone</span></em> \(\mu \in (0, \Delta )\) zu
verzichten, wobei \(\Delta \) die minimale relevante Abweichung von \(\mu = 0\) darstellt. Damit kann man die Fehlerwahrscheinlichkeit 2. Art (auch <em><span class="dashuline" >\(\beta \)-Fehler</span></em>) im
modifzierten Test \(H_0\colon \mu \le 0\) vs. \(H_\Delta \colon \mu \ge \Delta \) kontrollieren. Fu&#x0308;r einen vorgebenenen maximalen Fehler \(\beta \) ist \(\beta = \Phi \!\left (z_{1-\alpha } - \frac
{\Delta }{\sigma /\sqrt {n}}\right ) \iff \Delta = \frac {\sigma }{\sqrt {n}} (z_{1-\alpha } - z_\beta )\). Bei vorgegebenem \(\beta \) und \(\Delta \) betra&#x0308;gt der minimale Stichprobenumfang
\(n \ge \frac {\sigma ^2(z_{1-\alpha } - z_\beta )^2}{\Delta ^2}\).
</p>



<h2 id="dualitaet-zwischen-konfidenzintervallen-und-hypothesentests">Dualität zwischen Konfidenzintervallen und Hypothesentests</h2>

</p>


<p>
<em>Beispiel</em>: Seien \(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\) mit \(\theta = \mu \in \Theta := \real \) unbekannt und \(\sigma ^2\) bekannt. Den <em><span class="dashuline" >zweiseitigen <span
class="textsc" >Gauß</span>-Test</span></em> fu&#x0308;r den Erwartungswert \(\mu \) kann man aus dem \((1 - \alpha )\)-KI \(\overline {X} \pm z_{1-\alpha /2} \frac {\sigma }{\sqrt {n}}\) fu&#x0308;r
\(\mu \) herleiten: Es gilt nach der Definition eines Konfidenzintervalls, dass<br />
\(\forall _{\theta \in \Theta }\; \PP _\theta (\theta \in [\Tu (X), \To (X)]) \ge 1 - \alpha \iff \forall _{\theta \in \Theta }\; \PP _\theta (\theta \notin [\Tu (X), \To (X)]) \le \alpha \).
Dabei gilt \(\theta \notin \overline {X} \pm z_{1-\alpha /2} \frac {\sigma }{\sqrt {n}}\) genau dann, wenn \(\left |\frac {\overline {X} - \theta }{\sigma /\sqrt {n}}\right | \ge z_{1-\alpha /2}\)
gilt. Daher ist die KI-Definition a&#x0308;quivalent zu \(\forall _{\theta _0 \in \Theta }\; \PP _{\theta _0}(\delta (X) = 1) \le \alpha \) mit \(\delta (X) := \1_{\{|T(X)| \ge z_{1-\alpha /2}\}}\) und
\(T(X) := \frac {\overline {X} - \theta }{\sigma /\sqrt {n}}\). Man erha&#x0308;lt also einen zweiseitigen Hypothesentest \(\delta (X)\) zum Niveau \(\alpha \). Die Rechnung kann man auch umgekehrt
fu&#x0308;hren (ausgehend von einem Test zum Niveau \(\alpha )\). Allgemeiner gilt folgender Satz.
</p>

<p>
<span class="uline" ><em>Satz</em> (<span class="textsl" >Dualita&#x0308;tssatz</span>):</span>
</p>
<ul style="list-style-type:none">

<li class="list-item-f4"><p>Ist \([\Tu (X), \To (X)]\) ein \((1-\alpha )\)-Konfidenzintervall fu&#x0308;r \(\theta \), so ist \(\delta (X, \theta _0) := \1_{\{\theta _0 \notin [\Tu (X), \To (X)]\}}\) fu&#x0308;r alle
\(\theta _0 \in \Theta \) ein (nicht-randomisierter) Hypothesentest zum Niveau \(\alpha \) fu&#x0308;r \(H_0\colon \theta = \theta _0\) vs. \(H_1\colon \theta \not = \theta _0\).
</p>
</li>
<li class="list-item-f5"><p>Ist \(\delta (X, \theta _0)\) ein (nicht-randomisierter) Hypothesentest zum Niveau \(\alpha \) fu&#x0308;r \(H_0\colon \theta = \theta _0\) vs. \(H_1\colon \theta \not = \theta _0\) und existieren
Statistiken \(\Tu (X), \To (X)\) mit \(\forall _{x \in \X }\; \{\theta _0 \in \Theta \;|\; \delta (x, \theta _0) = 0\} = [\Tu (x), \To (x)]\), so ist \([\Tu (X), \To (X)]\) ein \((1-\alpha
)\)-Konfidenzintervall fu&#x0308;r \(\theta \).
</p>
</li>
</ul>



<h2 id="bayesianisches-testen"><span style="font-variant: small-caps;">Bayes</span>ianisches Testen</h2>

</p>


<p>
<em>Bemerkung</em>: Beim bayesianischen Ansatz ist \(\theta \) eine Zufallsvariable, wobei \(\theta \sim \pi \) mit der <em><span class="dashuline" >a-priori-Dichte</span></em> \(\pi \) (Za&#x0308;hl-/L.-B.-Dichte).
\(X|\theta \sim p(\cdot |\theta )\) ist die sogenannte <em><span class="dashuline" >Likelihood</span></em> von \(X\) und \(\theta |X=x \sim p(\cdot ,x)\) der <em><span class="dashuline"
>a-posteriori-Dichte</span></em>. Die a-posteriori-Dichte berechnet sich nach der Formel von Bayes \(p(\cdot |x) = \frac {\pi (\theta ) p(x|\theta )}{m(x)}\) mit \(m(x) := \sum _{\theta _i’} \pi (\theta _i’)
p(\theta _i’|x)\) bzw. \(m(x) := \int \pi (\theta ’) p(\theta ’|x) \d \theta ’\) (falls \(\theta \) diskret bzw. stetig verteilt ist).
</p>

<p>
\(\theta \) nimmt nur Werte in \(\Theta = \Theta _0 \mathbin {\dot {\cup }} \Theta _1\) an. Das Hypothesenpaar ist wie u&#x0308;blich \(H_0\colon \theta \in \Theta _0\) vs. \(H_1\colon \theta \in \Theta
_1\). Die a-priori-Wahrscheinlichkeit fu&#x0308;r \(H_0\) betra&#x0308;gt \(\pi _0 := \int _{\theta \in \Theta _0} \pi (\theta )\d \theta \), die fu&#x0308;r \(H_1\) betra&#x0308;gt \(\pi _1 := \int _{\theta
\in \Theta _1} \pi (\theta )\d \theta \). Die a-posteriori-Wahrscheinlichkeiten berechnen sich nach der Formel von Bayes: \(\PP (H_0|X=x) = \int _{\Theta _0} p(\theta |x)\d \theta \) bzw. \(\PP (H_1|X=x) =
\int _{\Theta _1} p(\theta |x)\d \theta \) mit \(p(\theta |x)\) wie oben. (Zum Beispiel ist \(\PP (H_0|X=x) = \frac {\int _{\Theta _0} p(x|\theta )\pi (\theta )\d \theta } {\int _\Theta p(x|\theta
)\pi (\theta )\d \theta }\).)
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Seien wieder \(X_1, \dotsc , X_n \sim \N (\theta , \sigma ^2)\) mit \(\theta \in \Theta = \Theta _0 \mathbin {\dot {\cup }} \Theta _1 = \real \) unbekannt und \(\sigma ^2 &gt; 0\)
bekannt. Dann gilt \(\overline {X} \sim \N \!\left (\theta , \frac {\sigma ^2}{n}\right )\). Geht man von der a-priori-Verteilung \(\theta \sim \N (\mu , \tau ^2)\) aus, so ist es (rechnerisch und
interpretatorisch) sinnvoll, die Varianz \(\tau ^2\) als \(\sigma ^2/n_0\) zu schreiben mit \(n_0 := \sigma ^2/\tau ^2\) dem sog. <em><span class="dashuline" >impliziten Stichprobenumfang</span></em>, also \(\theta
\sim \N (\mu , \sigma ^2/n_0)\). Man erha&#x0308;lt so eine a-posteriori-Verteilung von \(\theta |X=x \sim \N \!\left (\frac {n_0 \mu + n\overline {x}}{n_0 + n}, \frac {\sigma ^2}{n_0+n}\right )\).
</p>

<p>
Fu&#x0308;r das Hypothesenpaar \(H_0\colon \theta \le \theta _0\) vs. \(H_1\colon \theta &gt; \theta _0\) erha&#x0308;lt man also eine a-priori-Wahrscheinlichkeit fu&#x0308;r \(H_0\) bzw. \(H_1\) von \(\PP
(H_0) = \PP (\Theta _0) = \int _{\Theta _0} \pi (\theta )\d \theta = \int _{(-\infty , \theta _0]} \pi (\theta )\d \theta = \Phi \!\left (\frac {\theta _0 - \mu }{\sigma /\sqrt {n_0}}\right
)\) bzw. von \(\PP (H_1) = 1 - \Phi \!\left (\frac {\theta _0 - \mu }{\sigma /\sqrt {n_0}}\right )\).
</p>

<p>
Die a-posteriori-Wahrscheinlichkeit fu&#x0308;r \(H_0\) betra&#x0308;gt \(\PP (H_0|X=x) = \frac {\int _{\Theta _0} p(\theta |x)\d \theta } {\int _\Theta p(\theta |x)\pi (\theta )\d \theta }\)<br />
\(= \int _{\Theta _0} p(\theta |x)\d \theta = \Phi \!\left (\frac {\theta _0 - \frac {n_0 \mu + n\overline {x}}{n_0 + n}} {\sigma /\sqrt {n_0+n}}\right ) \xrightarrow {n_0 \to 0} \Phi \!\left
(\frac {\theta _0 - \overline {x}}{\sigma /\sqrt {n}}\right )\). Der Grenzwert stellt die a-posteriori-Wahrscheinlichkeit fu&#x0308;r \(H_0\) bei einer uninformativen a-priori-Verteilung dar.
</p>

<p>
Berechnet man den frequentistischen \(p\)-Wert, so erha&#x0308;lt man<br />
\(\PP (\overline {X} \ge \overline {x}|H_0) = \PP _{\theta _0}\!\left (\frac {\overline {X} - \theta _0}{\sigma /\sqrt {n}} \ge \frac {\overline {x} - \theta _0}{\sigma /\sqrt {n}}\right ) =
1 - \Phi \!\left (\frac {\overline {x} - \theta _0}{\sigma /\sqrt {n}}\right ) = \Phi \!\left (\frac {\theta _0 - \overline {x}}{\sigma /\sqrt {n}}\right )\), also denselben Wert.
</p>

<p>
Daher konvergiert hier die a-posteriori-Wahrscheinlichkeit fu&#x0308;r \(H_0\) gegen den (frequentistischen) \(p\)-Wert, falls die a-priori-Verteilung fu&#x0308;r \(\theta \) gegen die uninformative a-priori-Verteilung
konvergiert.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Der Vergleich der <em><span class="dashuline" >a-priori-Chancen</span></em> (oder <em><span class="dashuline" >Odds</span></em>) von \(H_1\) vs. \(H_0\) erfolgt mit der Formel \(\frac {\pi
_1}{\pi _0} = \frac {\int _{\Theta _1} \pi (\theta )\d \theta }{\int _{\Theta _0} \pi (\theta )\d \theta }\) (wobei man im Falle einer Za&#x0308;hldichte \(\pi (\theta )\) die Integrale durch Summen
ersetzt).
</p>

<p>
Der Vergleich der <em><span class="dashuline" >a-posteriori-Chancen</span></em> von \(H_1\) vs. \(H_0\) la&#x0308;uft analog mit<br />
\(\frac {p_1}{p_0} = \frac {\PP (\theta \in \Theta _1|X=x)}{\PP (\theta \in \Theta _0|X=x)} = \frac {\int _{\Theta _1} p(\theta |X=x)\d \theta } {\int _{\Theta _0} p(\theta |X=x)\d \theta } =
\frac {\int _{\Theta _1} \pi (\theta )p(x|\theta )\d \theta } {\int _{\Theta _0} \pi (\theta )p(x|\theta )\d \theta } = B \cdot \frac {\pi _1}{\pi _0}\),<br />
wobei \(B := \frac {\int _{\Theta _1} \pi (\theta )p(x|\theta )/\pi _1\d \theta } {\int _{\Theta _0} \pi (\theta )p(x|\theta )/\pi _0\d \theta }\) der sog. <em><span class="dashuline" ><span
class="textsc" >Bayes</span>-Faktor</span></em> darstellt. Die a-posteriori-Odds ergeben sich also als Produkt des Bayes-Faktors (der alle Informationen u&#x0308;ber die Daten entha&#x0308;lt) mit den a-priori-Odds. Der
Bayes-Faktor gibt dabei an, in welchem Maße die a-priori-Odds korrigiert werden mu&#x0308;ssen. Er spielt im bayesianischen Testen eine a&#x0308;hnliche Rolle wie der \(p\)-Wert im frequentistischen Testen.
</p>

<p>
Bewertung des Bayes-Faktors nach Jeffrey:
</p>
<table>

<tr class="tbrule">
<td class="tdc"><b>\(B\)</b></td>
<td class="tdl"><b>Wie stark spricht \(H_1\) gegen \(H_0\)?</b></td>
</tr>


<tr class="hline" >
<td class="tdc">1 – 3</td>
<td class="tdl">kaum der Rede wert</td>
</tr>


<tr class="hline" >
<td class="tdc">3 – 10</td>
<td class="tdl">substanziell</td>
</tr>


<tr class="hline" >
<td class="tdc">10 – 30</td>
<td class="tdl">stark</td>
</tr>


<tr class="hline" >
<td class="tdc">30 – 100</td>
<td class="tdl">sehr stark</td>
</tr>


<tr class="hline" >
<td class="tdc">\(&gt;\) 100</td>
<td class="tdl">entschieden</td>
</tr>


<tr class="tbrule">
<td class="tdc"></td>
<td class="tdl"></td>
</tr>

</table>

<p>
Eine a&#x0308;hnliche Tabelle la&#x0308;sst sich fu&#x0308;r den \(p\)-Wert aufstellen (inklusive der z.&#x202f;B. in <b>R</b> gebra&#x0308;uchlichen Symbole):
</p>
<table>

<tr class="tbrule">
<td class="tdc"><b>\(p\)-Wert</b></td>
<td class="tdl"><b>Wie stark spricht \(H_1\) gegen \(H_0\)?</b></td>
</tr>


<tr class="hline" >
<td class="tdc">\(\num {0.05}\) – \(\num {0.1}\)</td>
<td class="tdl">schwach signifikant (\(\cdot \))</td>
</tr>


<tr class="hline" >
<td class="tdc">\(\num {0.01}\) – \(\num {0.05}\)</td>
<td class="tdl">signifikant (\(\ast \))</td>
</tr>


<tr class="hline" >
<td class="tdc">\(\num {0.001}\) – \(\num {0.01}\)</td>
<td class="tdl">stark signifikant (\(\ast \ast \))</td>
</tr>


<tr class="hline" >
<td class="tdc">\(&lt; \num {0.001}\)</td>
<td class="tdl">sehr stark signifikant (\(\ast {\ast }\ast \))</td>
</tr>


<tr class="tbrule">
<td class="tdc"></td>
<td class="tdl"></td>
</tr>

</table>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Fu&#x0308;r einfache Hypothesen \(\Theta = \{\theta _0, \theta _1\}\) mit \(H_0\colon \theta = \theta _0\) und \(H_1\colon \theta = \theta _1\) gilt \(B = \frac {p(x, \theta
_1)}{p(x, \theta _0)}\) (<em><span class="dashuline" >Likelihood-Quotient</span></em>, siehe na&#x0308;chstes Kapitel).
</p>



<h2 id="zusatz-gaengige-konfidenzintervalle-und-hypothesentests">Zusatz: Gängige Konfidenzintervalle und-Hypothesentests</h2>

</p>


<p>
<b>Einstichproben-Konfidenzintervalle</b>: \([\Tu (X), \To (X)]\) mit \(X_1, \dotsc , X_n\) i.i.d.
</p>
<table>

<tr class="tbrule">
<td class="tdl"><em>Zufallsstichprobe</em></td>
<td class="tdl"><em>Zielgro&#x0308;ße</em></td>
<td class="tdl"><em>Parameter</em></td>
<td class="tdl"><em>Herleitung: \(1 - \alpha \; \cdots \)</em></td>
<td class="tdl"><em>(appr.) \((1-\alpha )\)-KI \([\Tu (X), \To (X)]\)</em></td>
</tr>


<tr class="hline" >
<td class="tdl">\(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\)</td>
<td class="tdl">\(\mu \)</td>
<td class="tdl">\(\theta = \mu \) unbek., \(\sigma ^2\) bek.</td>
<td class="tdl">\(= \PP _\theta \!\left (\left |\frac {\overline {X} - \mu } {\sigma /\sqrt {n}}\right | \le z_{1-\alpha /2}\right )\)</td>
<td class="tdl">\(\overline {X} \pm \frac {\sigma }{\sqrt {n}} z_{1-\alpha /2}\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl tdrule">\(\theta = (\mu , \sigma ^2)\) unbek.</td>
<td class="tdl tdrule">\(= \PP _\theta \!\left (\left |\frac {\overline {X} - \mu } {S(X)/\sqrt {n}}\right | \le t_{n-1,1-\alpha /2}\right )\)</td>
<td class="tdl tdruler">\(\overline {X} \pm \frac {S(X)}{\sqrt {n}} t_{n-1,1-\alpha /2}\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl tdrule">\(\sigma ^2\)</td>
<td class="tdl tdrule">\(\mu \) bek., \(\theta = \sigma ^2\) unbek.</td>
<td class="tdl tdrule">\(= \PP _\theta \!\left (\frac {{S^\ast }^2(X)}{\sigma ^2/n} \in \left [\chi _{n,\alpha /2}^2, \chi _{n,1-\alpha /2}^2\right ]\right )\)</td>
<td class="tdl tdruler">\(\left [\frac {n {S^\ast }^2(X)}{\chi _{n,1-\alpha /2}^2}, \frac {n {S^\ast }^2(X)}{\chi _{n,\alpha /2}^2}\right ]\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl tdrule">\(\theta = (\mu , \sigma ^2)\) unbek.</td>
<td class="tdl tdrule">\(= \PP _\theta \!\left (\frac {S^2(X)}{\sigma ^2/(n-1)} \in \left [\chi _{n-1,\alpha /2}^2, \chi _{n-1,1-\alpha /2}^2\right ]\right )\)</td>
<td class="tdl tdruler">\(\left [\frac {(n-1) S^2(X)}{\chi _{n-1,1-\alpha /2}^2}, \frac {(n-1) S^2(X)}{\chi _{n-1,\alpha /2}^2}\right ]\)</td>
</tr>


<tr class="hline" >
<td class="tdl">\(X_1, \dotsc , X_n \sim \Bin (1, p)\)</td>
<td class="tdl">\(p\)</td>
<td class="tdl">\(\theta = p\)</td>
<td class="tdl">\(\approx \PP _\theta \!\left (\left |\frac {\overline {X} - p} {\sqrt {p(1-p)/n}}\right | \le z_{1-\alpha /2}\right )\)</td>
<td class="tdl">\(\left \{p \in [0, 1] \;\left |\; n (\overline {X} - p)^2 \le (z_{1-\alpha /2})^2 p (1-p)\right .\right \}\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl tdrule">\(\approx \PP _\theta \!\left (\left |\frac {\overline {X} - p} {\sqrt {\overline {X}(1-\overline {X})/n}}\right | \le z_{1-\alpha /2}\right )\)</td>
<td class="tdl tdruler">\(\overline {X} \pm \sqrt {\frac {\overline {X} (1 - \overline {X})}{n}} z_{1-\alpha /2}\)</td>
</tr>


<tr class="tbrule">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>

</table>

<p>
<b>Einstichproben-Hypothesentests</b>: \(\delta (X) := \1_{\{T(X) \ge c\}}\) mit \(X_1, \dotsc , X_n\) i.i.d.
</p>
<table>

<tr class="tbrule">
<td class="tdl"><em>Zufallsstichprobe</em></td>
<td class="tdl"><em>Parameter</em></td>
<td class="tdl"><em>Testname</em></td>
<td class="tdl"><em>Hypothesen</em></td>
<td class="tdl"><em>Teststatistik \(T(X)\)</em></td>
<td class="tdl"><em>kritischer Wert \(c\)</em></td>
</tr>


<tr class="hline" >
<td class="tdl">\(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\)</td>
<td class="tdl">\(\theta = \mu \) unbek., \(\sigma ^2\) bek.</td>
<td class="tdl">Gauß-Test</td>
<td class="tdl">\(H_0\colon \mu \le \mu _0\) vs. \(H_1\colon \mu &gt; \mu _0\)</td>
<td class="tdl">\(\frac {\overline {X} - \mu _0}{\sigma /\sqrt {n}}\)</td>
<td class="tdl">\(z_{1-\alpha }\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl tdrule">\(H_0\colon \mu \ge \mu _0\) vs. \(H_1\colon \mu &lt; \mu _0\)</td>
<td class="tdl tdrule">\(\frac {\mu _0 - \overline {X}}{\sigma /\sqrt {n}}\)</td>
<td class="tdl tdruler">\(z_{1-\alpha }\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl tdrule">\(H_0\colon \mu = \mu _0\) vs. \(H_1\colon \mu \not = \mu _0\)</td>
<td class="tdl tdrule">\(\left |\frac {\overline {X} - \mu _0}{\sigma /\sqrt {n}}\right |\)</td>
<td class="tdl tdruler">\(z_{1-\alpha /2}\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl tdrule">\(\theta = (\mu , \sigma ^2)\) unbek.</td>
<td class="tdl tdrule">\(t\)-Test</td>
<td class="tdl tdrule">\(H_0\colon \mu \le \mu _0\) vs. \(H_1\colon \mu &gt; \mu _0\)</td>
<td class="tdl tdrule">\(\frac {\overline {X} - \mu _0}{S(X)/\sqrt {n}}\)</td>
<td class="tdl tdruler">\(t_{n-1,1-\alpha }\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl tdrule">\(H_0\colon \mu \ge \mu _0\) vs. \(H_1\colon \mu &lt; \mu _0\)</td>
<td class="tdl tdrule">\(\frac {\mu _0 - \overline {X}}{S(X)/\sqrt {n}}\)</td>
<td class="tdl tdruler">\(t_{n-1,1-\alpha }\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl tdrule">\(H_0\colon \mu = \mu _0\) vs. \(H_1\colon \mu \not = \mu _0\)</td>
<td class="tdl tdrule">\(\left |\frac {\overline {X} - \mu _0}{S(X)/\sqrt {n}}\right |\)</td>
<td class="tdl tdruler">\(t_{n-1,1-\alpha /2}\)</td>
</tr>


<tr class="tbrule">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>

</table>

<p>
<b>Zweistichproben-Konfidenzintervalle</b>:<br />
\([\Tu (X, Y), \To (X, Y)]\) mit \(X_1, \dotsc , X_n \sim \N (\mu _X, \sigma _X^2)\) i.i.d., \(Y_1, \dotsc , Y_n \sim \N (\mu _Y, \sigma _Y^2)\) i.i.d. und \(X_1, \dotsc , X_n, Y_1, \dotsc , Y_n\)
unabha&#x0308;ngig
</p>
<table>

<tr class="tbrule">
<td class="tdl"><em>Zielgro&#x0308;ße</em></td>
<td class="tdl"><em>Parameter</em></td>
<td class="tdl"><em>Herleitung: \(1 - \alpha \; \cdots \)</em></td>
<td class="tdl"><em>(appr.) \((1-\alpha )\)-KI \([\Tu (X, Y), \To (X, Y)]\)</em></td>
</tr>


<tr class="hline" >
<td class="tdl">\(\mu _X - \mu _Y\)</td>
<td class="tdl">\(\theta = (\mu _X, \mu _Y)\) unbek., \(\sigma _X^2, \sigma _Y^2\) bek.</td>
<td class="tdl">\(= \PP _\theta \!\left (\left |\frac {(\overline {X} - \overline {Y}) - (\mu _X - \mu _Y)} {\sqrt {(\sigma _X^2 + \sigma _Y^2)/n}}\right | \le z_{1-\alpha /2}\right )\)</td>
<td class="tdl">\((\overline {X} - \overline {Y}) \pm \sqrt {\frac {\sigma _X^2 + \sigma _Y^2}{n}} z_{1-\alpha /2}\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl tdrule">\(\theta = (\mu _X, \mu _Y, \sigma _X^2, \sigma _Y^2)\) unbek.</td>
<td class="tdl tdrule">\(= \PP _\theta \!\left (\left |\frac {(\overline {X} - \overline {Y}) - (\mu _X - \mu _Y)} {S(X - Y)/\sqrt {n}}\right | \le t_{n-1,1-\alpha /2}\right )\)</td>
<td class="tdl tdruler">\((\overline {X} - \overline {Y}) \pm \frac {S(X - Y)}{\sqrt {n}} t_{n-1,1-\alpha /2}\)</td>
</tr>


<tr class="hline" >
<td class="tdl">\(\sigma _X^2/\sigma _Y^2\)</td>
<td class="tdl">\(\mu _X, \mu _Y\) bek., \(\theta = (\sigma _X^2, \sigma _Y^2)\) unbek.</td>
<td class="tdl">\(= \PP _\theta \!\left (\frac {{S^\ast }^2(X)/\sigma _X^2}{{S^\ast }^2(Y)/\sigma _Y^2} \in \left [f_{n,n,\alpha /2}, f_{n,n,1-\alpha /2}\right ]\right )\)</td>
<td class="tdl">\(\left [\frac {{S^\ast }^2(X)/{S^\ast }^2(Y)}{f_{n,n,1-\alpha /2}}, \frac {{S^\ast }^2(X)/{S^\ast }^2(Y)}{f_{n,n,\alpha /2}}\right ]\)</td>
</tr>


<tr>
<td class="tdl"></td>
<td class="tdl tdrule">\(\theta = (\mu _X, \mu _Y, \sigma _X^2, \sigma _Y^2)\) unbek.</td>
<td class="tdl tdrule">\(= \PP _\theta \!\left (\frac {S^2(X)/\sigma _X^2}{S^2(Y)/\sigma _Y^2} \in \left [f_{n-1,n-1,\alpha /2}, f_{n-1,n-1,1-\alpha /2}\right ]\right )\)</td>
<td class="tdl tdruler">\(\left [\frac {S^2(X)/S^2(Y)}{f_{n-1,n-1,1-\alpha /2}}, \frac {S^2(X)/S^2(Y)}{f_{n-1,n-1,\alpha /2}}\right ]\)</td>
</tr>


<tr class="tbrule">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>

</table>

{% endraw %}
</div>
{:/nomarkdown}
