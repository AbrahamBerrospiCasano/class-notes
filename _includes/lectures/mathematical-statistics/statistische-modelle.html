
{::nomarkdown}
<div class="lwarp-contents">
{% raw %}
<div class="hidden" >

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\newcommand {\mathllap }[2][]{{#1#2}}\)

\(\newcommand {\mathrlap }[2][]{{#1#2}}\)

\(\newcommand {\mathclap }[2][]{{#1#2}}\)

\(\newcommand {\mathmbox }[1]{#1}\)

\(\newcommand {\clap }[1]{#1}\)

\(\newcommand {\LWRmathmakebox }[2][]{#2}\)

\(\newcommand {\mathmakebox }[1][]{\LWRmathmakebox }\)

\(\newcommand {\cramped }[2][]{{#1#2}}\)

\(\newcommand {\crampedllap }[2][]{{#1#2}}\)

\(\newcommand {\crampedrlap }[2][]{{#1#2}}\)

\(\newcommand {\crampedclap }[2][]{{#1#2}}\)

\(\newenvironment {crampedsubarray}[1]{}{}\)

\(\newcommand {\crampedsubstack }{}\)

\(\newcommand {\smashoperator }[2][]{#2\limits }\)

\(\newcommand {\adjustlimits }{}\)

\(\newcommand {\SwapAboveDisplaySkip }{}\)

\(\require {extpfeil}\)

\(\Newextarrow \xleftrightarrow {10,10}{0x2194}\)

\(\Newextarrow \xLeftarrow {10,10}{0x21d0}\)

\(\Newextarrow \xhookleftarrow {10,10}{0x21a9}\)

\(\Newextarrow \xmapsto {10,10}{0x21a6}\)

\(\Newextarrow \xRightarrow {10,10}{0x21d2}\)

\(\Newextarrow \xLeftrightarrow {10,10}{0x21d4}\)

\(\Newextarrow \xhookrightarrow {10,10}{0x21aa}\)

\(\Newextarrow \xrightharpoondown {10,10}{0x21c1}\)

\(\Newextarrow \xleftharpoondown {10,10}{0x21bd}\)

\(\Newextarrow \xrightleftharpoons {10,10}{0x21cc}\)

\(\Newextarrow \xrightharpoonup {10,10}{0x21c0}\)

\(\Newextarrow \xleftharpoonup {10,10}{0x21bc}\)

\(\Newextarrow \xleftrightharpoons {10,10}{0x21cb}\)

\(\newcommand {\LWRdounderbracket }[3]{\underset {#3}{\underline {#1}}}\)

\(\newcommand {\LWRunderbracket }[2][]{\LWRdounderbracket {#2}}\)

\(\newcommand {\underbracket }[1][]{\LWRunderbracket }\)

\(\newcommand {\LWRdooverbracket }[3]{\overset {#3}{\overline {#1}}}\)

\(\newcommand {\LWRoverbracket }[2][]{\LWRdooverbracket {#2}}\)

\(\newcommand {\overbracket }[1][]{\LWRoverbracket }\)

\(\newcommand {\LaTeXunderbrace }[1]{\underbrace {#1}}\)

\(\newcommand {\LaTeXoverbrace }[1]{\overbrace {#1}}\)

\(\newenvironment {matrix*}[1][]{\begin {matrix}}{\end {matrix}}\)

\(\newenvironment {pmatrix*}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bmatrix*}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bmatrix*}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vmatrix*}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vmatrix*}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newenvironment {smallmatrix*}[1][]{\begin {matrix}}{\end {matrix}}\)

\(\newenvironment {psmallmatrix*}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bsmallmatrix*}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bsmallmatrix*}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vsmallmatrix*}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vsmallmatrix*}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newenvironment {psmallmatrix}[1][]{\begin {pmatrix}}{\end {pmatrix}}\)

\(\newenvironment {bsmallmatrix}[1][]{\begin {bmatrix}}{\end {bmatrix}}\)

\(\newenvironment {Bsmallmatrix}[1][]{\begin {Bmatrix}}{\end {Bmatrix}}\)

\(\newenvironment {vsmallmatrix}[1][]{\begin {vmatrix}}{\end {vmatrix}}\)

\(\newenvironment {Vsmallmatrix}[1][]{\begin {Vmatrix}}{\end {Vmatrix}}\)

\(\newcommand {\LWRmultlined }[1][]{\begin {multline*}}\)

\(\newenvironment {multlined}[1][]{\LWRmultlined }{\end {multline*}}\)

\(\let \LWRorigshoveleft \shoveleft \)

\(\renewcommand {\shoveleft }[1][]{\LWRorigshoveleft }\)

\(\let \LWRorigshoveright \shoveright \)

\(\renewcommand {\shoveright }[1][]{\LWRorigshoveright }\)

\(\newenvironment {dcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {dcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {rcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {rcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {drcases}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {drcases*}{\begin {cases}}{\end {cases}}\)

\(\newenvironment {cases*}{\begin {cases}}{\end {cases}}\)

\(\newcommand {\MoveEqLeft }[1][]{}\)

\(\def \LWRAboxed #1&amp;#2&amp;#3!|!{\fbox {\(#1\)}&amp;\fbox {\(#2\)}} \newcommand {\Aboxed }[1]{\LWRAboxed #1&amp;&amp;!|!} \)

\( \newcommand {\LWRABLines }[1][\Updownarrow ]{#1 \notag \\}\newcommand {\ArrowBetweenLines }{\ifstar \LWRABLines \LWRABLines } \)

\(\newcommand {\shortintertext }[1]{\text {#1}\notag \\}\)

\(\newcommand {\vdotswithin }[1]{\hspace {.5em}\vdots }\)

\(\newcommand {\LWRshortvdotswithinstar }[1]{\vdots \hspace {.5em} &amp; \\}\)

\(\newcommand {\LWRshortvdotswithinnostar }[1]{&amp; \hspace {.5em}\vdots \\}\)

\(\newcommand {\shortvdotswithin }{\ifstar \LWRshortvdotswithinstar \LWRshortvdotswithinnostar }\)

\(\newcommand {\MTFlushSpaceAbove }{}\)

\(\newcommand {\MTFlushSpaceBelow }{\\}\)

\(\newcommand \lparen {(}\)

\(\newcommand \rparen {)}\)

\(\newcommand {\ordinarycolon }{:}\)

\(\newcommand {\vcentcolon }{\mathrel {\mathop \ordinarycolon }}\)

\(\newcommand \dblcolon {\vcentcolon \vcentcolon }\)

\(\newcommand \coloneqq {\vcentcolon =}\)

\(\newcommand \Coloneqq {\dblcolon =}\)

\(\newcommand \coloneq {\vcentcolon {-}}\)

\(\newcommand \Coloneq {\dblcolon {-}}\)

\(\newcommand \eqqcolon {=\vcentcolon }\)

\(\newcommand \Eqqcolon {=\dblcolon }\)

\(\newcommand \eqcolon {\mathrel {-}\vcentcolon }\)

\(\newcommand \Eqcolon {\mathrel {-}\dblcolon }\)

\(\newcommand \colonapprox {\vcentcolon \approx }\)

\(\newcommand \Colonapprox {\dblcolon \approx }\)

\(\newcommand \colonsim {\vcentcolon \sim }\)

\(\newcommand \Colonsim {\dblcolon \sim }\)

\(\newcommand {\nuparrow }{\mathrel {\cancel {\uparrow }}}\)

\(\newcommand {\ndownarrow }{\mathrel {\cancel {\downarrow }}}\)

\(\newcommand {\bigtimes }{\mathop {\Large \times }\limits }\)

\(\newcommand {\prescript }[3]{{}^{#1}_{#2}#3}\)

\(\newenvironment {lgathered}{\begin {gathered}}{\end {gathered}}\)

\(\newenvironment {rgathered}{\begin {gathered}}{\end {gathered}}\)

\(\newcommand {\splitfrac }[2]{{}^{#1}_{#2}}\)

\(\let \splitdfrac \splitfrac \)

\(\newcommand {\LWRoverlaysymbols }[2]{\mathord {\smash {\mathop {#2\strut }\limits ^{\smash {\lower 3ex{#1}}}}\strut }}\)

\(\newcommand{\alphaup}{\unicode{x03B1}}\)

\(\newcommand{\betaup}{\unicode{x03B2}}\)

\(\newcommand{\gammaup}{\unicode{x03B3}}\)

\(\newcommand{\digammaup}{\unicode{x03DD}}\)

\(\newcommand{\deltaup}{\unicode{x03B4}}\)

\(\newcommand{\epsilonup}{\unicode{x03F5}}\)

\(\newcommand{\varepsilonup}{\unicode{x03B5}}\)

\(\newcommand{\zetaup}{\unicode{x03B6}}\)

\(\newcommand{\etaup}{\unicode{x03B7}}\)

\(\newcommand{\thetaup}{\unicode{x03B8}}\)

\(\newcommand{\varthetaup}{\unicode{x03D1}}\)

\(\newcommand{\iotaup}{\unicode{x03B9}}\)

\(\newcommand{\kappaup}{\unicode{x03BA}}\)

\(\newcommand{\varkappaup}{\unicode{x03F0}}\)

\(\newcommand{\lambdaup}{\unicode{x03BB}}\)

\(\newcommand{\muup}{\unicode{x03BC}}\)

\(\newcommand{\nuup}{\unicode{x03BD}}\)

\(\newcommand{\xiup}{\unicode{x03BE}}\)

\(\newcommand{\omicronup}{\unicode{x03BF}}\)

\(\newcommand{\piup}{\unicode{x03C0}}\)

\(\newcommand{\varpiup}{\unicode{x03D6}}\)

\(\newcommand{\rhoup}{\unicode{x03C1}}\)

\(\newcommand{\varrhoup}{\unicode{x03F1}}\)

\(\newcommand{\sigmaup}{\unicode{x03C3}}\)

\(\newcommand{\varsigmaup}{\unicode{x03C2}}\)

\(\newcommand{\tauup}{\unicode{x03C4}}\)

\(\newcommand{\upsilonup}{\unicode{x03C5}}\)

\(\newcommand{\phiup}{\unicode{x03D5}}\)

\(\newcommand{\varphiup}{\unicode{x03C6}}\)

\(\newcommand{\chiup}{\unicode{x03C7}}\)

\(\newcommand{\psiup}{\unicode{x03C8}}\)

\(\newcommand{\omegaup}{\unicode{x03C9}}\)

\(\newcommand{\Alphaup}{\unicode{x0391}}\)

\(\newcommand{\Betaup}{\unicode{x0392}}\)

\(\newcommand{\Gammaup}{\unicode{x0393}}\)

\(\newcommand{\Digammaup}{\unicode{x03DC}}\)

\(\newcommand{\Deltaup}{\unicode{x0394}}\)

\(\newcommand{\Epsilonup}{\unicode{x0395}}\)

\(\newcommand{\Zetaup}{\unicode{x0396}}\)

\(\newcommand{\Etaup}{\unicode{x0397}}\)

\(\newcommand{\Thetaup}{\unicode{x0398}}\)

\(\newcommand{\Varthetaup}{\unicode{x03F4}}\)

\(\newcommand{\Iotaup}{\unicode{x0399}}\)

\(\newcommand{\Kappaup}{\unicode{x039A}}\)

\(\newcommand{\Lambdaup}{\unicode{x039B}}\)

\(\newcommand{\Muup}{\unicode{x039C}}\)

\(\newcommand{\Nuup}{\unicode{x039D}}\)

\(\newcommand{\Xiup}{\unicode{x039E}}\)

\(\newcommand{\Omicronup}{\unicode{x039F}}\)

\(\newcommand{\Piup}{\unicode{x03A0}}\)

\(\newcommand{\Varpiup}{\unicode{x03D6}}\)

\(\newcommand{\Rhoup}{\unicode{x03A1}}\)

\(\newcommand{\Sigmaup}{\unicode{x03A3}}\)

\(\newcommand{\Tauup}{\unicode{x03A4}}\)

\(\newcommand{\Upsilonup}{\unicode{x03A5}}\)

\(\newcommand{\Phiup}{\unicode{x03A6}}\)

\(\newcommand{\Chiup}{\unicode{x03A7}}\)

\(\newcommand{\Psiup}{\unicode{x03A8}}\)

\(\newcommand{\Omegaup}{\unicode{x03A9}}\)

\(\newcommand{\alphait}{\unicode{x1D6FC}}\)

\(\newcommand{\betait}{\unicode{x1D6FD}}\)

\(\newcommand{\gammait}{\unicode{x1D6FE}}\)

\(\newcommand{\digammait}{\mathit{\unicode{x03DD}}}\)

\(\newcommand{\deltait}{\unicode{x1D6FF}}\)

\(\newcommand{\epsilonit}{\unicode{x1D716}}\)

\(\newcommand{\varepsilonit}{\unicode{x1D700}}\)

\(\newcommand{\zetait}{\unicode{x1D701}}\)

\(\newcommand{\etait}{\unicode{x1D702}}\)

\(\newcommand{\thetait}{\unicode{x1D703}}\)

\(\newcommand{\varthetait}{\unicode{x1D717}}\)

\(\newcommand{\iotait}{\unicode{x1D704}}\)

\(\newcommand{\kappait}{\unicode{x1D705}}\)

\(\newcommand{\varkappait}{\unicode{x1D718}}\)

\(\newcommand{\lambdait}{\unicode{x1D706}}\)

\(\newcommand{\muit}{\unicode{x1D707}}\)

\(\newcommand{\nuit}{\unicode{x1D708}}\)

\(\newcommand{\xiit}{\unicode{x1D709}}\)

\(\newcommand{\omicronit}{\unicode{x1D70A}}\)

\(\newcommand{\piit}{\unicode{x1D70B}}\)

\(\newcommand{\varpiit}{\unicode{x1D71B}}\)

\(\newcommand{\rhoit}{\unicode{x1D70C}}\)

\(\newcommand{\varrhoit}{\unicode{x1D71A}}\)

\(\newcommand{\sigmait}{\unicode{x1D70E}}\)

\(\newcommand{\varsigmait}{\unicode{x1D70D}}\)

\(\newcommand{\tauit}{\unicode{x1D70F}}\)

\(\newcommand{\upsilonit}{\unicode{x1D710}}\)

\(\newcommand{\phiit}{\unicode{x1D719}}\)

\(\newcommand{\varphiit}{\unicode{x1D711}}\)

\(\newcommand{\chiit}{\unicode{x1D712}}\)

\(\newcommand{\psiit}{\unicode{x1D713}}\)

\(\newcommand{\omegait}{\unicode{x1D714}}\)

\(\newcommand{\Alphait}{\unicode{x1D6E2}}\)

\(\newcommand{\Betait}{\unicode{x1D6E3}}\)

\(\newcommand{\Gammait}{\unicode{x1D6E4}}\)

\(\newcommand{\Digammait}{\mathit{\unicode{x03DC}}}\)

\(\newcommand{\Deltait}{\unicode{x1D6E5}}\)

\(\newcommand{\Epsilonit}{\unicode{x1D6E6}}\)

\(\newcommand{\Zetait}{\unicode{x1D6E7}}\)

\(\newcommand{\Etait}{\unicode{x1D6E8}}\)

\(\newcommand{\Thetait}{\unicode{x1D6E9}}\)

\(\newcommand{\Varthetait}{\unicode{x1D6F3}}\)

\(\newcommand{\Iotait}{\unicode{x1D6EA}}\)

\(\newcommand{\Kappait}{\unicode{x1D6EB}}\)

\(\newcommand{\Lambdait}{\unicode{x1D6EC}}\)

\(\newcommand{\Muit}{\unicode{x1D6ED}}\)

\(\newcommand{\Nuit}{\unicode{x1D6EE}}\)

\(\newcommand{\Xiit}{\unicode{x1D6EF}}\)

\(\newcommand{\Omicronit}{\unicode{x1D6F0}}\)

\(\newcommand{\Piit}{\unicode{x1D6F1}}\)

\(\newcommand{\Rhoit}{\unicode{x1D6F2}}\)

\(\newcommand{\Sigmait}{\unicode{x1D6F4}}\)

\(\newcommand{\Tauit}{\unicode{x1D6F5}}\)

\(\newcommand{\Upsilonit}{\unicode{x1D6F6}}\)

\(\newcommand{\Phiit}{\unicode{x1D6F7}}\)

\(\newcommand{\Chiit}{\unicode{x1D6F8}}\)

\(\newcommand{\Psiit}{\unicode{x1D6F9}}\)

\(\newcommand{\Omegait}{\unicode{x1D6FA}}\)

\(\let \digammaup \Digammaup \)

\(\renewcommand {\digammait }{\mathit {\digammaup }}\)

\(\newcommand {\smallin }{\unicode {x220A}}\)

\(\newcommand {\smallowns }{\unicode {x220D}}\)

\(\newcommand {\notsmallin }{\LWRoverlaysymbols {/}{\unicode {x220A}}}\)

\(\newcommand {\notsmallowns }{\LWRoverlaysymbols {/}{\unicode {x220D}}}\)

\(\newcommand {\rightangle }{\unicode {x221F}}\)

\(\newcommand {\intclockwise }{\unicode {x2231}}\)

\(\newcommand {\ointclockwise }{\unicode {x2232}}\)

\(\newcommand {\ointctrclockwise }{\unicode {x2233}}\)

\(\newcommand {\oiint }{\unicode {x222F}}\)

\(\newcommand {\oiiint }{\unicode {x2230}}\)

\(\newcommand {\ddag }{\unicode {x2021}}\)

\(\newcommand {\P }{\unicode {x00B6}}\)

\(\newcommand {\copyright }{\unicode {x00A9}}\)

\(\newcommand {\dag }{\unicode {x2020}}\)

\(\newcommand {\pounds }{\unicode {x00A3}}\)

\(\newcommand {\iddots }{\unicode {x22F0}}\)

\(\newcommand {\utimes }{\overline {\times }}\)

\(\newcommand {\dtimes }{\underline {\times }}\)

\(\newcommand {\udtimes }{\overline {\underline {\times }}}\)

\(\newcommand {\leftwave }{\left \{}\)

\(\newcommand {\rightwave }{\right \}}\)

\(\newcommand {\toprule }[1][]{\hline }\)

\(\let \midrule \toprule \)

\(\let \bottomrule \toprule \)

\(\newcommand {\cmidrule }[2][]{}\)

\(\newcommand {\morecmidrules }{}\)

\(\newcommand {\specialrule }[3]{\hline }\)

\(\newcommand {\addlinespace }[1][]{}\)

\(\newcommand {\LWRsubmultirow }[2][]{#2}\)

\(\newcommand {\LWRmultirow }[2][]{\LWRsubmultirow }\)

\(\newcommand {\multirow }[2][]{\LWRmultirow }\)

\(\newcommand {\mrowcell }{}\)

\(\newcommand {\mcolrowcell }{}\)

\(\newcommand {\STneed }[1]{}\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\newcommand {\tothe }[1]{^{#1}}\)

\(\newcommand {\raiseto }[2]{{#2}^{#1}}\)

\(\newcommand {\ang }[2][]{(\mathrm {#2})\degree }\)

\(\newcommand {\num }[2][]{\mathrm {#2}}\)

\(\newcommand {\si }[2][]{\mathrm {#2}}\)

\(\newcommand {\LWRSI }[2][]{\mathrm {#1\LWRSInumber \,#2}}\)

\(\newcommand {\SI }[2][]{\def \LWRSInumber {#2}\LWRSI }\)

\(\newcommand {\numlist }[2][]{\mathrm {#2}}\)

\(\newcommand {\numrange }[3][]{\mathrm {#2\,\unicode {x2013}\,#3}}\)

\(\newcommand {\SIlist }[3][]{\mathrm {#2\,#3}}\)

\(\newcommand {\SIrange }[4][]{\mathrm {#2\,#4\,\unicode {x2013}\,#3\,#4}}\)

\(\newcommand {\tablenum }[2][]{\mathrm {#2}}\)

\(\newcommand {\ampere }{\mathrm {A}}\)

\(\newcommand {\candela }{\mathrm {cd}}\)

\(\newcommand {\kelvin }{\mathrm {K}}\)

\(\newcommand {\kilogram }{\mathrm {kg}}\)

\(\newcommand {\metre }{\mathrm {m}}\)

\(\newcommand {\mole }{\mathrm {mol}}\)

\(\newcommand {\second }{\mathrm {s}}\)

\(\newcommand {\becquerel }{\mathrm {Bq}}\)

\(\newcommand {\degreeCelsius }{\unicode {x2103}}\)

\(\newcommand {\coulomb }{\mathrm {C}}\)

\(\newcommand {\farad }{\mathrm {F}}\)

\(\newcommand {\gray }{\mathrm {Gy}}\)

\(\newcommand {\hertz }{\mathrm {Hz}}\)

\(\newcommand {\henry }{\mathrm {H}}\)

\(\newcommand {\joule }{\mathrm {J}}\)

\(\newcommand {\katal }{\mathrm {kat}}\)

\(\newcommand {\lumen }{\mathrm {lm}}\)

\(\newcommand {\lux }{\mathrm {lx}}\)

\(\newcommand {\newton }{\mathrm {N}}\)

\(\newcommand {\ohm }{\mathrm {\Omega }}\)

\(\newcommand {\pascal }{\mathrm {Pa}}\)

\(\newcommand {\radian }{\mathrm {rad}}\)

\(\newcommand {\siemens }{\mathrm {S}}\)

\(\newcommand {\sievert }{\mathrm {Sv}}\)

\(\newcommand {\steradian }{\mathrm {sr}}\)

\(\newcommand {\tesla }{\mathrm {T}}\)

\(\newcommand {\volt }{\mathrm {V}}\)

\(\newcommand {\watt }{\mathrm {W}}\)

\(\newcommand {\weber }{\mathrm {Wb}}\)

\(\newcommand {\day }{\mathrm {d}}\)

\(\newcommand {\degree }{\mathrm {^\circ }}\)

\(\newcommand {\hectare }{\mathrm {ha}}\)

\(\newcommand {\hour }{\mathrm {h}}\)

\(\newcommand {\litre }{\mathrm {l}}\)

\(\newcommand {\liter }{\mathrm {L}}\)

\(\newcommand {\arcminute }{^\prime }\)
\(\newcommand {\minute }{\mathrm {min}}\)

\(\newcommand {\arcsecond }{^{\prime \prime }}\)

\(\newcommand {\tonne }{\mathrm {t}}\)

\(\newcommand {\astronomicalunit }{au}\)

\(\newcommand {\atomicmassunit }{u}\)

\(\newcommand {\bohr }{\mathit {a}_0}\)

\(\newcommand {\clight }{\mathit {c}_0}\)

\(\newcommand {\dalton }{\mathrm {D}_\mathrm {a}}\)

\(\newcommand {\electronmass }{\mathit {m}_{\mathrm {e}}}\)

\(\newcommand {\electronvolt }{\mathrm {eV}}\)

\(\newcommand {\elementarycharge }{\mathit {e}}\)

\(\newcommand {\hartree }{\mathit {E}_{\mathrm {h}}}\)

\(\newcommand {\planckbar }{\mathit {\unicode {x210F}}}\)

\(\newcommand {\angstrom }{\mathrm {\unicode {x212B}}}\)

\(\let \LWRorigbar \bar \)

\(\newcommand {\bar }{\mathrm {bar}}\)

\(\newcommand {\barn }{\mathrm {b}}\)

\(\newcommand {\bel }{\mathrm {B}}\)

\(\newcommand {\decibel }{\mathrm {dB}}\)

\(\newcommand {\knot }{\mathrm {kn}}\)

\(\newcommand {\mmHg }{\mathrm {mmHg}}\)

\(\newcommand {\nauticalmile }{\mathrm {M}}\)

\(\newcommand {\neper }{\mathrm {Np}}\)

\(\newcommand {\yocto }{\mathrm {y}}\)

\(\newcommand {\zepto }{\mathrm {z}}\)

\(\newcommand {\atto }{\mathrm {a}}\)

\(\newcommand {\femto }{\mathrm {f}}\)

\(\newcommand {\pico }{\mathrm {p}}\)

\(\newcommand {\nano }{\mathrm {n}}\)

\(\newcommand {\micro }{\mathrm {\unicode {x00B5}}}\)

\(\newcommand {\milli }{\mathrm {m}}\)

\(\newcommand {\centi }{\mathrm {c}}\)

\(\newcommand {\deci }{\mathrm {d}}\)

\(\newcommand {\deca }{\mathrm {da}}\)

\(\newcommand {\hecto }{\mathrm {h}}\)

\(\newcommand {\kilo }{\mathrm {k}}\)

\(\newcommand {\mega }{\mathrm {M}}\)

\(\newcommand {\giga }{\mathrm {G}}\)

\(\newcommand {\tera }{\mathrm {T}}\)

\(\newcommand {\peta }{\mathrm {P}}\)

\(\newcommand {\exa }{\mathrm {E}}\)

\(\newcommand {\zetta }{\mathrm {Z}}\)

\(\newcommand {\yotta }{\mathrm {Y}}\)

\(\newcommand {\percent }{\mathrm {\%}}\)

\(\newcommand {\meter }{\mathrm {m}}\)

\(\newcommand {\metre }{\mathrm {m}}\)

\(\newcommand {\gram }{\mathrm {g}}\)

\(\newcommand {\kg }{\kilo \gram }\)

\(\newcommand {\of }[1]{_{\mathrm {#1}}}\)

\(\newcommand {\squared }{^2}\)

\(\newcommand {\square }[1]{\mathrm {#1}^2}\)

\(\newcommand {\cubed }{^3}\)

\(\newcommand {\cubic }[1]{\mathrm {#1}^3}\)

\(\newcommand {\per }{/}\)

\(\newcommand {\celsius }{\unicode {x2103}}\)

\(\newcommand {\fg }{\femto \gram }\)

\(\newcommand {\pg }{\pico \gram }\)

\(\newcommand {\ng }{\nano \gram }\)

\(\newcommand {\ug }{\micro \gram }\)

\(\newcommand {\mg }{\milli \gram }\)

\(\newcommand {\g }{\gram }\)

\(\newcommand {\kg }{\kilo \gram }\)

\(\newcommand {\amu }{\mathrm {u}}\)

\(\newcommand {\nm }{\nano \metre }\)

\(\newcommand {\um }{\micro \metre }\)

\(\newcommand {\mm }{\milli \metre }\)

\(\newcommand {\cm }{\centi \metre }\)

\(\newcommand {\dm }{\deci \metre }\)

\(\newcommand {\m }{\metre }\)

\(\newcommand {\km }{\kilo \metre }\)

\(\newcommand {\as }{\atto \second }\)

\(\newcommand {\fs }{\femto \second }\)

\(\newcommand {\ps }{\pico \second }\)

\(\newcommand {\ns }{\nano \second }\)

\(\newcommand {\us }{\micro \second }\)

\(\newcommand {\ms }{\milli \second }\)

\(\newcommand {\s }{\second }\)

\(\newcommand {\fmol }{\femto \mol }\)

\(\newcommand {\pmol }{\pico \mol }\)

\(\newcommand {\nmol }{\nano \mol }\)

\(\newcommand {\umol }{\micro \mol }\)

\(\newcommand {\mmol }{\milli \mol }\)

\(\newcommand {\mol }{\mol }\)

\(\newcommand {\kmol }{\kilo \mol }\)

\(\newcommand {\pA }{\pico \ampere }\)

\(\newcommand {\nA }{\nano \ampere }\)

\(\newcommand {\uA }{\micro \ampere }\)

\(\newcommand {\mA }{\milli \ampere }\)

\(\newcommand {\A }{\ampere }\)

\(\newcommand {\kA }{\kilo \ampere }\)

\(\newcommand {\ul }{\micro \litre }\)

\(\newcommand {\ml }{\milli \litre }\)

\(\newcommand {\l }{\litre }\)

\(\newcommand {\hl }{\hecto \litre }\)

\(\newcommand {\uL }{\micro \liter }\)

\(\newcommand {\mL }{\milli \liter }\)

\(\newcommand {\L }{\liter }\)

\(\newcommand {\hL }{\hecto \liter }\)

\(\newcommand {\mHz }{\milli \hertz }\)

\(\newcommand {\Hz }{\hertz }\)

\(\newcommand {\kHz }{\kilo \hertz }\)

\(\newcommand {\MHz }{\mega \hertz }\)

\(\newcommand {\GHz }{\giga \hertz }\)

\(\newcommand {\THz }{\tera \hertz }\)

\(\newcommand {\mN }{\milli \newton }\)

\(\newcommand {\N }{\newton }\)

\(\newcommand {\kN }{\kilo \newton }\)

\(\newcommand {\MN }{\mega \newton }\)

\(\newcommand {\Pa }{\pascal }\)

\(\newcommand {\kPa }{\kilo \pascal }\)

\(\newcommand {\MPa }{\mega \pascal }\)

\(\newcommand {\GPa }{\giga \pascal }\)

\(\newcommand {\mohm }{\milli \ohm }\)

\(\newcommand {\kohm }{\kilo \ohm }\)

\(\newcommand {\Mohm }{\mega \ohm }\)

\(\newcommand {\pV }{\pico \volt }\)

\(\newcommand {\nV }{\nano \volt }\)

\(\newcommand {\uV }{\micro \volt }\)

\(\newcommand {\mV }{\milli \volt }\)

\(\newcommand {\V }{\volt }\)

\(\newcommand {\kV }{\kilo \volt }\)

\(\newcommand {\W }{\watt }\)

\(\newcommand {\uW }{\micro \watt }\)

\(\newcommand {\mW }{\milli \watt }\)

\(\newcommand {\kW }{\kilo \watt }\)

\(\newcommand {\MW }{\mega \watt }\)

\(\newcommand {\GW }{\giga \watt }\)

\(\newcommand {\J }{\joule }\)

\(\newcommand {\uJ }{\micro \joule }\)

\(\newcommand {\mJ }{\milli \joule }\)

\(\newcommand {\kJ }{\kilo \joule }\)

\(\newcommand {\eV }{\electronvolt }\)

\(\newcommand {\meV }{\milli \electronvolt }\)

\(\newcommand {\keV }{\kilo \electronvolt }\)

\(\newcommand {\MeV }{\mega \electronvolt }\)

\(\newcommand {\GeV }{\giga \electronvolt }\)

\(\newcommand {\TeV }{\tera \electronvolt }\)

\(\newcommand {\kWh }{\kilo \watt \hour }\)

\(\newcommand {\F }{\farad }\)

\(\newcommand {\fF }{\femto \farad }\)

\(\newcommand {\pF }{\pico \farad }\)

\(\newcommand {\K }{\mathrm {K}}\)

\(\newcommand {\dB }{\mathrm {dB}}\)

\(\newcommand {\kibi }{\mathrm {Ki}}\)

\(\newcommand {\mebi }{\mathrm {Mi}}\)

\(\newcommand {\gibi }{\mathrm {Gi}}\)

\(\newcommand {\tebi }{\mathrm {Ti}}\)

\(\newcommand {\pebi }{\mathrm {Pi}}\)

\(\newcommand {\exbi }{\mathrm {Ei}}\)

\(\newcommand {\zebi }{\mathrm {Zi}}\)

\(\newcommand {\yobi }{\mathrm {Yi}}\)

\(\require {mhchem}\)

\(\require {cancel}\)

\(\newcommand {\fint }{âĺŊ}\)

\(\newcommand {\hdots }{\cdots }\)

\(\newcommand {\mathnormal }[1]{#1}\)

\(\newcommand {\vecs }[2]{\vec {#1}_{#2}}\)

\(\renewcommand {\A }{\mathcal {A}}\)

\(\newcommand {\B }{\mathcal {B}}\)

\(\renewcommand {\C }{\mathcal {C}}\)

\(\newcommand {\EE }{\mathbb {E}}\)

\(\renewcommand {\N }{\mathcal {N}}\)

\(\renewcommand {\P }{\mathcal {P}}\)

\(\newcommand {\PP }{\mathbb {P}}\)

\(\newcommand {\T }{\mathcal {T}}\)

\(\renewcommand {\U }{\mathcal {U}}\)

\(\newcommand {\X }{\mathcal {X}}\)

\(\newcommand {\Y }{\mathcal {Y}}\)

\(\newcommand {\Bin }{\operatorname {Bin}}\)

\(\newcommand {\Pois }{\operatorname {Pois}}\)

\(\newcommand {\Exp }{\operatorname {Exp}}\)

\(\newcommand {\BetaV }{\operatorname {Beta}}\)

\(\newcommand {\GammaV }{\operatorname {Gamma}}\)

\(\newcommand {\pot }{\mathfrak {P}}\)

\(\renewcommand {\1}{𝟙}\)

\(\newcommand {\id }{\mathrm {id}}\)

\(\renewcommand {\i }{\mathrm {i}}\)

\(\renewcommand {\theta }{\vartheta }\)

\(\newcommand {\Tu }{\underline {T}}\)

\(\newcommand {\To }{\overline {T}}\)

\(\newcommand {\Var }{\mathrm {Var}}\)

\(\newcommand {\Cov }{\mathrm {Cov}}\)

\(\newcommand {\name }[1]{\textsc {#1}}\)

\(\newcommand {\smallpmatrix }[1]{\left (\begin {smallmatrix}#1\end {smallmatrix}\right )}\)

\(\newcommand {\matlab }{{\fontfamily {bch}\scshape \selectfont {}Matlab}}\)

\(\newcommand {\innerproduct }[1]{\left \langle {#1}\right \rangle }\)

\(\newcommand {\norm }[1]{\left \Vert {#1}\right \Vert }\)

\(\renewcommand {\natural }{\mathbb {N}}\)

\(\newcommand {\integer }{\mathbb {Z}}\)

\(\newcommand {\rational }{\mathbb {Q}}\)

\(\newcommand {\real }{\mathbb {R}}\)

\(\newcommand {\complex }{\mathbb {C}}\)

\(\renewcommand {\d }{\mathop {}\!\mathrm {d}}\)

\(\newcommand {\dr }{\d {}r}\)

\(\newcommand {\ds }{\d {}s}\)

\(\newcommand {\dt }{\d {}t}\)

\(\newcommand {\du }{\d {}u}\)

\(\newcommand {\dv }{\d {}v}\)

\(\newcommand {\dw }{\d {}w}\)

\(\newcommand {\dx }{\d {}x}\)

\(\newcommand {\dy }{\d {}y}\)

\(\newcommand {\dz }{\d {}z}\)

\(\newcommand {\dsigma }{\d {}\sigma }\)

\(\newcommand {\dphi }{\d {}\phi }\)

\(\newcommand {\dvarphi }{\d {}\varphi }\)

\(\newcommand {\dtau }{\d {}\tau }\)

\(\newcommand {\dxi }{\d {}\xi }\)

\(\newcommand {\dtheta }{\d {}\theta }\)

\(\newcommand {\tp }{\mathrm {T}}\)

</div>

<style type="text/css">
.lwarp-contents li.list-item-f0::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f1::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f2::marker {
  content:'•\00a0\00a0';
}
.lwarp-contents li.list-item-f3::marker {
  font-style:italic;
  content:'(1)\00a0\00a0';
}
.lwarp-contents li.list-item-f4::marker {
  font-style:italic;
  content:'(2)\00a0\00a0';
}
.lwarp-contents li.list-item-f5::marker {
  font-style:italic;
  content:'(1)\00a0\00a0';
}
.lwarp-contents li.list-item-f6::marker {
  font-style:italic;
  content:'(2)\00a0\00a0';
}
.lwarp-contents li.list-item-f7::marker {
  font-style:italic;
  content:'(1)\00a0\00a0';
}
.lwarp-contents li.list-item-f8::marker {
  font-style:italic;
  content:'(2)\00a0\00a0';
}
</style>
<p>

</p>


<p>
<em>Bemerkung</em>: In der Wahrscheinlichkeitstheorie ist meist ein W-Raum \((\Omega , \A , P)\) gegeben und man soll \(P(A)\) fu&#x0308;r \(A \in \A \) berechnen oder approximieren. Dagegen geht die mathematische
Statistik gewissermaßen umgekehrt vor: Dort sind eine Familie \(\P \) von W-Maßen auf dem Messraum \((\Omega , \A )\) und eine Folge \(X_1, \dotsc , X_n\) von reellwertigen Zufallsvariablen mit Werten \(x_1, \dotsc ,
x_n\) gegeben. Welches \(\PP \in \P \) oder welche Teilmenge \(\P _0 \subset \P \) eigenet sich „am Besten“ zur Erkla&#x0308;rung der Realisierung/des Datensatzes \(x_1, \dotsc , x_n\)?
</p>

<p>
Die Wahrscheinlichkeitstheorie liefert eine axiomatiche Begru&#x0308;ndung des Pha&#x0308;nomens „Zufall“ und konstruiert und beschreibt Modelle fu&#x0308;r zufa&#x0308;llige Prozesse. Die Statistik behandelt die zur WT
„inverse“ Fragestellung: Die <em><span class="dashuline" >mathematische Statistik</span></em> (auch <em><span class="dashuline" >Inferenzstatistik</span></em> oder <em><span class="dashuline" >induktive
Statistik</span></em>) sucht zu gegebenen Daten das „beste“ Modell oder die „besten“ Modelle aus einer vorgegebenen Familie von Modellen aus. Davon zu unterscheiden ist die <em><span class="dashuline" >deskriptive
Statistik</span></em>, die man landla&#x0308;ufig unter dem Begriff „Statistik“ versteht. Bei dieser Art von Statistik werden die vorliegenden Daten ohne Verwendung eines wahrscheinlichkeitstheoretischen Modells beschrieben
(z.&#x202f;B. Fußball-Statistik, amtliche Statistiken). Die Weihnachtsgeschichte zeigt, dass diese Statistik schon sehr lange betrieben wird – dennoch ist sie immer noch aktuell (bspw. Chartanalyse bei Aktienkursen).
</p>



<h2 id="grundbegriffe">Grundbegriffe</h2>

</p>


<p>
<em>Bemerkung</em>: Eine konkrete Beobachtung fasst der Statistiker auf als ein Element \(x \in \X \) (z.&#x202f;B. \(x = (x_1, \dotsc , x_n) \in \real ^n = \X \)) und interpretiert \(x\) als eine <em><span
class="dashuline" >Realisierung</span></em> \(x = X(\omega )\) einer Zufallsvariablen \(X\colon (\Omega , \A ) \rightarrow (\X , \B )\). \((\Omega , \A )\) heißt <em><span class="dashuline"
>Grundraum</span></em> und \((\X , \B )\) <em><span class="dashuline" >Stichprobenraum</span></em> der <em><span class="dashuline" >Stichprobe</span></em> \(X\). Liegt auf \((\Omega , \A )\) ein W-Maß
\(\PP \) vor, so induziert dies auf \((\X , \B )\) ein W-Maß \(\PP _X\) durch \(\PP _X(B) := \PP (X \in B)\) = \(\PP (X^{-1}(B)) = \PP (\{\omega \in \Omega \;|\; X(\omega ) \in B\})\) fu&#x0308;r \(B \in
\B \), das <em><span class="dashuline" >Verteilung</span></em> von \(X\) genannt wird.
</p>

<p>
Typischerweise ist \(X = (X_1, \dotsc , X_n)\) ein Zufallsvektor mit stochastisch unabha&#x0308;ngigen Komponenten \(X_1, \dotsc , X_n\). Falls die \(X_i\) alle reellwertig sind, gilt \((\X , \B ) = (\real ^n, \B
^n)\) mit \(\B ^n\) der <em><span class="dashuline" ><span class="textsc" >borel</span>schen \(\sigma \)-Algebra</span></em> des \(\real ^n\) (kleinste \(\sigma \)-Algebra, die alle offenen Mengen des \(\real
^n\) entha&#x0308;lt) und \(\PP _X = \bigotimes _{i=1}^n \PP _{X_i}\) dem Produktmaß der \(\PP _{X_i}\) auf \(\B ^n\). Da die Verteilung \(\PP _X\) dem Statistiker nicht (vollsta&#x0308;ndig) bekannt ist, wird
fu&#x0308;r \(\PP _X\) ein <em><span class="dashuline" >statistisches Modell</span></em> bestimmt, das heißt \(\P = \{P_\theta \;|\; \theta \in \Theta \}\) mit \(P_\theta \) Verteilung auf \((\X , \B )\). Kann
\(\P \) mit einer Parametermenge \(\Theta \subset \real ^d\) parametrisiert werden, so spricht man von einem <em><span class="dashuline" >parametrischem Modell</span></em>, andernfalls von einem <em><span
class="dashuline" >nicht-parametrischem Modell</span></em>.
</p>

<p>
Das Ziel ist, basierend auf einer Stichprobe \(X\) ein \(P_\theta \in \P \) zu finden, das der tatsa&#x0308;chlichen Verteilung von \(X\) „mo&#x0308;glichst a&#x0308;hnlich“ ist. Die Verteilung von \(X\) muss nicht
notwendigerweise in \(\P \) enthalten sein.
</p>

<p>
<em>Beispiel</em>: \(\P = \{\N (\mu , \sigma ^2) \;|\; (\mu , \sigma ^2) \in \real \times \real ^+\}\) ist ein parametrisches Modell fu&#x0308;r eine reellwertige Messgro&#x0308;ße (z.&#x202f;B.
Ko&#x0308;rpergro&#x0308;ße der Studenten im Ho&#x0308;rsaal).<br />
Dagegen ist \(\P = \{P \;|\; P \text { ist Verteilung auf } (\real , \B ^1)\text {, welche eine Lebesgue-Borel-Dichte besitzt}\}\) ein nicht-parametrisches Modell. Ein W-Maß \(P\) besitzt eine <em><span
class="dashuline" >L.-B.-Dichte</span></em>, falls es eine L.-B.-messbare Funktion \(f\colon \real \rightarrow \real _0^+\) so gibt, dass \(P(B) = \int _B f d\lambda \) fu&#x0308;r alle \(B \in \B ^1\). Dabei
bezeichnet \(\lambda \) das L.-B.-Maß auf \(\real \).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<b>statistischer Raum</b>:&#x2003; Sei \(\P \) eine Menge von W-Maßen auf dem Messraum \((\X , \B )\).<br />
Dann heißt \((\X , \B , \P )\) <em><span class="dashuline" >statistischer Raum</span></em>.
</p>

<p>
<em>Bemerkung</em>: Vereinfacht gesagt ist ein statistischer Raum ein W-Raum mit vielen W-Maßen.<br />
Ha&#x0308;ufig ist die genaue Gestalt der Stichprobe \(X\) nicht von Interesse, daher wird \(X\) „begrenzt“ (wenn man z.&#x202f;B. die Geschlechterverteilung der Studierenden untersuchen will, dann interessiert nicht das
Geschlecht jedes einzelnen Studenten, sondern nur die Anzahl der Frauen und Ma&#x0308;nner).
</p>

<p>
<b>Statistik</b>:&#x2003; Sei \(T\colon (\X , \B ) \rightarrow (\Y , \C )\) eine messbare Abbildung.<br />
Dann heißt \(T(X)\) eine <em><span class="dashuline" >Statistik</span></em> (oder <em><span class="dashuline" >Stichprobenfunktion</span></em>) der Stichprobe \(X\).
</p>

<p>
<em>Bemerkung</em>: \(\Y \) wird i.&#x202f;A. „kleiner“ gewa&#x0308;hlt als \(\X \).
</p>

<p>
<em>Beispiel</em>: Eine klinische Studie untersucht bei \(n = 100\) Patienten die Wirkung eines neuen Medikaments. Dafu&#x0308;r definiert man \(n\) Zufallsvariablen \(X_i\) mit \(X_i := 0\) bzw. \(X_i := 1\), falls das
Medikament auf Patient \(i\) keine bzw. eine Wirkung zeigt. Man nimmt an, dass \(X_1, \dotsc , X_n\) unabha&#x0308;ngig und identisch verteilt (i.i.d.) sind mit \(X_i \sim \Bin (1, \theta )\), dabei sei \(\theta \in
[0, 1]\) unbekannt. Die Zufallsvariable \(X = (X_1, \dotsc , X_n) \sim \bigotimes _{i=1}^n \Bin (1, \theta )\) hat Werte in \(\X = \prod _{i=1}^n \{0, 1\} = \{0, 1\}^n\), der Raum ist diskret, d.&#x202f;h.
\(\B = \pot (\X )\). Damit ko&#x0308;nnen wir nun ein statistisches Modell aufstellen durch \(\P = \left \{\bigotimes _{i=1}^n \Bin (1, \theta ) \;|\; \theta \in [0, 1]\right \}\). Eine typische Statistik
fu&#x0308;r \(X\) ist z.&#x202f;B. die Anzahl \(T(X) = \sum _{i=1}^n X_i\) der Patienten, auf die das Medikament eine Wirkung zeigt, oder der relative Anteil \(T(X) = \frac {1}{n} \sum _{i=1}^n X_i\) dieser Patienten.
</p>

<p>
<em>Bemerkung</em>: Typische Fragen in der Statistik sind beispielsweise:
</p>
<ul style="list-style-type:none">

<li class="list-item-f0"><p><em><span class="dashuline" >Scha&#x0308;tzproblem</span></em>: Finde zu gegebener Stichprobe \(X\colon \Omega \rightarrow \X \) einen Scha&#x0308;tzwert fu&#x0308;r den wahren, aber unbekannten
Parameter \(\theta \).
</p>
</li>
<li class="list-item-f1"><p><em><span class="dashuline" >Bereichsscha&#x0308;tzung</span></em>: Scha&#x0308;tze basierend auf der Stichprobe \(X\) ein Intervall \(I\), sodass z.&#x202f;B. \(\PP _\theta (\theta \in I) \ge 0{,}95\)
(<em><span class="dashuline" >\(95\,\%\)-Konfidenzintervall</span></em>). \(I\) soll so klein wie mo&#x0308;glich sein.
</p>
</li>
<li class="list-item-f2"><p><em><span class="dashuline" >Testproblem</span></em>: Entscheide basierend auf der Stichprobe \(X\), ob z.&#x202f;B. \(\theta &gt; 0{,}5\) (mit hoher Sicherheit) angenommen werden kann.
</p>
</li>
</ul>

<p>
<em>Bemerkung</em>: Man verwendet bei den verschiedenen statistischen Ra&#x0308;umen folgende Notation: Die W-Maße des in der Regel uninteressanten Raums \((\Omega , \A , (\PP _\theta )_{\theta \in \Theta
})\) werden mit Doppelstrich-Buchstaben versehen. Dieser Raum wird durch die Zufallsvariable \(X\) abgebildet auf den statistischen Raum \((\X , \B , (P_\theta )_{\theta \in \Theta })\), der normalerweise gegeben ist.
Die W-Maße \(P_\theta \) entsprechen den Bildmaßen \((\PP _\theta )_X\) von \(\PP _\theta \) unter \(X\). Mittels einer Statistik \(T\) wird dieser Raum wiederum abgebildet auf \((\Y , \C , ((P_\theta )_T)_{\theta
\in \Theta })\).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Um unno&#x0308;tige maßtheoretische Argumentationen zu vermeiden, wird in Zukunft meistens davon ausgegangen, dass das statistische Modell \(\P \) regula&#x0308;r ist.
</p>

<p>
<b>regula&#x0308;r</b>:&#x2003;<br />
Ein statistisches Modell \(\P \) heißt <em><span class="dashuline" >regula&#x0308;r</span></em>, falls eine der beiden folgenden Bedingungen erfu&#x0308;llt ist:
</p>
<ul style="list-style-type:none">

<li class="list-item-f3"><p>Alle \(P \in \P \) besitzen     eine Dichte \(p\colon \X \rightarrow \real _0^+\) (bzgl. dem L.-B.-Maß),<br />
d.&#x202f;h. \(\forall _{B \in \B }\;        \PP (X \in B) = P(B) = \int _B p(x)\dx \).
</p>
</li>
<li class="list-item-f4"><p>Alle \(P \in \P \) besitzen     eine Za&#x0308;hldichte \(p\colon \X \rightarrow \real _0^+\) (bzgl. dem Za&#x0308;hlmaß),<br />
d.&#x202f;h. \(\forall _{B \in \B }\;        \PP (X \in B) = P(B) = \sum _{x \in B} p(x)\).
</p>
</li>
</ul>

<p>
Fu&#x0308;r ein regula&#x0308;res Modell schreibt man oft \(\P = \{P_\theta \;|\; \theta \in \Theta \}\), wobei \(p(\cdot , \theta )\) die L.-B.-Dichte bzw. die Za&#x0308;hldichte von \(P_\theta \) bezeichnet.
</p>



<h2 id="suffizienz">Suffizienz</h2>

</p>


<p>
<em>Bemerkung</em>: Eine Statistik \(T\) soll zwar die Stichprobe \(X\) „komprimieren“, jedoch nicht zu stark, d.&#x202f;h. es darf keine Information verloren gehen. Kennt man also \(T(X) = t\), dann darf \(X\) keine weiteren
Informationen u&#x0308;ber \(\theta \) enthalten.
</p>

<p>
<b>suffizient</b>:&#x2003; Seien \((\X , \B , \P )\) ein statistischer Raum und \(X\) eine Stichprobe aus \(\X \).<br />
Dann heißt die Statistik \(T(X)\) von \(X\) <em><span class="dashuline" >suffizient fu&#x0308;r \(P \in \P \)</span></em>, falls die bedingte Verteilung von \(X\) gegeben \(T(X) = t\) unabha&#x0308;ngig von \(P\) ist
(bzw. unabha&#x0308;ngig von \(\theta \) fu&#x0308;r \(\P \) parametrisierbar).
</p>

<p>
<b>bedingte Verteilung</b>:&#x2003; Sind \(X\) und \(Y\) zwei diskrete Zufallsvariablen mit gemeinsamer Za&#x0308;hldichte \(p(x, y)\), so ist die <em><span class="dashuline" >bedingte Verteilung von \(X\) gegeben
\(Y\)</span></em> (von \(X|Y\)) definiert durch die Za&#x0308;hldichte \(p(x|y) = \frac {p(x, y)}{p_Y(y)} = \PP (X = x|Y = y)\), wobei \(p_Y\) mit \(p_Y(y) = \PP (Y = y) = \sum _{x’} p(x’, y)\) die
Randverteilung von \(Y\) bezeichnet.<br />
Sind \(X\) und \(Y\) zwei stetige Zufallsvariablen mit gemeinsamer L.-B.-Dichte \(p(x, y)\), so ist die <em><span class="dashuline" >bedingte Verteilung von \(X\) gegeben \(Y\)</span></em> (von \(X|Y\)) definiert durch die
Dichte \(p(x|y) = \frac {p(x, y)}{p_Y(y)}\) mit \(p_Y(y) = \int p(x’, y)\dx ’\).
</p>

<p>
<em>Beispiel</em>: Man konstruiert eine suffiziente Statistik fu&#x0308;r die Binomialverteilung.<br />
Dazu seien \(X_1, \dotsc , X_n \sim \Bin (1, p)\) i.i.d., \(X := (X_1, \dotsc , X_n)\) der Zufallsvektor und<br />
\(Y := \sum _{i=1}^n X_i \sim \Bin (n, p)\). Um zu pru&#x0308;fen, ob \(T(X) := Y\) eine suffiziente Statistik fu&#x0308;r<br />
\(\P = \{\Bin (n, p) \;|\; p \in [0, 1]\}\) ist, muss man die bedingte Verteilung von \(X|Y\) berechnen.<br />
Fu&#x0308;r \(x \in \{0, 1\}^n\) und \(y \in \{0, \dotsc , n\}\) gilt \(p(x|y) = \frac {\PP (X = x, Y = y)}{\PP (Y = y)} = \frac {p^y (1-p)^{n-y}}{\binom {n}{y} p^y (1-p)^{n-y}} = \frac {1}{\binom
{n}{y}}\) unabha&#x0308;ngig von \(p\), denn es gilt \(\PP (X = x) = \PP (X_1 = x_1) \dotsm \PP (X_n = x_n) = p^{x_1} (1-p)^{1-x_1} \dotsm p^{x_n} (1-p)^{1-x_n} = p^{x_1+\dotsb +x_n}
(1-p)^{1-(x_1+\dotsb +x_n)}\), weil die \(X_i\) unabha&#x0308;ngig sind.<br />
Also ist die bedingte Verteilung von \(X|Y = y\) eine Gleichvert. auf \(\{x \in \{0, 1\}^n \;|\; \sum _{i=1}^n x_i = y\}\) (diese Menge besitzt ja \(\binom {n}{y}\) viele Elemente). Damit ist \(T(X) := \sum _{i=1}^n
X_i\) eine suffiziente Statistik fu&#x0308;r \(\P = \{\Bin (n, p) \;|\; p \in [0, 1]\}\). Dies gilt auch fu&#x0308;r das arithmetische Mittel \(T(X) := \frac {1}{n} \sum _{i=1}^n X_i\).
</p>

<p>
<em>Beispiel</em>: Man konstruiert eine suffiziente Statistik fu&#x0308;r die Normalverteilung.<br />
Dazu seien \(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\) i.i.d., wobei \((\mu , \sigma ^2) \in \real \times \real ^+\) nicht bekannt ist.<br />
Das arithm. Mittel \(\overline {X} := \frac {1}{n} \sum _{i=1}^n X_i\) und die Stichprobenvarianz \(S^2(X) := \frac {1}{n-1} \sum _{i=1}^n (X_i - \overline {X})^2\) sind bei gegebener Stichprobe \(X :=
(X_1, \dotsc , X_n)\) brauchbare Scha&#x0308;tzer fu&#x0308;r \(\mu \) und \(\sigma ^2\).<br />
Ist \(T(X) := (\overline {X}, S^2(X))\) eine suffiziente Statistik fu&#x0308;r \(\P = \{\N (\mu , \sigma ^2) \;|\; (\mu , \sigma ^2) \in \real \times \real ^+\}\)?
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Die Definition der Suffizienz einer Statistik gibt leider keine Mo&#x0308;glichkeit, wie eine suffiziente Statistik konstruiert werden kann.
</p>

<p>
<span class="uline" ><em>Satz</em> (<span class="textsl" >Faktorisierungssatz</span>):</span> Sei \(\P = \{P_\theta \;|\; \theta \in \Theta \}\) ein regula&#x0308;res Modell. Dann sind a&#x0308;quivalent:
</p>
<ul style="list-style-type:none">

<li class="list-item-f5"><p>\(T(X)\) ist suffizient fu&#x0308;r \(\theta \).
</p>
</li>
<li class="list-item-f6"><p>Es existieren Abbildungen \(g\colon \Y \times \Theta \rightarrow \real \) und \(h\colon \real ^n \rightarrow \real \), sodass fu&#x0308;r alle \(x \in \real ^n\) und \(\theta \in \Theta \) gilt,
dass \(p(x, \theta ) = g(T(x), \theta ) \cdot h(x)\).
</p>
</li>
</ul>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Wenn man das Beispiel von eben mit der Normalverteilung fortsetzt und die Zufallsvariable \(T_1(X) := (\sum _{i=1}^n X_i, \sum _{i=1}^n X_i^2)\) betrachet, dann gilt mit \(h(x) := 1\) und<br />
\(g(T_1(x), \theta ) := \frac {1}{(2\pi \sigma )^{n/2}} e^{-\frac {n\mu ^2}{2\sigma ^2}} \exp \!\left (-\frac {1}{2\sigma ^2} \left (\sum _{i=1}^n x_i^2 - 2\mu \sum _{i=1}^n x_i\right )\right
)\)<br />
\(= \prod _{i=1}^n \frac {1}{\sqrt {2\pi \sigma ^2}} \exp \!\left (-\frac {(x_i - \mu )^2}{2\sigma ^2}\right )\), dass \(p(x, \theta ) = g(T_1(x), \theta ) \cdot h(x)\).<br />
\(T_1(X)\) ist also nach dem Faktorisierungssatz eine suffiziente Statistik fu&#x0308;r \(\theta = (\mu , \sigma ^2)\).<br />
Wegen \(\overline {X} = \frac {1}{n} \sum _{i=1}^n X_i\) und \(S^2(X) = \frac {1}{n-1} \sum _{i=1}^n (X_i^2 - (\overline {X})^2)\) ist auch \(T(X) := (\overline {X}, S^2(X))\) eine suffiziente Statistik
fu&#x0308;r \(\theta \).
</p>



<h2 id="exponentialfamilien">Exponentialfamilien</h2>

</p>


<p>
<em>Bemerkung</em>: Die Exponentialfamilien (auch exponentielle Familien) bilden wichtige Klassen von Verteilungen mit einem Parameter oder mehreren Parametern. Im Folgenden seien Mengen und Funktionen immer als
messbar vorausgesetzt, falls dies beno&#x0308;tigt wird.
</p>

<p>
<b>\(1\)-parametrige Exponentialfamilie</b>:&#x2003; Eine Familie von Verteilungen \(\P = \{P_\theta \;|\; \theta \in \Theta \}\) auf \((\real ^n, \B ^n)\) mit \(\Theta \subset \real \) heißt <em><span
class="dashuline" >\(1\)-parametrige Exponentialfamilie</span></em>, falls es Funktionen<br />
\(c, d\colon \Theta \rightarrow \real \) und \(T, S\colon \real ^n \rightarrow \real \) sowie eine Menge \(A \subset \real ^n\) gibt,<br />
sodass die L.-B.-Dichte/Za&#x0308;hldichte \(p(x, \theta )\) von \(P_\theta \) fu&#x0308;r \(x \in \real ^n\) durch<br />
\(p(x, \theta ) = \1_A(x) \cdot \exp (c(\theta ) T(x) + d(\theta ) + S(x))\) dargestellt werden kann.
</p>

<p>
<em>Bemerkung</em>: \(A\) ist unabha&#x0308;ngig von \(\theta \). \(d(\theta )\) dient zur Normierung (damit \(\int _{\real ^n} p(x, \theta )\dx = 1\)).<br />
Nach dem Faktorisierungssatz ist \(T(x)\) immer eine suffiziente Statistik fu&#x0308;r \(\theta \in \Theta \), denn mit \(g(t, \theta ) := \exp (c(\theta ) t + d(\theta ))\) und \(h(x) := \1_A(x) \cdot \exp
(S(x))\) gilt \(p(x, \theta ) = g(T(x), \theta ) \cdot h(x)\). Die Statistik \(T\) heißt daher <em><span class="dashuline" >natu&#x0308;rliche suffiziente Statistik</span></em>.<br />
Fu&#x0308;r den Fall \(c = \id _\Theta \) spricht man von einer <em><span class="dashuline" >natu&#x0308;rlichen Exponentialfamilie</span></em>. Jede Exponentialfamilie hat eine Darstellung als natu&#x0308;rliche
Exponentialfamilie, was man mit der Umparametrisierung \(\eta = c(\theta )\) erreichen kann, in diesem Fall gilt \(p_0(x, \eta ) = \1_A(x) \cdot \exp (\eta \cdot T(x) + d_0(\eta ) + S(x))\), wobei \(d_0(\eta
)\) die neue Normierungskonstante darstellt.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Bei bekannter Varianz \(\sigma ^2 &gt; 0\) ist \(\P = \{\N (\mu , \sigma ^2) \;|\; \mu \in \real \}\) eine \(1\)-parametrige Exponentialfamilie, denn es gilt fu&#x0308;r die L.-B.-Dichte \(p(x, \mu
) = \frac {1}{\sqrt {2\pi \sigma ^2}} \exp \!\left (-\frac {(x - \mu )^2}{2\sigma ^2}\right )\)<br />
\(= \1_\real (x) \cdot \exp \!\left (\frac {\mu }{\sigma ^2} x + \frac {-\mu ^2}{2\sigma ^2} + \left (-\frac {x^2}{2\sigma ^2} - \ln \sqrt {2\pi \sigma ^2}\right )\right )\).<br />
Man wa&#x0308;hlt also \(A := \real \), \(c(\mu ) = \frac {\mu }{\sigma ^2}\), \(T(x) := x\), \(d(\mu ) := \frac {-\mu ^2}{2\sigma ^2}\) und \(S(x) := -\frac {x^2}{2\sigma ^2} - \ln \sqrt {2\pi
\sigma ^2}\).
</p>

<p>
<em>Beispiel</em>: Die Familie \(\P = \{\Bin (n, \theta ) \;|\; \theta \in (0, 1)\}\) der Binomialverteilungen bei bekanntem \(n\) ist eine \(1\)-parametrige Exponentialfamilie, da \(p(k, \theta ) = \binom {n}{k}
\theta ^k (1-\theta )^{n-k}\)<br />
\(= \1_{\{0,\dotsc ,n\}}(k) \cdot \exp \!\left (\ln \!\left (\frac {\theta }{1 - \theta }\right ) \cdot k + n \cdot \ln (1 - \theta ) + \ln \binom {n}{k}\right )\). Man wa&#x0308;hlt also \(A :=
\{0, \dotsc , n\}\),<br />
\(c(\theta ) := \ln \!\left (\frac {\theta }{1 - \theta }\right )\), \(T(k) := k\), \(d(\theta ) := n \cdot \ln (1 - \theta )\) und \(S(k) = \ln \binom {n}{k}\).
</p>

<p>
<em>Beispiel</em>: Die Gleichverteilung auf \((0, \theta )\) bildet keine \(1\)-parametrige Exponentialfamilie.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Sind \(X_1, \dotsc , X_m\) i.i.d. \(n\)-dimensionale Zufallsvektoren mit Verteilungen aus einer Exponentialfamilie \(\P = \{P_\theta \;|\; \theta \in \Theta \}\), so besitzt auch der Zufallsvektor
\(X := (X_1^T, \dotsc , X_m^T)^T\) mit Werten \(x = (x_1^T, \dotsc , x_m^T)^T \in \real ^{n \cdot m}\) eine Verteilung aus einer Exponentialfamilie, denn die Dichte von \(X\) ist aufgrund der
Unabha&#x0308;ngigkeit<br />
\(p_X(x, \theta ) = \prod _{i=1}^m p(x_i, \theta ) = \prod _{i=1}^m \1_A(x_i) \cdot \exp \!\left (c(\theta ) T(x_i) + d(\theta ) + S(x_i)\right )\)<br />
\(= \left (\prod _{i=1}^m \1_A(x_i)\right ) \cdot \exp \!\left (c(\theta ) \sum _{i=1}^m T(x_i) + m d(\theta ) + \sum _{i=1}^m S(x_i)\right )\).<br />
Wa&#x0308;hlt man \(A’ := A^m\) (dann gilt \(\prod _{i=1}^m \1_A(x_i) = \1_{A’}(x)\)), \(c’(\theta ) := c(\theta )\), \(T’(x) := \sum _{i=1}^m T(x_i)\), \(d’(\theta ) := m d(\theta )\) und \(S’(x) :=
\sum _{i=1}^m S(x_i)\), so erha&#x0308;lt man eine Darstellung als \(1\)-parametrige Exponentialfamilie.
</p>

<p>
<em>Beispiel</em>: Sind \(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\) i.i.d. und \(X := (X_1, \dotsc , X_n)^T\), dann sind \(T(X) := \sum _{i=1}^n X_i\) und \(\overline {X} := \frac {1}{n} \sum _{i=1}^n
X_i\) suffiziente Statistiken fu&#x0308;r den unbekannten Erwartungswert \(\mu \) und die Verteilung von \(X\) bildet wieder eine \(1\)-parametrige Exponentialfamilie.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<b>\(k\)-parametrige Exponentialfamilie</b>:&#x2003; Eine Familie von Verteilungen \(\P = \{P_\theta \;|\; \theta \in \Theta \}\) auf \((\real ^n, \B ^n)\) mit \(\Theta \subset \real ^k\) heißt <em><span
class="dashuline" >\(k\)-parametrige Exponentialfamilie</span></em>, falls es Funktionen<br />
\(c_j, d\colon \Theta \rightarrow \real \) und \(T_j, S\colon \real ^n \rightarrow \real \), \(j = 1, \dotsc , k\), sowie eine Menge \(A \subset \real ^n\) gibt,<br />
sodass die L.-B.-Dichte/Za&#x0308;hldichte \(p(x, \theta )\) von \(P_\theta \) fu&#x0308;r \(x \in \real ^n\) durch<br />
\(p(x, \theta ) = \1_A(x) \cdot \exp \!\left (\sum _{j=1}^k c_j(\theta ) T_j(x) + d(\theta ) + S(x)\right )\) dargestellt werden kann.
</p>

<p>
<em>Bemerkung</em>: Analog zur \(1\)-parametrigen Exponentialfamilie ist \(T(X) := (T_1(X), \dotsc , T_k(X))^T\) eine suffiziente Statistik fu&#x0308;r \(\theta \in \Theta \), die <em><span class="dashuline"
>natu&#x0308;rliche suffiziente Statistik</span></em>.
</p>

<p>
<em>Beispiel</em>: Die Familie \(\P = \{\N (\mu , \sigma ^2) \;|\; (\mu , \sigma ^2) \in \real \times \real ^+\}\) bildet eine \(2\)-parametrige Exponentialfamilie mit \(\theta = (\mu , \sigma ^2) \in \Theta
= \real \times \real ^+\), denn es gilt fu&#x0308;r die Dichte<br />
\(p(x, \theta ) = \frac {1}{\sqrt {2\pi \sigma ^2}} \exp \!\left (-\frac {(x - \mu )^2}{2\sigma ^2}\right ) = \1_\real (x) \cdot \exp \!\left (\frac {\mu }{\sigma ^2}x - \frac {1}{2\sigma ^2}
x^2 - \frac {1}{2} \left (\frac {\mu ^2}{\sigma ^2} + \ln (2\pi \sigma ^2)\right )\right )\).<br />
Wa&#x0308;hlt man \(A := \real \), \(c_1(\theta ) := \frac {\mu }{\sigma ^2}\), \(T_1(x) := x\), \(c_2(\theta ) := -\frac {1}{2\sigma ^2}\), \(T_2(x) := x^2\),<br />
\(d(\theta ) := -\frac {1}{2} \left (\frac {\mu ^2}{\sigma ^2} + \ln (2\pi \sigma ^2)\right )\) und \(S(x) := 0\), so erha&#x0308;lt man eine Darstellung als \(2\)-parametrige Exponentialfamilie.
</p>



<h2 id="bayesianische-modelle"><span style="font-variant: small-caps;">Bayes</span>ianische Modelle</h2>

</p>


<p>
<em>Bemerkung</em>: Der bayesianische Ansatz in der Statistik geht davon aus, dass der Wert einer unbekannten Verteilung eine Realisierung einer Zufallsvariablen mit gegebener a-priori-Verteilung ist. Diese a-priori-Verteilung kann
zur Modellierung einer subjektiven Einscha&#x0308;tzung (z.&#x202f;B. Expertenwissen) oder einer Vorabinformation dienen.
</p>

<p>
<b><span class="textsc" >bayes</span>ianisches Modell</b>:&#x2003;<br />
Ein <em><span class="dashuline" ><span class="textsc" >bayes</span>ianisches Modell</span></em> fu&#x0308;r die Daten \(X\) mit dem Parameter \(\theta \) ist bestimmt durch
</p>
<ul style="list-style-type:none">

<li class="list-item-f7"><p>eine <em><span class="dashuline" >a-priori-Verteilung</span></em> \(\pi \), sodass \(\theta \sim \pi \), und
</p>
</li>
<li class="list-item-f8"><p>eine regula&#x0308;re Verteilung \(\PP _\theta \), sodass \(X|\theta \sim \PP _\theta \).
</p>
</li>
</ul>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Bemerkung</em>: Nach Erhebung der Daten kann die a-priori-Verteilung \(\pi (\theta )\) von \(\theta \) zur <em><span class="dashuline" >a-posteriori-Verteilung</span></em> \(p(\theta |x) := p(\theta |X=x)\)
mittels <em><span class="dashuline" ><span class="textsc" >Bayes</span>-Formel</span></em> aktualisiert werden:<br />
\(p(\theta |x) = \frac {\pi (\theta ) p(x|\theta )}{m(x)}\), wobei \(m(x) := \sum _{\theta ’ \in \Theta } \pi (\theta ’) p(x|\theta ’)\), falls \(\theta \) die Za&#x0308;hldichte \(\pi (\theta )\)
besitzt, und \(m(x) := \int _\Theta \pi (\theta ’) p(x|\theta ’) \d \theta ’\), falls \(\theta \) die L.-B.-Dichte \(\pi (\theta )\) besitzt. \(m(x)\) heißt <em><span class="dashuline" >marginale Verteilung
(Randverteilung)</span></em> von \(X\).<br />
Ist der Za&#x0308;hler in \(\frac {\pi (\theta ) p(x|\theta )}{m(x)}\) bekannt, dann auch der Nenner, da \(p(x|\theta )\) u&#x0308;ber \(\theta \) summiert bzw. integriert gleich \(1\) sein muss. Deshalb schreibt
man obige Formel ha&#x0308;ufig kurz durch<br />
\(p(\theta |x) \propto \pi (\theta ) p(x|\theta )\).
</p>

<p>
<em>Bemerkung</em>: Die Bayes-Formel fu&#x0308;r Ereignisse sieht a&#x0308;hnlich aus: Fu&#x0308;r \(A, B \in \A \), \(P(A), P(B) &gt; 0\), gilt \(P(A|B) = \frac {P(A \cap B)}{P(B)}\), also \(P(B|A) = \frac
{P(A \cap B)}{P(A)} = \frac {P(A|B) \cdot P(B)}{P(A)} = \frac {P(A|B) \cdot P(B)}{P(A|B) \cdot P(B) + P(A|B^C) \cdot P(B^C)}\).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Seien \(X_1, \dotsc , X_n\) unabha&#x0308;ngige Bernoulli-verteilte Zufallsvariablen mit zufa&#x0308;lligem Parameter \(\theta \in (0, 1)\), d.&#x202f;h. \(\PP (X_i = 1|\vartheta ) = \vartheta
\). Die a-priori-Verteilung \(\pi \) von \(\theta \) sei durch eine <em><span class="dashuline" >Beta-Verteilung</span></em> gegeben, also mit L.-B.-Dichte \(p_{a,b}(x) = \frac {x^{a-1} (1-x)^{b-1}}{B(a, b)}
\1_{(0,1)}(x)\) fu&#x0308;r \(a, b &gt; 0\), wobei \(B(a, b) = \int _0^1 t^{a-1} (1-t)^{b-1} \dt \) die Beta-Funktion ist. Die Beta-Verteilung verallgemeinert die Gleichverteilung auf dem Intervall \((0, 1)\)
(fu&#x0308;r \(a = b = 1\) erha&#x0308;lt man die Gleichverteilung).
</p>

<p>
Sei jetzt \(s = \sum _{i=1}^n x_i\) die Summe der Werte von \(X_1, \dotsc , X_n\).<br />
Dann ist \(p(x|\theta ) = \theta ^s (1 - \theta )^{n-s} \cdot \1_{(0, 1)}(\theta )\) die Za&#x0308;hldichte von \(X\) (\(x \in \{0, 1\}^n\)) und die a-posteriori-Dichte von \(\theta |X = x\) berechnet sich
durch<br />
\(p(\theta |x) = \frac {\pi (\theta ) p(x|\theta )}{\int _\Theta \pi (\theta ’) p(x|\theta ’) \d \theta ’} = \frac {\theta ^{a-1} (1 - \theta )^{b-1} \cdot \theta ^s (1 - \theta )^{n-s}}{\int
_0^1 \dots } \cdot \frac {B(a, b)}{B(a, b)} \cdot \1_{(0, 1)}(\theta ) \propto \theta ^{a+s-1} (1 - \theta )^{b+n-s-1} \1_{(0,1)}(\theta )\).<br />
Also gilt \(\theta |X = x \sim \BetaV (a + s, b + n - s)\).
</p>

<p>
<em>Bemerkung</em>: Damit ist die a-posteriori-Verteilung von \(\theta \) aus derselben Klasse wie die a-priori-Verteilung, die Beta-Verteilung ist eine <em><span class="dashuline" >(zur <span class="textsc"
>Bernoulli</span>-Verteilung) konjugierte Verteilung</span></em>.<br />
Fu&#x0308;r bestimmte Verteilungen, die sich als Exponentialfamilie darstellen lassen, la&#x0308;sst sich eine konjugierte Familie (ebenfalls als Exponentialfamilie) angeben, wie das folgende Lemma zeigt.
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<span class="uline" ><em>Satz</em> (<span class="textsl" >konjugierte Familie fu&#x0308;r Familie der Exponentialverteilungen</span>):</span><br />
Sei \(X|\theta \) eine i.i.d.-Stichprobe einer \(k\)-parametrigen Exponentialfamilie mit Za&#x0308;hl-/L.-B.-Dichte<br />
\(p(x|\theta ) = \1_A(x) \cdot \exp \!\left (\sum _{j=1}^k c_j(\theta ) \sum _{i=1}^n T_j(x_i) + \sum _{i=1}^n S(x_i) + n d(\theta )\right )\), \(x = (x_1, \dotsc , x_n)\).<br />
Dann wird durch die \((k + 1)\)-parametrige Exponentialfamilie gegeben durch<br />
\(\pi (\theta ; t_1, \dotsc , t_{k+1}) \propto \exp \!\left (\sum _{j=1}^k c_j(\theta ) t_j + d(\theta ) t_{k+1}\right )\) eine zu obiger Verteilung von \(X|\theta \) konjugierte Familie definiert.
Fu&#x0308;r die a-posteriori-Verteilung von \(\theta |X = x\) gilt<br />
\(p(\theta |x) \propto \pi \!\left (\theta ; t_1 + \sum _{i=1}^n T_1(x_i), \dotsc , t_k + \sum _{i=1}^n T_k(x_i), t_{k+1} + n\right )\).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Sei \(X \sim \Bin (1, \theta )\) Bernoulli-verteilt mit \(\theta \in (0, 1)\). Dann ist die Dichte von \(X|\theta \) gleich \(p(x|\theta ) = \theta ^x (1 - \theta )^{1-x} = \exp \!\left (x \ln
\!\left (\frac {\theta }{1-\theta }\right ) + \ln (1 - \theta )\right ) \cdot \1_{\{0, 1\}}(x)\). Mit dem Satz erha&#x0308;lt man eine dazu konjugierte \(2\)-parametrige Exponentialfamilie mit
a-priori-Dichte<br />
\(\pi (\theta ; t_1, t_2) \propto \exp (t_1 c(\theta ) + t_2 d(\theta )) = \exp \!\left (t_1 \ln \!\left (\frac {\theta }{1 - \theta }\right ) + t_2 \ln (1 - \theta )\right ) \cdot
\1_{(0,1)}(\theta )\)<br />
\(= \theta ^{t_1} (1 - \theta )^{t_2 - t_1} \1_{(0, 1)}(\theta )\) mit \(t_1, t_2 - t_1 &gt; -1\). Mittels Reparametrisierung \(t_1 \mapsto a - 1\) und \(t_2 \mapsto b + a\) ergibt sich \(\widetilde {\pi
}(\theta ; a, b) \propto \vartheta ^{a-1} (1 - \vartheta )^{b-1} \cdot \1_{(0,1)}(\theta )\), \(a, b &gt; 0\), als konjugierte a-priori-Verteilung (Beta-Verteilung).
</p>

<p>
Die a-posteriori-Verteilung folgt mit obigem Satz: \(p(\theta |x) \propto \pi (\theta ; t_1 + 1, t_2 + 1) = \theta ^{t_1+x} (1 - \theta )^{t_2 + 1 - (t_1 + x)} = \theta ^{a-1+x} (1 - \theta )^{b-x}\).
Dies ist die Dichte der Beta-Verteilung \(\BetaV (a+x, b+1-x)\), die den Erwartungswert \(\frac {a+x}{a+b+1}\) und die Varianz \(\frac {(a+x)(b+1-x)}{(a+b+2)(a+b+1)^2}\) besitzt. Die Beta-Verteilung ist also eine zur
Binomialverteilung konjugierte Verteilung (was auch schon aus obigem Beispiel fu&#x0308;r \(n = 1\) folgt).
</p>

<p>
<span
    class="textcolor"
    style="color:#808080"
>
</span>
</p>

<p>
<em>Beispiel</em>: Seien \(X_1, \dotsc , X_n \sim \N (\mu , \sigma ^2)\) i.i.d. mit bekannter Varianz \(\sigma ^2\) und unbekanntem Erwartungswert \(\mu = \theta \). Es gilt \(p(x|\theta ) \propto \exp \!\left
(-\frac {(x - \mu )^2}{2\sigma ^2}\right ) \propto \exp \!\left (\frac {\theta x}{\sigma ^2} - \frac {\theta ^2}{2\sigma ^2}\right )\). Wenn man in obigem Satz also \(T_1(x) = x\), \(c_1(\theta ) =
\frac {\theta }{\sigma ^2}\) und \(d(\theta ) = -\frac {\theta ^2}{2\sigma ^2}\) wa&#x0308;hlt, erha&#x0308;lt man eine konjugierte \(2\)-parametrige Exponentialfamilie als a-priori-Verteilung mit der Dichte \(\pi
(\theta ; t_1, t_2) \propto \exp \!\left (\frac {\theta }{\sigma ^2} t_1 - \frac {\theta ^2}{2\sigma ^2} t_2\right )\)<br />
\(\propto \exp \!\left (\frac {t_2}{2\sigma ^2} \left (\theta ^2 - \frac {2\sigma ^2}{t_2} \cdot \frac {\theta t_1}{\sigma ^2} + \left (\frac {t_1}{t_2}\right )^2\right )\right ) = \exp
\!\left (\frac {t_2}{2\sigma ^2} \left (\theta - \frac {t_1}{t_2}\right )^2\right )\). Nach \(t_2 &gt; 0\) ist \(\pi (\theta ; t_1, t_2)\) die Dichte einer \(\N \!\left (\frac {t_1}{t_2}, \frac
{\sigma ^2}{t_2}\right )\)-Verteilung. Durch die Reparametrisierung \(t_1 \mapsto \eta \frac {\sigma ^2}{\tau ^2}\) und \(t_2 \mapsto \frac {\sigma ^2}{\tau ^2}\) mit \(\eta \in \real \) und \(\tau ^2
&gt; 0\) erha&#x0308;lt man als a-priori-Verteilung eine \(\N (\eta , \tau ^2)\)-Verteilung.
</p>

<p>
Nach dem Satz hat die a-posteriori-Verteilung die Dichte \(p(\theta |x) \propto \pi (\theta , t_1 + \sum _{i=1}^n T_1(x_i), t_2 + n)\). Mit \(s = \sum _{i=1}^n x_i\) und \(T_1(x_i) = x_i\) erha&#x0308;lt man
also die Dichte von \(\N \!\left (\frac {t_1 + s}{t_2 + n}, \frac {\sigma ^2}{t_2 + n}\right )\).
</p>

<p>
Der Erwartungswert \(\frac {t_1 + s}{t_2 + n} = \frac {n}{\sigma ^2/\tau ^2 + n} \overline {x} + \frac {\sigma ^2/\tau ^2}{\sigma ^2/\tau ^2+n} \eta \) geht fu&#x0308;r \(n \to \infty \) gegen
\(\overline {x}\) (wenn man \(n\) gegen Null laufen lassen wu&#x0308;rde, geht der Erwartungswert gegen \(\eta \)). Die Varianz \(\frac {\sigma ^2}{t_2 + n} = \frac {\sigma ^2}{\sigma ^2/\tau ^2 + n}\) geht
fu&#x0308;r \(n \to \infty \) gegen \(0\) (fu&#x0308;r \(n \to 0\) gegen \(\tau ^2\)). Also wird fu&#x0308;r \(n \to \infty \) der Einfluss der a-priori-Verteilung auf die a-posteriori-Verteilung immer geringer.
</p>

<p>
<em>Bemerkung</em>: Hat man keine a-priori-Information u&#x0308;ber den unbekannten Parameter \(\theta \), so kann dies durch \(\pi (\theta ) \propto 1\), die sog. <em><span class="dashuline" >nicht-informative
a-priori-Verteilung</span></em>, zum Ausdruck gebracht werden. Ist \(\Theta \) jedoch nicht endlich bzw. beschra&#x0308;nkt, so handelt es sich bei \(\pi (\theta )\) nicht um eine Za&#x0308;hl-/L.-B.-Dichte (wegen
fehlender Normierbarkeit). Ist die a-posteriori-Dichte \(p(\theta |x)\) dennoch normierbar, so kann die uneigentliche a-priori-Dichte \(\pi (\theta ) \propto 1\) trotzdem verwendet werden.
</p>

{% endraw %}
</div>
{:/nomarkdown}
