\section{%
    Optimale Tests und Likelihood-Quotienten-Tests%
}

\begin{Bem}
    Man sucht nach optimalen Tests basierend auf Likelihood-Quotienten für
    \begin{itemize}
        \item
        einfache Hypothesen
        $H_0\colon \theta = \theta_0$ vs. $H_1\colon \theta = \theta_1$,

        \item
        für einseitige (zusammengesetzte) Hypothesen, z.\,B.
        $H_0\colon \theta \le \theta_0$ vs. $H_1\colon \theta > \theta_0$, und

        \item
        für zweiseitige Hypothesen
        $H_0\colon \theta = \theta_0$ vs. $H_1\colon \theta \not= \theta_0$,
        wobei in diesem Fall die Klasse der betrachteten Tests eingeschränkt wird.
    \end{itemize}
\end{Bem}

\subsection{%
    Das \name{Neyman}-\name{Pearson}-Lemma%
}

\begin{Def}{UMP-Test}
    Ein Test $\delta^\ast$ zum Niveau $\alpha \in [0, 1]$ heißt
    \begriff{gleichmäßig bester Test (uniformly most powerful test, UMP-Test)},
    für das Testproblem $H_0\colon \theta \in \Theta_0$ vs. $H_1\colon \theta \in \Theta_1$,
    falls für jeden weiteren Test $\delta$ zum selben Niveau $\alpha$ gilt,
    dass $\forall_{\theta \in \Theta_1}\; G_\delta(\theta) \le G_{\delta^\ast}(\theta)$.
\end{Def}

\begin{Bem}
    Da $G_\delta(\theta)$ für $\theta \in \Theta_1$ gleich $1$ minus der
    Fehlerwahrscheinlichkeit 2. Art entspricht,
    sind UMP-Tests charakterisiert durch Minimierung der Fehlerwahrscheinlichkeit 2. Art
    unter allen Tests zum Niveau $\alpha$.
\end{Bem}

\begin{Def}{Likelihood-Quotienten-Statistik}\\
    Sei $p$ Zähl- oder L.-B.-Dichte von $X$, wobei $X$ Werte in $\real^n$ annehme.\\
    Dann heißt $L(x, \theta_0, \theta_1) := \frac{p(x, \theta_1)}{p(x, \theta_0)}$
    \begriff{Likelihood-Quotienten-Statistik} zur Beobachtung $x$.\\
    Man definiert $L(x, \theta_0, \theta_1) := 0$ für $p(x, \theta_1) = p(x, \theta_0) = 0$
    und $L(x, \theta_0, \theta_1) := \infty$ für $p(x, \theta_1) > 0$ und $p(x, \theta_0) = 0$.
\end{Def}

\begin{Bem}
    Große Werte von $L$ sprechen eher für $\theta_1$, kleine eher für $\theta_0$.
\end{Bem}

\begin{Satz}{\name{Neyman}-\name{Pearson}-Lemma}\\
    Seien $(\X, \A, \P)$ ein statistischer Raum mit einem regulären statistischen Modell\\
    $\P = \{p(\cdot, \theta) \;|\; \theta \in \Theta\}$ und
    $\Theta = \{\theta_0, \theta_1\}$ mit Testproblem
    $H_0\colon \theta = \theta_0$ vs. $H_1\colon \theta = \theta_1$.\\
    Dann gibt es für alle $\alpha \in [0, 1]$ Zahlen $k \in [0, \infty]$ und $\gamma \in [0, 1]$,
    sodass $\delta\colon \X \rightarrow [0, 1]$ ein UMP-Test zum Niveau $\alpha$ ist, wobei
    $\delta$ definiert ist durch
    $\delta(x) := \begin{cases}0 & L(x, \theta_0, \theta_1) < k,\\
    \gamma & L(x, \theta_0, \theta_1) = k,\\1 & L(x, \theta_0, \theta_1) > k.\end{cases}$
\end{Satz}

\begin{Bem}
    Im Beweis betrachtet man die Verteilungsfunktion $g$ von $Y\colon \X \rightarrow [0, \infty)$
    mit $Y(x) := L(x, \theta_0, \theta_1)$ für $p(x, \theta_0) > 0$ und $Y(x) := 0$ sonst.
    Für den Fall, dass es ein $\overline{k} \in [0, \infty)$ gibt mit
    $g(\overline{k}) = 1 - \alpha$, wählt man $k := \overline{k}$ und $\gamma := 0$.
    Sonst (falls es kein solches $\overline{k}$ gibt) gibt es ein $\overline{k}$, sodass
    $\lim_{k \to \overline{k}-0} g(k) \le 1 - \alpha < \lim_{k \to \overline{k}+0} g(k)$.
    In diesem Fall wählt man $k := \overline{k}$ und $\gamma \in [0, 1]$, sodass
    $P_{\theta_0}(\{x \;|\; Y(x) \le \overline{k}\}) -
    \gamma P_{\theta_0}(\{x \;|\; Y(x) = \overline{k}\}) = 1 - \alpha$.

    Die Randomisierung bewirkt, dass das vorgegebene Niveau $\alpha$ voll ausgeschöpft wird,
    d.\,h. $\EE_{\theta_0}(\delta(X)) = \alpha$.
    Dies hat aber auch zur Folge, dass die Gütefunktion für $\theta = \theta_1$
    größer (oder gleich) und damit die
    Fehlerwahrscheinlichkeit 2. Art kleiner (oder gleich) wird
    im Vergleich zum nicht-randomisierten Test.
\end{Bem}

\linie
\pagebreak

\begin{Bsp}
    Es wird ein nicht-randomisierter Test zum Niveau $\alpha = 0.05$ gesucht für
    $H_0\colon \theta = \theta_0$ vs. $H_1\colon \theta = \theta_1$, wobei
    $X \sim \Bin(20, \theta)$ und $n := 20$ mit $\theta \in \{0.2, 0.8\}$ und
    $\theta_0 = 0.2$, $\theta_1 = 0.8$.\\
    Der Test ist definiert durch $\delta_{\text{nr}}(x) := \1_{\{p(x, 0.8)/p(x, 0.2) \ge k\}}$.
    Dabei ist\\$\frac{p(x, 0.8)}{p(x, 0.2)} =
    \frac{\binom{n}{x} 0.8^x 0.2^{n-x}}{\binom{n}{x} 0.2^x 0.8^{n-x}}
    = 4^x (1/4)^{n-x} = 4^{2x} 4^{-n}$ monoton in $x$,
    d.\,h. $\frac{p(x, 0.8)}{p(x, 0.2)} \ge k \iff x \ge k'$.\\
    Wegen $\PP_{0.2}(X \le 6) \approx 0.913$ und $\PP_{0.2}(X \le 7) \approx 0.968$ wird
    $H_0\colon \theta = 0.2$ abgelehnt, falls $x > 7$,
    denn dann ist $\PP(H_0 \text{ abl.} | H_0 \text{ wahr}) = \PP_{0.2}(X > 7)
    = 1 - 0.968 = 0.032 < \alpha$.
    Außerdem gilt $\PP(H_0 \text{ abl.} | H_1 \text{ wahr}) = \PP_{0.8}(X > 7)
    = 1 - \PP_{0.8}(X \le 7) = 1 - 1.5 \cdot 10^{-5}$,
    d.\,h. die Fehlerwahrscheinlichkeit 2. Art ist sehr klein.

    Nun betrachtet man den randomisierten Test
    $\delta_{\text{r}}(x) := 0$ für $\frac{p(x, 0.8)}{p(x, 0.2)} < k$,
    $\delta_{\text{r}}(x) := \gamma$ für $\frac{p(x, 0.8)}{p(x, 0.2)} = k$ und
    $\delta_{\text{r}}(x) := 1$ für $\frac{p(x, 0.8)}{p(x, 0.2)} > k$.
    Dies entspricht den Fällen $x < 7$, $x = 7$ und $x > 7$
    (sonst kein Test zum Niveau $\alpha$).
    Nach dem Beweis des Satzes muss $\gamma \in [0, 1]$ so gewählt werden, dass
    $\PP_{0.2}(X > 7) + \gamma \PP_{0.2}(X = 7) = \alpha = 0.05$,
    also $\gamma = \frac{\alpha - \PP_{0.2}(X > 7)}{\PP_{0.2}(X = 7)} \approx 0.327$.
    Damit ergibt sich
    $\PP(H_0 \text{ abl.} | H_1 \text{ wahr}) = \EE_{0.8}(\delta_{\text{r}})
    = \gamma \cdot \PP_{0.8}(X = 7) + 1 \cdot \PP_{0.8}(X > 7) \approx 0.99999$.
    Damit gilt für die beiden zu $\delta_{\text{nr}}$ und $\delta_{\text{r}}$ zugehörigen
    Gütefunktionen, dass
    $G_{\delta_{\text{nr}}}(\theta) < G_{\delta_{\text{r}}}(\theta)$ für
    $\theta = \theta_0, \theta_1$.
    Also ist $\delta_{\text{r}}$ ein besserer Test zum Niveau $\alpha = 0.05$
    als $\delta_{\text{nr}}$ (sogar optimal zum Niveau $\alpha = 0.05$ nach dem
    Neyman-Pearson-Lemma).
\end{Bsp}

\subsection{%
    Optimale einseitige Tests%
}

\begin{Def}{monotoner Dichtequotient}
    Sei $\P = \{p(\cdot, \theta) \;|\; \theta \in \Theta\}$ ein reguläres, einparametriges
    statistisches Modell (d.\,h. $\Theta \subset \real$).
    Dann besitzt $\P$ einen \begriff{monotonen Dichtequotienten} bzgl. der Statistik $T$,
    falls es für alle $\theta_1, \theta_2 \in \Theta$ mit $\theta_1 < \theta_2$
    eine streng monoton wachsende Funktion
    $q_{\theta_1,\theta_2}\colon \real \rightarrow [0, \infty]$ gibt mit
    $q_{\theta_1,\theta_2}(T(x)) = \frac{p(x, \theta_2)}{p(x, \theta_1)}$ für alle $x \in \X$.
\end{Def}

\begin{Bsp}
    Einparametrige Exp.familien mit Dichte
    $p(x, \theta) = \1_A(x) \cdot \exp(c(\theta)T(x) + d(\theta) + S(x))$
    besitzen einen monotonen Dichtequotienten bzgl. der Statistik $T$,
    wenn $c\colon \Theta \rightarrow \real$ streng monoton wachsend ist,
    da $q_{\theta_1,\theta_2}(T(x)) := \exp((c(\theta_2) - c(\theta_1)) T(x) +
    d(\theta_2) - d(\theta_1))$ in $T(x)$ streng monoton wachsend ist
    (für $\theta_1 < \theta_2$).
\end{Bsp}

\begin{Satz}{UMP-Tests bei rechtsseitigen Hypothesen}
    $\P = \{p(\cdot, \theta) \;|\; \theta \in \Theta\}$ ($\Theta \subset \real$) besitze
    einen monotonen Dichtequotienten bzgl. der Statistik $T$ und
    $H_0\colon \theta \le \theta_0$ vs. $H_1\colon \theta > \theta_0$.\\
    Dann gibt es für alle $\alpha \in (0, 1)$ Zahlen $c \in \real$ und $\gamma \in [0, 1]$,
    sodass $\delta\colon \X \rightarrow [0, 1]$ ein UMP-Test zum Niveau $\alpha$ ist, wobei
    $\delta$ definiert ist durch
    $\delta(x) :=  \begin{cases}0 & T(x) < c,\\
    \gamma & T(x) = c,\\1 & T(x) > c.\end{cases}$
\end{Satz}

\begin{Bem}
    $\delta$ ist sogar ein Level-$\alpha$-Test.\\
    $\gamma$ und $c$ ergeben sich genauso wie beim Beweis vom Neyman-Pearson-Lemma,
    wenn man $Y(x)$ durch $T(x)$ ersetzt.
    Der im Satz definierte Test $\delta$ ist ein UMP-Test für jedes
    $c \in \real$ und $\gamma \in [0, 1]$, sodass
    $\PP_{\theta_0}(T(X) \le c) - \gamma  \PP_{\theta_0}(T(X) = c) = 1 - \alpha$.
    Im Fall $\PP_{\theta_0}(T(X) = c) = 0$ ist jedes $\gamma$ erlaubt und $c$ ist
    dann das $(1 - \alpha)$-Quantil der Verteilung von $T(X)$ unter $\theta = \theta_0$.\\
    Ist $H_0\colon \theta \ge \theta_0$ vs $H_1\colon \theta < \theta_0$ zu testen,
    so gibt es unter den Voraussetzungen des Satzes von eben
    für alle $\alpha \in (0, 1)$ Zahlen $c \in \real$ und $\gamma \in [0, 1]$,
    sodass $\delta\colon \X \rightarrow [0, 1]$ ein UMP-Test zum Niveau $\alpha$ ist, wobei
    $\delta$ definiert ist durch
    $\delta(x) :=  \begin{cases}0 & T(x) > c,\\
    \gamma & T(x) = c,\\1 & T(x) < c.\end{cases}$
\end{Bem}

\vspace{3mm}
\linie
\pagebreak

\begin{Bsp}
    Seien $X_1, \dotsc, X_n \sim \N(\mu, \sigma^2)$ i.i.d. mit $\mu$ unbekannt und $\sigma^2$
    bekannt, wobei $X = (X_1, \dotsc, X_n)$.
    Zu testen ist $H_0\colon \mu \le \mu_0$ vs. $H_1\colon \mu > \mu_0$.
    Für die Dichte $p(\cdot, \mu)$ von $X$ gilt\\
    $\ln p(x, \mu) = -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 -
    \frac{n}{2} \ln(2\pi\sigma^2)
    = c(\mu) T(x) - \frac{n}{2} \left(\frac{\mu^2}{\sigma^2} +
    \ln(2\pi\sigma^2)\right) - \frac{n\overline{x}}{2\sigma^2}$\\
    mit $T(x) := \frac{\overline{x}}{\sigma/\sqrt{n}}$ und $c(\mu) := \frac{\mu}{\sigma/\sqrt{n}}$.
    Also gehört die Verteilung von $X$ zu einer $1$-parametrigen Exponentialfamilie.
    $c\colon \real \rightarrow \real$ ist streng monoton wachsend, d.\,h.
    $\P$ besitzt nach obiger Bemerkung einen monotonen Dichtequotienten bzgl. $T$.

    Wegen $\PP_\mu(T(X) = c) = 0$ für alle $c \in \real$
    kann $\gamma$ beliebig gewählt werden, z.\,B. $\gamma = 1$.
    Der nicht-randomisierte Test $\delta(x) := \1_{\{T(X) \ge c\}}$ aus dem Satz hat die
    Gütefunktion\\
    $G_\delta(\mu) = \EE_\mu(\delta(X)) = \PP_\mu(\delta(X) = 1) =
    \PP_\mu\!\left(\frac{\overline{X}}{\sigma/\sqrt{n}} \ge c\right)
    = \PP_\mu\!\left(\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \ge
    c - \frac{\mu}{\sigma/\sqrt{n}}\right)$\\
    $= 1 - \Phi\!\left(c - \frac{\mu}{\sigma/\sqrt{n}}\right)$.
    Für einen Level-$\alpha$-Test muss\\
    $\sup_{\mu \le \mu_0} G_\delta(\mu) =
    \sup_{\mu \le \mu_0} \left[1 - \Phi\!\left(c - \frac{\mu}{\sigma/\sqrt{n}}\right)\right]
    \overset{!}{=} \alpha$ gelten.
    Der Ausdruck in eckigen Klammern ist monoton wachsend in $\mu$, daher ist dies äquivalent zu\\
    $1 - \Phi\!\left(c - \frac{\mu_0}{\sigma/\sqrt{n}}\right) = \alpha
    \iff \Phi\!\left(c - \frac{\mu_0}{\sigma/\sqrt{n}}\right) = 1 - \alpha
    \iff c = z_{1-\alpha} + \frac{\mu_0}{\sigma/\sqrt{n}}$.

    Nach dem Satz ist daher
    $\delta(X) = \1_{\left\{\frac{\overline{X} - \mu_0}
    {\sigma/\sqrt{n}} \,\ge\, z_{1-\alpha}\right\}}$
    ein UMP-Test für $H_0\colon \mu \le \mu_0$ vs. $H_1\colon \mu > \mu_0$
    (einseitiger Gauß-Test).
\end{Bsp}

\subsection{%
    Optimale zweiseitige Tests%
}

\begin{Bem}
    Im Folgenden werden verschiedene Arten von zweiseitigen Hypothesen betrachtet:
    \begin{enumerate}[label=\arabic*.]
        \item
        $H_0\colon \theta = \theta_0$ vs.
        $H_1\colon \theta \not= \theta_0$

        \item
        $H_0\colon \theta \in [\theta_1, \theta_2]$ vs.
        $H_1\colon \theta \notin [\theta_1, \theta_2]$

        \item
        $H_0\colon \theta \notin (\theta_1, \theta_2)$ vs.
        $H_1\colon \theta \in (\theta_1, \theta_2)$
    \end{enumerate}
    UMP-Tests zu diesen Hypothesen existieren nur unter speziellen Bedingungen.
\end{Bem}

\begin{Bsp}
    Seien wieder $X_1, \dotsc, X_n \sim \N(\mu, \sigma^2)$ i.i.d. mit $\mu$ unbekannt und
    $\sigma^2$ bekannt.
    Das Testproblem sei $H_0\colon \mu = \mu_0$ vs. $H_1\colon \mu \not= \mu_0$.\\
    Dann ist der zweiseitige Gauß-Test $\delta(X) := \1_{\{|T(X)| \ge z_{1-\alpha/2}\}}$
    zum Niveau $\alpha$ mit $T(X) := \frac{\overline{X} - \mu_0}{\sigma/\sqrt{n}}$
    kein UMP-Test für dieses Testproblem,
    da die Gütefunktion des Neyman-Pearson-Tests für
    $H_0\colon \mu = \mu_0$ vs. $H_1\colon \mu = \mu_1$ für ein beliebiges (aber festes)
    $\mu_1 > \mu_0$ für $\mu = \mu_1$ größer ist.\\
    Alternativ kann man auch argumentieren, dass die Gütefunktion des einseitigen Gauß-Tests für
    $H_0\colon \mu \le \mu_0$ vs. $H_1\colon \mu > \mu_0$ für alle $\mu > \mu_0$ besser ist
    als die des zweiseitigen Gauß-Tests.
    Jedoch ist der einseitige Gauß-Test zum zweiseitigen Testproblem ein verfälschter Test,
    da für $\mu < \mu_0$ (Spezialfall der Alternativhypothese) die Wahrscheinlichkeit
    $H_0$ abzulehnen kleiner ist als die Wahrscheinlichkeit $H_0$ abzulehnen, wenn $H_0$
    wahr ist (also die Fehlerw.keit 1. Art).
\end{Bsp}

\begin{Def}{unverfälscht}\\
    Ein statistischer Hypothesentest $\delta$ zum Niveau $\alpha$ heißt \begriff{unverfälscht},
    falls $\forall_{\theta \in \Theta_1}\; G_\delta(\theta) \ge \alpha$.
\end{Def}

\linie
\pagebreak

\begin{Bem}
    Für spezielle $1$-parametrige Exponentialfamilien mit monotonem Dichtequotienten können
    unter gewissen weiteren Regularitätsvoraussetzungen gleichmäßig beste Tests
    (unter allen unverfälschten Tests) konstruiert werden.


    Diese hier angesprochenen Tests erhält man auch als Kombination zweier einseitiger Tests.
    Im Folgenden seien die Annahmen des Satzes zu optimalen einseitigen Tests erfüllt.
    \begin{enumerate}[label=\arabic*.]
        \item
        Bestimme die Konstanten $\gamma_r$ und $c_r$ zum rechtsseitigen Testproblem\\
        $H_0\colon \theta \le \theta_0$ vs.
        $H_1\colon \theta > \theta_0$.

        \item
        Bestimme die Konstanten $\gamma_\ell$ und $c_\ell$ zum linksseitigen Testproblem\\
        $H_0\colon \theta \ge \theta_0$ vs.
        $H_1\colon \theta < \theta_0$.
    \end{enumerate}
    Dadurch erhält man zwei UMP-Tests
    $\delta_\ell(x) :=  \begin{cases}0 & T(x) > c_\ell,\\
    \gamma_\ell & T(x) = c_\ell,\\1 & T(x) < c_\ell,\end{cases}$
    und
    $\delta_r(x) :=  \begin{cases}0 & T(x) < c_r,\\
    \gamma_r & T(x) = c_r,\\1 & T(x) > c_r,\end{cases}$\\
    Falls $\alpha < 1$ ist, so gilt stets $c_\ell \le c_r$.
    Für $c_\ell < c_r$ können $\delta_\ell$ und $\delta_r$ zu einem einzigen Test kombiniert
    werden:
    $\delta(x) :=  \begin{cases}0 & T(x) \in (c_\ell, c_r),\\
    \gamma_\ell & T(x) = c_\ell,\\\gamma_r & T(x) = c_r,\\
    1 & T(x) \notin [c_\ell, c_r].\end{cases}$\\
    Man kann zeigen, dass dies ein UMP-Test unter allen unverfälschten Tests zum Niveau $\alpha$
    für das zweiseitige Testproblem $H_0\colon \theta = \theta_0$ vs.
    $H_1\colon \theta \not= \theta_0$ ist.
\end{Bem}

\subsection{%
    Likelihood-Quotienten-Tests%
}

\begin{Bem}
    Das Ziel ist die Verallgemeinerung der Neyman-Pearson-Teststatistik $L(x, \theta_0, \theta_1)$
    für das Testproblem $H_0\colon \theta = \theta_0$ vs. $H_1\colon \theta = \theta_1$
    auf allgemeine Testprobleme der Form\\
    $H_0\colon \theta \in \Theta_0$ vs. $H_1\colon \theta \in \Theta_1$.
\end{Bem}

\begin{Def}{verallgemeinerte Likelihood-Quotienten-Statistik}\\
    Sei $\P = \{p(\cdot, \theta) \;|\; \theta \in \Theta\}$ ein reguläres statistisches Modell.\\
    Dann heißt $L(X) := \frac{\sup_{\theta \in \Theta_1} p(X, \theta)}
    {\sup_{\theta \in \Theta_0} p(X, \theta)}$
    \begriff{verallgemeinerte Likelihood-Quotienten-Statistik}.
\end{Def}

\begin{Def}{verallgemeinerter Likelihood-Quotienten-Test}
    Der Hypothesentest $\delta(X) := \1_{\{L(X) \ge c\}}$ heißt
    \begriff{verallgemeinerter Likelihood-Quotienten-Test} zu einem kritischen Wert
    $c \in [0, \infty]$.
\end{Def}

\begin{Bem}
    Der Zähler der verallg. L.-Q.-Statistik ist häufig schwer zu berechnen.
    Daher geht man in der Praxis häufig wie folgt vor:
    \begin{enumerate}[label=\arabic*.]
        \item
        Berechne den MLS $\widehat{\theta}$ von $\theta \in \Theta$.

        \item
        Berechne den MLS $\widehat{\theta}_0$ von $\theta \in \Theta_0$.

        \item
        Berechne $\lambda(x) := \frac{p(x, \widehat{\theta})}{p(x, \widehat{\theta}_0)}
        = \frac{\sup_{\theta \in \Theta} p(x, \theta)}{\sup_{\theta \in \Theta_0} p(x, \theta)}$
        (leichter zu berechnender Zähler).

        \item
        Finde eine strikt monotone Funktion $h$ auf dem Bild von $\lambda$,
        sodass die Verteilung von $h(\lambda(X))$ unter $H_0$ bekannt ist.
    \end{enumerate}
    Dadurch erhält man einen verallg. L.-Q.-Test der Form
    $\delta(X) := \1_{\{h(\lambda(X)) \ge h_{1-\alpha}\}}$
    mit $h_{1-\alpha}$ dem $(1-\alpha)$-Quantil der Verteilung von $h(\lambda(X))$ unter $H_0$.
    Der Zusammenhang zwischen $\lambda$ und $L$ wird durch
    $\lambda(x) = \frac{\max\{\sup_{\theta \in \Theta_1} p(x, \theta),\;
    \sup_{\theta \in \Theta_0} p(x, \theta)\}}{\sup_{\theta \in \Theta_0} p(x, \theta)}
    = \max\{L(x), 1\}$ ersichtlich.
    Wenn $\lambda(x)$ bzw. $L(x)$ "`deutlich"' größer als $1$ ist, so spricht dies eher
    gegen $H_0$.
\end{Bem}

\linie
\pagebreak

\begin{Bem}
    Basierend auf der Dualität zwischen Hypothesentests und Konfidenzintervallen lassen sich
    Konfidenzbereiche für den unbekannten Parameter $\theta \in \Theta \subset \real^d$
    konstruieren.\\
    Man betrachtet dazu das Testproblem $H_0\colon \theta = \theta_0$ vs.
    $H_1\colon \theta \not= \theta_0$.
    Bestimme $c(\theta_0)$ durch $\alpha =
    \PP_{\theta_0}\!\left(\frac{\sup_{\theta \in \Theta}
    p(X, \theta)}{p(X, \theta_0)} \ge c(\theta_0)\right)
    = \PP_{\theta_0}(\lambda(X) \ge c(\theta_0))$.\\
    Falls der Annahmebereich
    $C(x) := \left\{\theta \in \Theta \;|\;
    p(x, \theta) > \frac{\sup_{\theta \in \Theta} p(x, \theta)}{c(\theta_0)}\right\}$
    des verallg. L.-Q.-Tests $\delta(X) := \1_{\{\lambda(X) \ge c(\theta_0)\}}$
    in der Form
    $[\underline{C}_1(x), \overline{C}_1(x)] \times \dotsb \times
    [\underline{C}_d(x), \overline{C}_d(x)]$ geschrieben werden kann,
    so ist $C(x)$ ein $(1-\alpha)$-Konfidenzbereich für den unbekannten Parameter
    $\theta \in \Theta$.
\end{Bem}

\linie

\begin{Bsp}
    Seien $X_1, \dotsc, X_n \sim \N(\mu, \sigma^2)$ i.i.d.
    mit $\theta = (\mu, \sigma^2) \in \Theta := \real \times \real^+$ unbekannt.
    Das zu testende Hypothesenpaar lautet $H_0\colon \mu = \mu_0$ vs. $H_1\colon \mu \not= \mu_0$,
    also $\Theta_0 := \{(\mu_0, \sigma^2) \;|\; \sigma^2 \in \real^+\}$ und
    $\Theta_1 := \Theta \setminus \Theta_0$.
    Die Dichte von $X := (X_1, \dotsc, X_n)$ ist gleich\\
    $p(x, \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}
    \exp\!\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
    = \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\!\left(-\frac{1}{2\sigma^2}
    \sum_{i=1}^n (x_i - \mu)^2\right)$.\\
    Man berechnet nun den MLS $\widehat{\theta} := (\overline{X}, \widehat{\sigma}^2)$
    für $\theta \in \Theta$, wobei
    $\widehat{\sigma}^2 := \frac{n-1}{n} S^2(X)$ die unkorrigierte Stichprobenvarianz ist.
    Für $\mu = \mu_0$ ergibt sich als MLS für $\sigma^2$ der Schätzer
    $\widehat{\sigma}_0^2 := {S^\ast}^2(X) = \frac{1}{n} \sum_{i=1}^n (X_i - \mu_0)^2$,
    also ist $\widehat{\theta}_0 := (\mu_0, \widehat{\sigma}_0^2)$ der MLS für
    $\theta \in \Theta_0$.

    Somit erhält man den verallg. L.-Q.-Test $\delta(X) = \1_{\{h(\lambda(X)) \ge h_{1-\alpha}\}}$
    mit $\lambda(x) := \frac{p(x, \widehat{\theta})}{p(x, \widehat{\theta}_0)}$.\\
    Also gilt $\ln \lambda(x)
    = \ln p(x, \widehat{\theta}) - \ln p(x, \widehat{\theta}_0)$\\
    $= -\frac{1}{2\widehat{\sigma}^2} \sum_{i=1}^n (x_i - \overline{x})^2 -
    \frac{n}{2} \ln(2\pi\widehat{\sigma}^2) +
    \frac{1}{2\widehat{\sigma}_0^2} \sum_{i=1}^n (x_i - \mu_0)^2 -
    \frac{n}{2} \ln(2\pi\widehat{\sigma}_0^2)
    = \frac{n}{2} \ln(\widehat{\sigma}_0^2/\widehat{\sigma}^2)$.\\
    Wegen der strengen Monotonie von $\ln$ kann der Test auch
    durch $\delta(X) = \1_{\{\widehat{\sigma}_0^2(X)/\widehat{\sigma}^2(X) > c\}}$
    definiert werden, wobei der kritische Wert $c$ so gewählt wird,
    dass das vorgegebene Niveau $\alpha$ eingehalten wird.

    Zur Bestimmung der Verteilung von $\widehat{\sigma}_0^2/\widehat{\sigma}^2$
    berechnet man
    $\widehat{\sigma}_0^2/\widehat{\sigma}^2
    = \frac{\widehat{\sigma}^2 + (\overline{X} - \mu_0)^2}{\widehat{\sigma}^2}
    = 1 + \frac{(\overline{X} - \mu_0)^2}{\widehat{\sigma}^2}$\\
    $= 1 + \frac{1}{n-1} T(X)^2$ mit $T(X) := \frac{\overline{X} - \mu_0}{S(X)/\sqrt{n}} \sim
    t_{n-1}$ unter $H_0\colon \mu = \mu_0$.
    Damit ist $\delta$ äquivalent zu einem Test
    $\widetilde{\delta}(X) := \1_{\{|T(X)| > \widetilde{c}\}}$ mit
    $\widetilde{c} := t_{n-1,1-\alpha/2}$.

    Die Gütefunktion berechnet sich zu
    $G_{\widetilde{\delta}}(\theta)
    = \EE_\theta(\widetilde{\delta}(X))
    = \PP_\theta(|T(X)| > t_{n-1,1-\alpha/2})$\\
    $= \PP_\theta\!\left(\left|\frac{\overline{X} - \mu}{S(X)/\sqrt{n}} +
    \frac{\mu - \mu_0}{S(X)/\sqrt{n}}\right| > t_{n-1,1-\alpha/2}\right)$,
    denn $T(X)$ besitzt eine nicht-zentrale $t$-Verteilung mit Nichtzentralitätsparameter
    $\Delta = \Delta(\theta) = \frac{\mu - \mu_0}{\sigma/\sqrt{n}}$.

    Der Annahmebereich $C(X)$ des Tests $\widetilde{\delta}$ ist ein
    $(1-\alpha)$-Konfidenzintervall für $\mu$,
    dabei gilt $C(X) = \{\mu \in \real \;|\; |T(X)| \le t_{n-1,1-\alpha/2}\}
    = \overline{X} \pm \frac{S(X)}{\sqrt{n}} t_{n-1,1-\alpha/2}$.
\end{Bsp}

\pagebreak
