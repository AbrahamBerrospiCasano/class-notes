\section{%
    Approximation%
}

\subsection{%
    Lineare Least-Squares-Approximation%
}

\textbf{Least Squares in 1D}:
Gegeben sind $x_1, \dotsc, x_N \in \real$, $f_1, \dotsc, f_N \in \real$ und $m \in \natural$.\\
Gesucht ist eine Funktion $g \in \PP_m(\real)$ mit
$F := \sum_{i=1}^N (f_i - g(x_i))^2 \to \min$.\\
Sei zunächst $\{p_k\}_{k=0}^m$ eine Basis von $\PP_m(\real)$ (z.\,B. Monome).
Dann erhält man mit\\
$g(x) = \sum_{k=0}^m c_k p_k(x)$, dass
$0 \overset{!}{=} \partial_{c_j} F = -\sum_{i=1}^N 2(f_i - \sum_{k=0}^m c_k p_k(x_i)) p_j(x_i)$
für $j = 0, \dotsc, m$
erfüllt sein muss.
Die hinreichende Bedingung für ein Minimum, dass die Hesse-Matrix\\
$F'' = (\partial_{c_i} \partial_{c_j} F)_{i,j}
= (\sum_{k=1}^N p_i(x_k) p_j(x_k))_{i,j}$ positiv definit ist,
ist erfüllt, weil die $p_i$ l.\,u. sind.\\
Damit bekommt man
$A (c_0, \dotsc, c_m)^\tp = ((f, p_0), \dotsc, (f, p_m))^\tp$
mit $A := ((p_i, p_j))_{i,j=0}^m$ und $(p_i, p_j) := \sum_{k=1}^N p_i(x_k) p_j(x_k)$.

\linie

\textbf{lineare Least-Squares-Approximation}:\\
Gegeben seien $C \in \real^{N \times k}$ mit $N > k$ und vollem Spaltenrang $k$ sowie
$\vec{d} \in \real^N$.\\
Gesucht ist $\vec{y} \in \real^k$ mit $|\vec{r}|^2 \to \min$ und
\begriff{Residuum} $\vec{r} := C\vec{y} - \vec{d}$
(da $C\vec{y} = \vec{d}$ überbest.).\\
Es gilt
$|\vec{r}|^2
= \vec{r}^\tp \vec{r}
= \vec{y}^\tp A \vec{y} - 2\vec{b}^\tp \vec{y} + \vec{d}^\tp \vec{d}$
mit $A := C^\tp C \in \real^{k \times k}$ und $\vec{b} = C^\tp \vec{d} \in \real^k$.\\
$A$ ist positiv definit, weil $C$ vollen Spaltenrang besitzt.
Durch $\forall_{i=1,\dotsc,k}\; \partial_{y_i} \vec{r}^\tp \vec{r} \overset{!}{=} 0$
erhält man die \begriff{Normalengleichungen} $A\vec{y} = \vec{b}$.
Damit erhält man $\vec{y} = A^{-1} \vec{b} = (C^\tp C)^{-1} C^\tp \vec{d}$
(Hesse-Matrix ist $H = 2A$, d.\,h. positiv definit).\\
Ein Nachteil der Normalengleichungen ist, dass die Konditionszahl groß sein kann.

\textbf{Zusammenhang mit Regression}:
Seien nun $\vec*{x}{1}, \dotsc, \vec*{x}{N} \in \real^d$ und $f_1, \dotsc, f_N \in \real$
gegeben.
Gesucht ist $g \in \PP_m(\real^d)$ wie vorhin.
Ist $\{p_k\}_{k=1}^M$ eine Basis von $\PP_m(\real^d)$
(mit $M := \binom{d+m}{m}$),
so erhält man das für $N > M$ überbestimmte LGS
$\forall_{i=1,\dotsc,N}\; g(\vec*{x}{i}) = \vec{p}(\vec*{x}{i})^\tp \vec{c} = f_i$
mit $\vec{p}(\vec{x}) := (p_1(\vec{x}), \dotsc, p_M(\vec{x}))^\tp$.
Damit erhält man die Normalengleichungen
$A\vec{c} = \vec{b}$ mit\\
$A := \sum_{i=1}^N \vec{p}(\vec*{x}{i}) \vec{p}(\vec*{x}{i})^\tp$ und
$\vec{b} := \sum_{i=1}^N f_i \vec{p}(\vec*{x}{i})$
($A = C^\tp C$ mit $C_{i,j} = p_j(\vec*{x}{i})$).

\subsection{%
    Weighted Least Squares (WLS)%
}

\textbf{Weighted Least Squares (WLS)}:
Gegeben seien $\vec*{x}{1}, \dotsc, \vec*{x}{N} \in \real^d$ und
$f_1, \dotsc, f_N \in \real$.\\
Die \begriff{WLS-Methode} sucht nun für $\vec{\xi} \in \real^d$ fest
ein Polynom $g \in \PP_m(\real^d)$ mit\\
$F := \sum_{i=1}^N \theta(|\vec{\xi} - \vec*{x}{i}|) (f_i - g(x_i))^2 \to \min$
mit einer Gewichtsfunktion $\theta\colon [0, \infty) \to \real$.\\
Mit dem Ansatz $g(\vec{x}) = \vec{p}(\vec{x})^\tp \vec{c}$ und
$\vec{\nabla} F(\vec{c}) = \vec{0}$ erhält man das LGS\\
$\sum_{i=1}^N \theta(r_i) \vec{p}(\vec*{x}{i}) \vec{p}(\vec*{x}{i})^\tp \cdot \vec{c}
= \sum_{i=1}^N \theta(r_i) f_i \vec{p}(\vec*{x}{i})$ mit $r_i := |\vec{\xi} - \vec*{x}{i}|$.\\
Gebräuchliche Gewichtsfunktionen sind
$\theta(r) := e^{-r^2/h^2}$ für $h > 0$ (\begriff{Gauß}),\\
$\theta(r) := (1 - \frac{r}{h})^4 (\frac{4r}{h} + 1)$ für $r \in [0, h]$ und $h > 0$
(\begriff{\name{Wendland}}) und
$\theta(r) := \frac{1}{r^2 + \varepsilon^2}$ für $\varepsilon > 0$.\\
Wegen numerischen Instabilitäten kann es hilfreich sein,
$\vec{\xi}$ in den Ursprung zu verschieben.

\linie

\textbf{WLS in 1D}:
Seien $x_1, \dotsc, x_N \in \real$, $f_1, \dotsc, f_N \in \real$ und $\xi \in \real$.
\begin{itemize}
    \item
    linearer Ansatz:
    $g(x) := \vec{p}(x)^\tp \vec{c}$ mit $\vec{p}(x) := (1, x)^\tp$
    führt zu $\vec{c} = A^{-1} \vec{b}$ mit\\
    $A := \sum_{i=1}^N \theta(|\xi - x_i|) \smallpmatrix{1&x_i\\x_i&x_i^2}$ und
    $\vec{b} := \sum_{i=1}^N \theta(|\xi - x_i|) f_i \smallpmatrix{1\\x_i}$

    \item
    quadratischer Ansatz:
    $g(x) := \vec{p}(x)^\tp \vec{c}$ mit $\vec{p}(x) := (1, x, x^2)^\tp$
    führt zu $\vec{c} = A^{-1} \vec{b}$ mit\\
    $A := \sum_{i=1}^N \theta(|\xi - x_i|)
    \smallpmatrix{1&x_i&x_i^2\\x_i&x_i^2&x_i^3\\x_i^2&x_i^3&x_i^4}$ und
    $\vec{b} := \sum_{i=1}^N \theta(|\xi - x_i|) f_i \smallpmatrix{1\\x_i\\x_i^2}$
\end{itemize}

\pagebreak

\subsection{%
    Moving Least Squares (MLS)%
}

\textbf{Moving Least Squares (MLS)}:
Gegeben seien $\vec*{x}{1}, \dotsc, \vec*{x}{N} \in \real^d$ und
$f_1, \dotsc, f_N \in \real$.\\
Die \begriff{MLS-Methode} sucht nun für jedes $\vec{x} \in \real^d$
ein Polynom $g_{\vec{x}} \in \PP_m(\real^d)$ mit\\
$F_{\vec{x}} := \sum_{i=1}^N \theta(|\vec{x} - \vec*{x}{i}|) (f_i - g(x_i))^2 \to \min$
mit einer Gewichtsfunktion $\theta\colon [0, \infty) \to \real$.\\
Mit dem Ansatz $g_{\vec{x}}(\vec{y}) = \vec{p}(\vec{y})^\tp \vec*{c}{\vec{x}}$ und
$\vec{\nabla} F_{\vec{x}}(\vec*{c}{\vec{x}}) = \vec{0}$ erhält man das LGS\\
$\sum_{i=1}^N \theta(r_i) \vec{p}(\vec*{x}{i}) \vec{p}(\vec*{x}{i})^\tp \cdot \vec*{c}{\vec{x}}
= \sum_{i=1}^N \theta(r_i) f_i \vec{p}(\vec*{x}{i})$ mit $r_i := |\vec{x} - \vec*{x}{i}|$.\\
Die Approximation ist dann gegeben durch
$h\colon \real^d \to \real$, $h(\vec{x}) := g_{\vec{x}}(\vec{x})$,
d.\,h. für jeden Auswertungspunkt $\vec{x}$ muss ein WLS-Problem gelöst werden.

\linie

\textbf{Bilddeformation mit MLS}:\\
Seien $\Omega := [0, 1]^2$ und $\vec*{p}{i}, \vec*{q}{i} \in \Omega$
für $i = 1, \dotsc, N$ Kontrollpunkte.\\
Gesucht ist nun eine Deformationsfunktion $\vec{f}$ mit
$\vec{f}(\vec*{p}{i}) = \vec*{q}{i}$ für $i = 1, \dotsc, N$ (Interpolation),
$\vec{f}$ glatt und
$[\forall_{i=1,\dotsc,N}\; \vec*{p}{i} = \vec*{q}{i}] \implies \vec{f} = \id_\Omega$.\\
Dazu benutzt man MLS mit af"|finen Transformationen, d.\,h. man setzt
$\vec{f}(\vec{x}) = \vec*{g}{\vec{x}}(\vec{x})$ an,
wobei $\vec*{g}{\vec{x}}(\vec{y}) := M_{\vec{x}} \vec{y} + \vec*{t}{\vec{x}}$,
$F_{\vec{x}} := \sum_{i=1}^N w_i |\vec*{g}{\vec{x}}(\vec*{p}{i}) - \vec*{q}{i}|^2 \to \min$ und
$w_i := \frac{1}{|\vec*{p}{i} - \vec{x}|^{2\alpha}}$.\\
Durch $\vec*{\nabla}{\vec{t}} F \overset{!}{=} \vec{0}$
erhält man $\vec{t} = \vec*{q}{\ast} - M\vec*{p}{\ast}$ mit
$\vec*{p}{\ast} := \frac{\sum_{i=1}^N w_i\vec*{p}{i}}{\sum_{i=1}^N w_i}$ und
$\vec*{q}{\ast} := \frac{\sum_{i=1}^N w_i\vec*{q}{i}}{\sum_{i=1}^N w_i}$.\\
Ist $M = (m_{i,j})_{i,j=1}^2$, so bekommt man mit $\partial_{m_{i,j}} F \overset{!}{=} 0$,
$\vec*{\varrho}{i} = \vec*{p}{i} - \vec*{p}{\ast}$ und
$\vec*{\sigma}{i} = \vec*{q}{i} - \vec*{q}{\ast}$ die Bedingung
$MR = S$ mit
$R := \sum_{i=1}^N w_i \vec*{\varrho}{i} \vec*{\varrho}{i}^\tp$ und
$S := \sum_{i=1}^N w_i \vec*{\varrho}{i} \vec*{\sigma}{i}^\tp$,
d.\,h. $M = SR^{-1}$.\\
Somit bekommt man $\vec*{g}{\vec{x}}(\vec{y}) = M(\vec{y} - \vec*{p}{\ast}) + \vec*{q}{\ast}$.

\pagebreak
\ifthenelse{\equal{\standalonedoc}{true}}{\addtocontents{toc}{\protect\newpage}}{}
